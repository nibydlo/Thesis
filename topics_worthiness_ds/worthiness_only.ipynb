{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from struct import unpack\n",
    "from base64 import b64decode\n",
    "from functools import partial\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_LEN = 1024\n",
    "TXT_LEN = 300\n",
    "N_TOPICS = 50\n",
    "N_WORTHINESSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpck(l, x):\n",
    "    return unpack('%df' % l, b64decode(x.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpck_img = partial(unpck, IMG_LEN)\n",
    "unpck_txt = partial(unpck, TXT_LEN)\n",
    "filename = \"data/topics_worthiness_dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    return pd.read_json(filename, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('file.json') as f:\n",
    "    ds_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fasttext_embed', 'image_embed', 'topic', 'worthiness'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_txt = np.stack(df['fasttext_embed'].map(unpck_txt), axis=0)\n",
    "x_img = np.stack(df['image_embed'].map(unpck_img), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tpc = to_categorical(np.array(df['topic']), N_TOPICS)\n",
    "y_wts = to_categorical(np.array(df['worthiness']), N_WORTHINESSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_pickle_out = open(\"unpacked_worth_topics.pickle\", \"wb\")\n",
    "pickle.dump((x_img, x_txt, y_tpc, y_wts), topics_pickle_out)\n",
    "topics_pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_train, x_img_test, x_txt_train, x_txt_test, y_tpc_train, y_tpc_test, y_wts_train, y_wts_test = train_test_split(\n",
    "    x_img, \n",
    "    x_txt, \n",
    "    y_tpc,\n",
    "    y_wts,\n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_tpc\n",
    ")\n",
    "\n",
    "x_img_train, x_img_val, x_txt_train, x_txt_val, y_tpc_train, y_tpc_val, y_wts_train, y_wts_val = train_test_split(\n",
    "    x_img_train,\n",
    "    x_txt_train,\n",
    "    y_tpc_train,\n",
    "    y_wts_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_tpc_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sscaler = StandardScaler()\n",
    "img_sscaler.fit(x_img_train)\n",
    "\n",
    "x_img_train = img_sscaler.transform(x_img_train)\n",
    "x_img_val = img_sscaler.transform(x_img_val)\n",
    "x_img_test = img_sscaler.transform(x_img_test)\n",
    "\n",
    "txt_sscaler = StandardScaler()\n",
    "txt_sscaler.fit(x_txt_train)\n",
    "\n",
    "x_txt_train = txt_sscaler.transform(x_txt_train)\n",
    "x_txt_val = txt_sscaler.transform(x_txt_val)\n",
    "x_txt_test = txt_sscaler.transform(x_txt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_train_t = torch.tensor(x_img_train).float()\n",
    "x_img_val_t = torch.tensor(x_img_val).float()\n",
    "x_img_test_t = torch.tensor(x_img_test).float()\n",
    "\n",
    "x_txt_train_t = torch.tensor(x_txt_train).float()\n",
    "x_txt_val_t = torch.tensor(x_txt_val).float()\n",
    "x_txt_test_t = torch.tensor(x_txt_test).float()\n",
    "\n",
    "y_tpc_train_t = torch.tensor(y_tpc_train).float()\n",
    "y_tpc_val_t = torch.tensor(y_tpc_val).float()\n",
    "y_tpc_test_t = torch.tensor(y_tpc_test).float()\n",
    "\n",
    "y_wts_train_t = torch.tensor(y_wts_train).float()\n",
    "y_wts_val_t = torch.tensor(y_wts_val).float()\n",
    "y_wts_test_t = torch.tensor(y_wts_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_img_train_t, x_txt_train_t, y_tpc_train_t, y_wts_train_t)\n",
    "val_ds = TensorDataset(x_img_val_t, x_txt_val_t, y_tpc_val_t, y_wts_val_t)\n",
    "test_ds = TensorDataset(x_img_test_t, x_txt_test_t, y_tpc_test_t, y_wts_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2048\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wts_train_ds = TensorDataset(x_img_train_t, x_txt_train_t, y_wts_train_t)\n",
    "wts_val_ds = TensorDataset(x_img_val_t, x_txt_val_t, y_wts_val_t)\n",
    "wts_test_ds = TensorDataset(x_img_test_t, x_txt_test_t, y_wts_test_t)\n",
    "\n",
    "wts_train_loader = DataLoader(wts_train_ds, batch_size=BATCH_SIZE)\n",
    "wts_val_loader = DataLoader(wts_val_ds, batch_size=BATCH_SIZE)\n",
    "wts_test_loader = DataLoader(wts_test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worthiness only models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WtsTrivialModel(nn.Module):\n",
    "    def __init__(self, d=128, drop=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_img = nn.Linear(IMG_LEN, d)\n",
    "        self.fc_txt = nn.Linear(TXT_LEN, d)\n",
    "\n",
    "        self.fc = nn.Linear(d * 2, d)\n",
    "        self.out = nn.Linear(d, N_WORTHINESSES)\n",
    "\n",
    "        self.dropout = nn.modules.Dropout(p=drop)\n",
    "        \n",
    "    def forward(self, inp_img, inp_txt):\n",
    "        x_img = self.dropout(F.relu(self.fc_img(inp_img)))\n",
    "        x_txt = self.dropout(F.relu(self.fc_txt(inp_txt)))\n",
    "\n",
    "        x = torch.cat((x_img, x_txt), 1)\n",
    "        x = self.dropout(F.relu(self.fc(x)))\n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WtsNormModelBN(nn.Module):\n",
    "    def __init__(self, d=128, drop=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_img_1 = nn.Linear(IMG_LEN, d * 4)\n",
    "        self.bn_img_1 = nn.BatchNorm1d(num_features=d*4)\n",
    "        self.fc_img_2 = nn.Linear(d * 4, d * 2)\n",
    "        self.bn_img_2 = nn.BatchNorm1d(num_features=d*2)\n",
    "\n",
    "        self.fc_txt_1 = nn.Linear(TXT_LEN, d * 2)\n",
    "        self.bn_txt_1 = nn.BatchNorm1d(num_features=d*2)\n",
    "        self.fc_txt_2 = nn.Linear(d * 2, d * 2)\n",
    "        self.bn_txt_2 = nn.BatchNorm1d(num_features=d*2)\n",
    "\n",
    "        self.fc1 = nn.Linear(d * 4, d)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=d)\n",
    "        self.fc2 = nn.Linear(d, d)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=d)\n",
    "        self.out = nn.Linear(d, N_WORTHINESSES)\n",
    "\n",
    "        self.dropout = nn.modules.Dropout(p=drop)\n",
    "        \n",
    "    def forward(self, inp_img, inp_txt):\n",
    "        x_img = self.bn_img_1(F.relu(self.fc_img_1(inp_img)))\n",
    "        x_img = self.dropout(x_img)\n",
    "        x_img = self.bn_img_2(F.relu(self.fc_img_2(x_img)))\n",
    "        x_img = self.dropout(x_img)\n",
    "\n",
    "        x_txt = self.bn_txt_1(F.relu(self.fc_txt_1(inp_txt)))\n",
    "        x_txt = self.dropout(x_txt)\n",
    "        x_txt = self.bn_txt_2(F.relu(self.fc_txt_2(x_txt)))\n",
    "        x_txt = self.dropout(x_txt)\n",
    "\n",
    "        x = torch.cat((x_img, x_txt), 1)\n",
    "        x = self.bn1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "\n",
    "        x = F.log_softmax(self.out(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WtsTridentModelBN(nn.Module):\n",
    "    def __init__(self, d=128, drop=0.25):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_img_1 = nn.Linear(IMG_LEN, d * 4)\n",
    "        self.bn_img_1 = nn.BatchNorm1d(num_features=d*4)\n",
    "        self.fc_img_2 = nn.Linear(d * 4, d * 2)\n",
    "        self.bn_img_2 = nn.BatchNorm1d(num_features=d*2)\n",
    "\n",
    "        self.fc_txt_1 = nn.Linear(TXT_LEN, d * 2)\n",
    "        self.bn_txt_1 = nn.BatchNorm1d(num_features=d*2)\n",
    "        self.fc_txt_2 = nn.Linear(d * 2, d * 2)\n",
    "        self.bn_txt_2 = nn.BatchNorm1d(num_features=d*2)\n",
    "\n",
    "        self.fc1 = nn.Linear(d * 4, d)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=d)\n",
    "        self.fc2 = nn.Linear(d, d)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=d)\n",
    "        \n",
    "        self.out = nn.Linear(d, N_WORTHINESSES)\n",
    "        self.out_img = nn.Linear(d * 2, N_WORTHINESSES)\n",
    "        self.out_txt = nn.Linear(d * 2, N_WORTHINESSES)\n",
    "        \n",
    "        self.dropout = nn.modules.Dropout(p=drop)\n",
    "        \n",
    "    def forward(self, inp_img, inp_txt):\n",
    "        x_img = self.dropout(self.bn_img_1(F.relu(self.fc_img_1(inp_img))))\n",
    "        x_img = self.dropout(self.bn_img_2(F.relu(self.fc_img_2(x_img))))\n",
    "\n",
    "        x_txt = self.dropout(self.bn_txt_1(F.relu(self.fc_txt_1(inp_txt))))\n",
    "        x_txt = self.dropout(self.bn_txt_2(F.relu(self.fc_txt_2(x_txt))))\n",
    "\n",
    "        x = torch.cat((x_img, x_txt), 1)\n",
    "        \n",
    "        x = self.dropout(self.bn1(F.relu(self.fc1(x))))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        \n",
    "        out = F.log_softmax(self.out(x), dim=1)\n",
    "        out_img = F.log_softmax(self.out_img(x_img), dim=1)\n",
    "        out_txt = F.log_softmax(self.out_txt(x_txt), dim=1)\n",
    "        \n",
    "        return out, out_img, out_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_wts_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    val_loader=None, \n",
    "    scheduler=None, \n",
    "    writer=None, \n",
    "    epochs=1, \n",
    "    es_dif=None, \n",
    "    es_tol=0,\n",
    "    weight=None\n",
    "):\n",
    "    prev_train_loss = None\n",
    "    tol_epochs = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_loss_sum = 0.0\n",
    "        train_loss_count = 0\n",
    "\n",
    "        for x_img_cur, x_txt_cur, y_wts_cur in train_loader:\n",
    "            model.zero_grad()\n",
    "            output = model(x_img_cur, x_txt_cur)\n",
    "                        \n",
    "            train_loss = F.nll_loss(output, torch.argmax(y_wts_cur, dim=1), weight=weight)\n",
    "            train_loss.backward()\n",
    "\n",
    "            train_loss_sum += train_loss\n",
    "            train_loss_count += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        avg_train_loss = train_loss_sum / train_loss_count\n",
    "        \n",
    "        print('epoch:', epoch, 'train_loss:', train_loss, 'average train loss', avg_train_loss)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train_loss', train_loss, epoch)\n",
    "            writer.add_scalar('avg_train_loss', avg_train_loss, epoch)\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            val_loss_sum = 0.0\n",
    "            val_loss_count = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x_img_cur, x_txt_cur, y_wts_cur in val_loader:\n",
    "                    output = model(x_img_cur, x_txt_cur)\n",
    "                    val_loss = F.nll_loss(output, torch.argmax(y_wts_cur, dim=1))\n",
    "                    val_loss_sum += val_loss\n",
    "                    val_loss_count += 1\n",
    "                    for idx, i in enumerate(output):\n",
    "                        if torch.argmax(i) == torch.argmax(y_wts_cur, dim=1)[idx]:\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "\n",
    "            print('val_acc:', correct / total, 'val_avg_loss:', val_loss_sum / val_loss_count)\n",
    "            if writer is not None:\n",
    "                writer.add_scalar('val_acc', correct / total, epoch)\n",
    "                writer.add_scalar('val_avg_loss', val_loss_sum / val_loss_count, epoch)\n",
    "                \n",
    "        # es part\n",
    "        if es_dif is not None and epoch != 0:\n",
    "            if tol_epochs != 0:  # already in tolerance mode\n",
    "                if prev_train_loss - avg_train_loss > es_dif: # leave tolerance mode\n",
    "                    tol_epochs = 0\n",
    "                elif tol_epochs >= es_tol: # tolerance limit exceeded\n",
    "                    return\n",
    "                else: # continue tolerance mode\n",
    "                    tol_epochs += 1\n",
    "            elif prev_train_loss - avg_train_loss <= es_dif: # not in tolerance but to slow learning\n",
    "                if es_tol == 0:  # no tolerance\n",
    "                    return \n",
    "                else: #enter tolerance mode\n",
    "                    tol_epochs += 1\n",
    "        prev_train_loss = avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_wts_trident_model(\n",
    "    model, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    val_loader=None, \n",
    "    scheduler=None, \n",
    "    writer=None, \n",
    "    epochs=1, \n",
    "    es_dif=None, \n",
    "    es_tol=0,\n",
    "    weight=None\n",
    "):\n",
    "    prev_train_loss = None\n",
    "    tol_epochs = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_loss_sum = 0.0\n",
    "        train_loss_common_sum = 0.0\n",
    "        train_loss_img_sum = 0.0\n",
    "        train_loss_txt_sum = 0.0\n",
    "        train_loss_count = 0\n",
    "\n",
    "        for x_img_cur, x_txt_cur, y_wts_cur in train_loader:\n",
    "            model.zero_grad()\n",
    "            out_common, out_img, out_txt = model(x_img_cur, x_txt_cur)\n",
    "                        \n",
    "            train_loss_common = F.nll_loss(out_common, torch.argmax(y_wts_cur, dim=1), weight=weight)\n",
    "            train_loss_img = F.nll_loss(out_img, torch.argmax(y_wts_cur, dim=1), weight=weight)\n",
    "            train_loss_txt = F.nll_loss(out_txt, torch.argmax(y_wts_cur, dim=1), weight=weight)\n",
    "            train_loss = train_loss_common + train_loss_img + train_loss_txt\n",
    "            train_loss.backward()\n",
    "\n",
    "            train_loss_sum += train_loss\n",
    "            train_loss_common_sum += train_loss_common\n",
    "            train_loss_img_sum += train_loss_img\n",
    "            train_loss_txt_sum += train_loss_txt\n",
    "            train_loss_count += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        avg_train_loss = train_loss_sum / train_loss_count\n",
    "        \n",
    "        print('epoch:', epoch, 'train_loss:', train_loss, 'average train loss', avg_train_loss)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar('train_loss', train_loss, epoch)\n",
    "            writer.add_scalar('avg_train_loss', train_loss_sum / train_loss_count, epoch)\n",
    "            writer.add_scalar('avg_train_loss_common', train_loss_common_sum / train_loss_count, epoch)\n",
    "            writer.add_scalar('avg_train_loss_img', train_loss_img_sum / train_loss_count, epoch)\n",
    "            writer.add_scalar('avg_train_loss_txt', train_loss_txt_sum / train_loss_count, epoch)\n",
    "\n",
    "\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "\n",
    "            correct_common = 0\n",
    "            correct_img = 0\n",
    "            correct_txt = 0\n",
    "            total = 0\n",
    "            \n",
    "            val_loss_common_sum = 0.0\n",
    "            val_loss_img_sum = 0.0\n",
    "            val_loss_txt_sum = 0.0\n",
    "            val_loss_sum = 0.0\n",
    "            val_loss_count = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x_img_cur, x_txt_cur, y_wts_cur in val_loader:\n",
    "                    out_common, out_img, out_txt = model(x_img_cur, x_txt_cur)\n",
    "                    target = torch.argmax(y_wts_cur, dim=1)\n",
    "                    val_loss_common = F.nll_loss(out_common, target)\n",
    "                    val_loss_img = F.nll_loss(out_img, target)\n",
    "                    val_loss_txt = F.nll_loss(out_txt, target)\n",
    "                    \n",
    "                    val_loss = val_loss_common + val_loss_img + val_loss_txt\n",
    "                    \n",
    "                    val_loss_common_sum += val_loss_common\n",
    "                    val_loss_img_sum += val_loss_img\n",
    "                    val_loss_txt_sum += val_loss_txt\n",
    "                    val_loss_sum += val_loss\n",
    "                    \n",
    "                    val_loss_count += 1\n",
    "                    for idx, i in enumerate(out_common):\n",
    "                        if torch.argmax(i) == target[idx]:\n",
    "                            correct_common += 1\n",
    "                        total += 1\n",
    "                    \n",
    "                    for idx, i in enumerate(out_img):\n",
    "                        if torch.argmax(i) == target[idx]:\n",
    "                            correct_img += 1\n",
    "                           \n",
    "                    for idx, i in enumerate(out_txt):\n",
    "                        if torch.argmax(i) == target[idx]:\n",
    "                            correct_txt += 1\n",
    "                            \n",
    "            print('val_acc:', correct_common / total, 'val_avg_loss:', val_loss_sum / val_loss_count)\n",
    "            if writer is not None:\n",
    "                writer.add_scalar('val_acc', correct_common / total, epoch)\n",
    "                writer.add_scalar('val_img_acc', correct_img / total, epoch)\n",
    "                writer.add_scalar('val_txt_acc', correct_txt / total, epoch)\n",
    "                writer.add_scalar('val_avg_loss', val_loss_sum / val_loss_count, epoch)\n",
    "                writer.add_scalar('val_avg_common_loss', val_loss_common_sum / val_loss_count, epoch)\n",
    "                writer.add_scalar('val_avg_img_loss', val_loss_img_sum / val_loss_count, epoch)\n",
    "                writer.add_scalar('val_avg_txt_loss', val_loss_txt_sum / val_loss_count, epoch)\n",
    "\n",
    "                \n",
    "\n",
    "        # es part\n",
    "        if es_dif is not None and epoch != 0:\n",
    "            if tol_epochs != 0:  # already in tolerance mode\n",
    "                if prev_train_loss - avg_train_loss > es_dif: # leave tolerance mode\n",
    "                    tol_epochs = 0\n",
    "                elif tol_epochs >= es_tol: # tolerance limit exceeded\n",
    "                    return\n",
    "                else: # continue tolerance mode\n",
    "                    tol_epochs += 1\n",
    "            elif prev_train_loss - avg_train_loss <= es_dif: # not in tolerance but to slow learning\n",
    "                if es_tol == 0:  # no tolerance\n",
    "                    return \n",
    "                else: #enter tolerance mode\n",
    "                    tol_epochs += 1\n",
    "        prev_train_loss = avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.6910, grad_fn=<NllLossBackward>) average train loss tensor(0.6613, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.644426746895555 val_avg_loss: tensor(0.6232)\n",
      "epoch: 1 train_loss: tensor(0.6621, grad_fn=<NllLossBackward>) average train loss tensor(0.6268, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6293543344246821 val_avg_loss: tensor(0.6179)\n",
      "epoch: 2 train_loss: tensor(0.5958, grad_fn=<NllLossBackward>) average train loss tensor(0.6163, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6619768162109548 val_avg_loss: tensor(0.5890)\n",
      "epoch: 3 train_loss: tensor(0.5158, grad_fn=<NllLossBackward>) average train loss tensor(0.6028, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6561366250774268 val_avg_loss: tensor(0.5834)\n",
      "epoch: 4 train_loss: tensor(0.5090, grad_fn=<NllLossBackward>) average train loss tensor(0.5984, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6498834911364776 val_avg_loss: tensor(0.5862)\n",
      "epoch: 5 train_loss: tensor(0.4845, grad_fn=<NllLossBackward>) average train loss tensor(0.5859, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6581128513700852 val_avg_loss: tensor(0.5773)\n",
      "epoch: 6 train_loss: tensor(0.4476, grad_fn=<NllLossBackward>) average train loss tensor(0.5737, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6579653717960062 val_avg_loss: tensor(0.5639)\n",
      "epoch: 7 train_loss: tensor(0.4562, grad_fn=<NllLossBackward>) average train loss tensor(0.5706, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6548093089107159 val_avg_loss: tensor(0.5624)\n",
      "epoch: 8 train_loss: tensor(0.4452, grad_fn=<NllLossBackward>) average train loss tensor(0.5628, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6705306315075362 val_avg_loss: tensor(0.5440)\n",
      "epoch: 9 train_loss: tensor(0.4568, grad_fn=<NllLossBackward>) average train loss tensor(0.5561, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6493525646697932 val_avg_loss: tensor(0.5663)\n",
      "epoch: 10 train_loss: tensor(0.4023, grad_fn=<NllLossBackward>) average train loss tensor(0.5467, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6808247057782497 val_avg_loss: tensor(0.5278)\n",
      "epoch: 11 train_loss: tensor(0.4089, grad_fn=<NllLossBackward>) average train loss tensor(0.5395, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6553107394625844 val_avg_loss: tensor(0.5587)\n",
      "epoch: 12 train_loss: tensor(0.4663, grad_fn=<NllLossBackward>) average train loss tensor(0.5334, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.641949090051028 val_avg_loss: tensor(0.5609)\n",
      "epoch: 13 train_loss: tensor(0.3737, grad_fn=<NllLossBackward>) average train loss tensor(0.5230, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6675220481963248 val_avg_loss: tensor(0.5225)\n",
      "epoch: 14 train_loss: tensor(0.3351, grad_fn=<NllLossBackward>) average train loss tensor(0.5162, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6980503200306758 val_avg_loss: tensor(0.5109)\n",
      "epoch: 15 train_loss: tensor(0.3509, grad_fn=<NllLossBackward>) average train loss tensor(0.5075, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6745125800076689 val_avg_loss: tensor(0.5211)\n",
      "epoch: 16 train_loss: tensor(0.3954, grad_fn=<NllLossBackward>) average train loss tensor(0.5037, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6989351974751496 val_avg_loss: tensor(0.5045)\n",
      "epoch: 17 train_loss: tensor(0.3323, grad_fn=<NllLossBackward>) average train loss tensor(0.4960, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7091112880866 val_avg_loss: tensor(0.4917)\n",
      "epoch: 18 train_loss: tensor(0.3186, grad_fn=<NllLossBackward>) average train loss tensor(0.4908, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7476919446656638 val_avg_loss: tensor(0.4468)\n",
      "epoch: 19 train_loss: tensor(0.3436, grad_fn=<NllLossBackward>) average train loss tensor(0.4836, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7155119016016281 val_avg_loss: tensor(0.4746)\n",
      "epoch: 20 train_loss: tensor(0.3418, grad_fn=<NllLossBackward>) average train loss tensor(0.4748, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7017963012122821 val_avg_loss: tensor(0.4825)\n",
      "epoch: 21 train_loss: tensor(0.3192, grad_fn=<NllLossBackward>) average train loss tensor(0.4782, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7276642185057369 val_avg_loss: tensor(0.4718)\n",
      "epoch: 22 train_loss: tensor(0.3029, grad_fn=<NllLossBackward>) average train loss tensor(0.4638, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7448308409285314 val_avg_loss: tensor(0.4431)\n",
      "epoch: 23 train_loss: tensor(0.3066, grad_fn=<NllLossBackward>) average train loss tensor(0.4553, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7118544081644692 val_avg_loss: tensor(0.4770)\n",
      "epoch: 24 train_loss: tensor(0.2830, grad_fn=<NllLossBackward>) average train loss tensor(0.4499, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7363360174615816 val_avg_loss: tensor(0.4469)\n",
      "epoch: 25 train_loss: tensor(0.2663, grad_fn=<NllLossBackward>) average train loss tensor(0.4587, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7712591806034864 val_avg_loss: tensor(0.4247)\n",
      "epoch: 26 train_loss: tensor(0.2782, grad_fn=<NllLossBackward>) average train loss tensor(0.4403, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7509069993805858 val_avg_loss: tensor(0.4340)\n",
      "epoch: 27 train_loss: tensor(0.2674, grad_fn=<NllLossBackward>) average train loss tensor(0.4393, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.741379818895083 val_avg_loss: tensor(0.4536)\n",
      "epoch: 28 train_loss: tensor(0.2411, grad_fn=<NllLossBackward>) average train loss tensor(0.4295, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7629413326254314 val_avg_loss: tensor(0.4227)\n",
      "epoch: 29 train_loss: tensor(0.3004, grad_fn=<NllLossBackward>) average train loss tensor(0.4266, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7528832256732443 val_avg_loss: tensor(0.4311)\n",
      "epoch: 30 train_loss: tensor(0.2215, grad_fn=<NllLossBackward>) average train loss tensor(0.4279, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7829100669557266 val_avg_loss: tensor(0.4048)\n",
      "epoch: 31 train_loss: tensor(0.2597, grad_fn=<NllLossBackward>) average train loss tensor(0.4148, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7985723977229153 val_avg_loss: tensor(0.3846)\n",
      "epoch: 32 train_loss: tensor(0.2266, grad_fn=<NllLossBackward>) average train loss tensor(0.4080, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8096628616936554 val_avg_loss: tensor(0.3626)\n",
      "epoch: 33 train_loss: tensor(0.2539, grad_fn=<NllLossBackward>) average train loss tensor(0.4060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7952983511783618 val_avg_loss: tensor(0.3811)\n",
      "epoch: 34 train_loss: tensor(0.2403, grad_fn=<NllLossBackward>) average train loss tensor(0.4032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7816712385334631 val_avg_loss: tensor(0.4066)\n",
      "epoch: 35 train_loss: tensor(0.2680, grad_fn=<NllLossBackward>) average train loss tensor(0.4009, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7941185145857299 val_avg_loss: tensor(0.3821)\n",
      "epoch: 36 train_loss: tensor(0.2119, grad_fn=<NllLossBackward>) average train loss tensor(0.3871, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8022888829897059 val_avg_loss: tensor(0.3707)\n",
      "epoch: 37 train_loss: tensor(0.2837, grad_fn=<NllLossBackward>) average train loss tensor(0.3882, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7945904492227827 val_avg_loss: tensor(0.3898)\n",
      "epoch: 38 train_loss: tensor(0.2310, grad_fn=<NllLossBackward>) average train loss tensor(0.3906, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8091024393121553 val_avg_loss: tensor(0.3680)\n",
      "epoch: 39 train_loss: tensor(0.2041, grad_fn=<NllLossBackward>) average train loss tensor(0.3844, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8244993068460018 val_avg_loss: tensor(0.3431)\n",
      "epoch: 40 train_loss: tensor(0.2450, grad_fn=<NllLossBackward>) average train loss tensor(0.3858, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8080405863787865 val_avg_loss: tensor(0.3656)\n",
      "epoch: 41 train_loss: tensor(0.2129, grad_fn=<NllLossBackward>) average train loss tensor(0.3670, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8338790077574256 val_avg_loss: tensor(0.3390)\n",
      "epoch: 42 train_loss: tensor(0.1780, grad_fn=<NllLossBackward>) average train loss tensor(0.3641, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8265640208831077 val_avg_loss: tensor(0.3452)\n",
      "epoch: 43 train_loss: tensor(0.2108, grad_fn=<NllLossBackward>) average train loss tensor(0.3685, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8332890894611097 val_avg_loss: tensor(0.3393)\n",
      "epoch: 44 train_loss: tensor(0.2319, grad_fn=<NllLossBackward>) average train loss tensor(0.3645, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8327876589092411 val_avg_loss: tensor(0.3423)\n",
      "epoch: 45 train_loss: tensor(0.2172, grad_fn=<NllLossBackward>) average train loss tensor(0.3573, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8326991711647936 val_avg_loss: tensor(0.3346)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.1785, grad_fn=<NllLossBackward>) average train loss tensor(0.3617, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8509276465209569 val_avg_loss: tensor(0.3151)\n",
      "epoch: 47 train_loss: tensor(0.2254, grad_fn=<NllLossBackward>) average train loss tensor(0.3536, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8284517594313188 val_avg_loss: tensor(0.3462)\n",
      "epoch: 48 train_loss: tensor(0.1739, grad_fn=<NllLossBackward>) average train loss tensor(0.3477, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8443795534318497 val_avg_loss: tensor(0.3276)\n",
      "epoch: 49 train_loss: tensor(0.1817, grad_fn=<NllLossBackward>) average train loss tensor(0.3413, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8369760788130843 val_avg_loss: tensor(0.3356)\n",
      "epoch: 50 train_loss: tensor(0.1930, grad_fn=<NllLossBackward>) average train loss tensor(0.3381, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8482140223579034 val_avg_loss: tensor(0.3174)\n",
      "epoch: 51 train_loss: tensor(0.1936, grad_fn=<NllLossBackward>) average train loss tensor(0.3384, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8166533935049995 val_avg_loss: tensor(0.3642)\n",
      "epoch: 52 train_loss: tensor(0.1478, grad_fn=<NllLossBackward>) average train loss tensor(0.3390, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8532873197062207 val_avg_loss: tensor(0.3153)\n",
      "epoch: 53 train_loss: tensor(0.1691, grad_fn=<NllLossBackward>) average train loss tensor(0.3303, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8467392266171135 val_avg_loss: tensor(0.3160)\n",
      "epoch: 54 train_loss: tensor(0.2602, grad_fn=<NllLossBackward>) average train loss tensor(0.3244, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8472111612541663 val_avg_loss: tensor(0.3258)\n",
      "epoch: 55 train_loss: tensor(0.1604, grad_fn=<NllLossBackward>) average train loss tensor(0.3320, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8547916113618264 val_avg_loss: tensor(0.3081)\n",
      "epoch: 56 train_loss: tensor(0.2045, grad_fn=<NllLossBackward>) average train loss tensor(0.3224, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8510751260950359 val_avg_loss: tensor(0.3147)\n",
      "epoch: 57 train_loss: tensor(0.1711, grad_fn=<NllLossBackward>) average train loss tensor(0.3338, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8454119104504026 val_avg_loss: tensor(0.3215)\n",
      "epoch: 58 train_loss: tensor(0.1789, grad_fn=<NllLossBackward>) average train loss tensor(0.3186, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8631389552546972 val_avg_loss: tensor(0.3003)\n",
      "epoch: 59 train_loss: tensor(0.1931, grad_fn=<NllLossBackward>) average train loss tensor(0.3233, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8494233548653511 val_avg_loss: tensor(0.3139)\n",
      "epoch: 60 train_loss: tensor(0.1638, grad_fn=<NllLossBackward>) average train loss tensor(0.3172, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8659115712473823 val_avg_loss: tensor(0.2972)\n",
      "epoch: 61 train_loss: tensor(0.1477, grad_fn=<NllLossBackward>) average train loss tensor(0.3090, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8715157950623839 val_avg_loss: tensor(0.2883)\n",
      "epoch: 62 train_loss: tensor(0.1530, grad_fn=<NllLossBackward>) average train loss tensor(0.3049, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8649087101436451 val_avg_loss: tensor(0.3002)\n",
      "epoch: 63 train_loss: tensor(0.1439, grad_fn=<NllLossBackward>) average train loss tensor(0.3074, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8670029200955668 val_avg_loss: tensor(0.2958)\n",
      "epoch: 64 train_loss: tensor(0.1534, grad_fn=<NllLossBackward>) average train loss tensor(0.3072, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8623130696398549 val_avg_loss: tensor(0.3042)\n",
      "epoch: 65 train_loss: tensor(0.2204, grad_fn=<NllLossBackward>) average train loss tensor(0.3036, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8665014895436982 val_avg_loss: tensor(0.2992)\n",
      "epoch: 66 train_loss: tensor(0.1916, grad_fn=<NllLossBackward>) average train loss tensor(0.3062, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8645252632510397 val_avg_loss: tensor(0.3035)\n",
      "epoch: 67 train_loss: tensor(0.1915, grad_fn=<NllLossBackward>) average train loss tensor(0.3035, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8637878653806448 val_avg_loss: tensor(0.3001)\n",
      "epoch: 68 train_loss: tensor(0.2079, grad_fn=<NllLossBackward>) average train loss tensor(0.3023, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8666489691177772 val_avg_loss: tensor(0.2991)\n",
      "epoch: 69 train_loss: tensor(0.1698, grad_fn=<NllLossBackward>) average train loss tensor(0.3011, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.866383505884435 val_avg_loss: tensor(0.3000)\n",
      "epoch: 70 train_loss: tensor(0.1364, grad_fn=<NllLossBackward>) average train loss tensor(0.3005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8884169542518361 val_avg_loss: tensor(0.2656)\n",
      "epoch: 71 train_loss: tensor(0.1480, grad_fn=<NllLossBackward>) average train loss tensor(0.2898, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8811904551219656 val_avg_loss: tensor(0.2801)\n",
      "epoch: 72 train_loss: tensor(0.2331, grad_fn=<NllLossBackward>) average train loss tensor(0.2942, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8739049641624635 val_avg_loss: tensor(0.2953)\n",
      "epoch: 73 train_loss: tensor(0.1543, grad_fn=<NllLossBackward>) average train loss tensor(0.2906, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8697460401734359 val_avg_loss: tensor(0.2938)\n",
      "epoch: 74 train_loss: tensor(0.2218, grad_fn=<NllLossBackward>) average train loss tensor(0.2930, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8731970622068843 val_avg_loss: tensor(0.2900)\n",
      "epoch: 75 train_loss: tensor(0.1183, grad_fn=<NllLossBackward>) average train loss tensor(0.2872, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8804530572515706 val_avg_loss: tensor(0.2781)\n",
      "epoch: 76 train_loss: tensor(0.1453, grad_fn=<NllLossBackward>) average train loss tensor(0.2828, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8824587794590449 val_avg_loss: tensor(0.2745)\n",
      "epoch: 77 train_loss: tensor(0.1475, grad_fn=<NllLossBackward>) average train loss tensor(0.2881, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8756452231365955 val_avg_loss: tensor(0.2931)\n",
      "epoch: 78 train_loss: tensor(0.1639, grad_fn=<NllLossBackward>) average train loss tensor(0.2761, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8820458366516237 val_avg_loss: tensor(0.2809)\n",
      "epoch: 79 train_loss: tensor(0.1557, grad_fn=<NllLossBackward>) average train loss tensor(0.2900, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8774444739403593 val_avg_loss: tensor(0.2834)\n",
      "epoch: 80 train_loss: tensor(0.1059, grad_fn=<NllLossBackward>) average train loss tensor(0.2773, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8931657965371796 val_avg_loss: tensor(0.2624)\n",
      "epoch: 81 train_loss: tensor(0.1583, grad_fn=<NllLossBackward>) average train loss tensor(0.2803, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8728431112290947 val_avg_loss: tensor(0.2948)\n",
      "epoch: 82 train_loss: tensor(0.1314, grad_fn=<NllLossBackward>) average train loss tensor(0.2806, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8863522402147302 val_avg_loss: tensor(0.2732)\n",
      "epoch: 83 train_loss: tensor(0.1587, grad_fn=<NllLossBackward>) average train loss tensor(0.2696, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8945226086187064 val_avg_loss: tensor(0.2611)\n",
      "epoch: 84 train_loss: tensor(0.1367, grad_fn=<NllLossBackward>) average train loss tensor(0.2719, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8834321446479663 val_avg_loss: tensor(0.2737)\n",
      "epoch: 85 train_loss: tensor(0.1369, grad_fn=<NllLossBackward>) average train loss tensor(0.2695, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.887709052296257 val_avg_loss: tensor(0.2729)\n",
      "epoch: 86 train_loss: tensor(0.1274, grad_fn=<NllLossBackward>) average train loss tensor(0.2730, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8850544199628352 val_avg_loss: tensor(0.2772)\n",
      "epoch: 87 train_loss: tensor(0.1398, grad_fn=<NllLossBackward>) average train loss tensor(0.2684, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8921924313482583 val_avg_loss: tensor(0.2664)\n",
      "epoch: 88 train_loss: tensor(0.1243, grad_fn=<NllLossBackward>) average train loss tensor(0.2690, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8903931805444946 val_avg_loss: tensor(0.2687)\n",
      "epoch: 89 train_loss: tensor(0.1442, grad_fn=<NllLossBackward>) average train loss tensor(0.2681, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8962038757632068 val_avg_loss: tensor(0.2600)\n",
      "epoch: 90 train_loss: tensor(0.1167, grad_fn=<NllLossBackward>) average train loss tensor(0.2548, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8912485620741527 val_avg_loss: tensor(0.2701)\n",
      "epoch: 91 train_loss: tensor(0.1207, grad_fn=<NllLossBackward>) average train loss tensor(0.2655, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8867946789369673 val_avg_loss: tensor(0.2751)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 train_loss: tensor(0.1372, grad_fn=<NllLossBackward>) average train loss tensor(0.2653, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8933132761112585 val_avg_loss: tensor(0.2675)\n",
      "epoch: 93 train_loss: tensor(0.1349, grad_fn=<NllLossBackward>) average train loss tensor(0.2663, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.890924107011179 val_avg_loss: tensor(0.2682)\n",
      "epoch: 94 train_loss: tensor(0.1110, grad_fn=<NllLossBackward>) average train loss tensor(0.2632, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8967348022298912 val_avg_loss: tensor(0.2607)\n",
      "epoch: 95 train_loss: tensor(0.1075, grad_fn=<NllLossBackward>) average train loss tensor(0.2564, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9025749933634192 val_avg_loss: tensor(0.2553)\n",
      "epoch: 96 train_loss: tensor(0.1088, grad_fn=<NllLossBackward>) average train loss tensor(0.2566, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8943751290446273 val_avg_loss: tensor(0.2672)\n",
      "epoch: 97 train_loss: tensor(0.1374, grad_fn=<NllLossBackward>) average train loss tensor(0.2607, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8940801698964693 val_avg_loss: tensor(0.2651)\n",
      "epoch: 98 train_loss: tensor(0.1280, grad_fn=<NllLossBackward>) average train loss tensor(0.2535, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8964988349113647 val_avg_loss: tensor(0.2679)\n",
      "epoch: 99 train_loss: tensor(0.1336, grad_fn=<NllLossBackward>) average train loss tensor(0.2512, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8926348700704952 val_avg_loss: tensor(0.2682)\n"
     ]
    }
   ],
   "source": [
    "model = WtsTrivialModel(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/trivial_d128_drop05_wd0005_weighted')\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94     41251\n",
      "           1       0.06      0.21      0.10      1128\n",
      "\n",
      "    accuracy                           0.89     42379\n",
      "   macro avg       0.52      0.56      0.52     42379\n",
      "weighted avg       0.95      0.89      0.92     42379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with weight tensor([   1., 1000.])\n",
      "epoch: 0 train_loss: tensor(0.2138, grad_fn=<NllLossBackward>) average train loss tensor(0.1830, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.9024)\n",
      "epoch: 1 train_loss: tensor(0.1734, grad_fn=<NllLossBackward>) average train loss tensor(0.1501, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.7815)\n",
      "epoch: 2 train_loss: tensor(0.1702, grad_fn=<NllLossBackward>) average train loss tensor(0.1467, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.7375)\n",
      "epoch: 3 train_loss: tensor(0.1773, grad_fn=<NllLossBackward>) average train loss tensor(0.1442, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.7738)\n",
      "epoch: 4 train_loss: tensor(0.1639, grad_fn=<NllLossBackward>) average train loss tensor(0.1419, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.7092)\n",
      "epoch: 5 train_loss: tensor(0.1610, grad_fn=<NllLossBackward>) average train loss tensor(0.1415, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.6569)\n",
      "epoch: 6 train_loss: tensor(0.1620, grad_fn=<NllLossBackward>) average train loss tensor(0.1402, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.027431200778692152 val_avg_loss: tensor(2.7232)\n",
      "epoch: 7 train_loss: tensor(0.1606, grad_fn=<NllLossBackward>) average train loss tensor(0.1389, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.03377282246408873 val_avg_loss: tensor(2.6850)\n",
      "epoch: 8 train_loss: tensor(0.1610, grad_fn=<NllLossBackward>) average train loss tensor(0.1384, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.03524761820487862 val_avg_loss: tensor(2.7320)\n",
      "epoch: 9 train_loss: tensor(0.1575, grad_fn=<NllLossBackward>) average train loss tensor(0.1369, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.039554021767985136 val_avg_loss: tensor(2.7060)\n",
      "epoch: 10 train_loss: tensor(0.1593, grad_fn=<NllLossBackward>) average train loss tensor(0.1361, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.049435153231277465 val_avg_loss: tensor(2.7617)\n",
      "epoch: 11 train_loss: tensor(0.1551, grad_fn=<NllLossBackward>) average train loss tensor(0.1355, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.05636669321299 val_avg_loss: tensor(2.6557)\n",
      "epoch: 12 train_loss: tensor(0.1512, grad_fn=<NllLossBackward>) average train loss tensor(0.1347, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.06772262041707224 val_avg_loss: tensor(2.6110)\n",
      "epoch: 13 train_loss: tensor(0.1469, grad_fn=<NllLossBackward>) average train loss tensor(0.1340, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07710232132849601 val_avg_loss: tensor(2.5580)\n",
      "epoch: 14 train_loss: tensor(0.1570, grad_fn=<NllLossBackward>) average train loss tensor(0.1352, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.06185293336872843 val_avg_loss: tensor(2.6934)\n",
      "epoch: 15 train_loss: tensor(0.1482, grad_fn=<NllLossBackward>) average train loss tensor(0.1346, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07202902398017874 val_avg_loss: tensor(2.6809)\n",
      "epoch: 16 train_loss: tensor(0.1404, grad_fn=<NllLossBackward>) average train loss tensor(0.1329, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.0786066129841017 val_avg_loss: tensor(2.6293)\n",
      "epoch: 17 train_loss: tensor(0.1577, grad_fn=<NllLossBackward>) average train loss tensor(0.1320, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10356015691826682 val_avg_loss: tensor(2.5554)\n",
      "epoch: 18 train_loss: tensor(0.1517, grad_fn=<NllLossBackward>) average train loss tensor(0.1330, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09388549685868507 val_avg_loss: tensor(2.5419)\n",
      "epoch: 19 train_loss: tensor(0.1437, grad_fn=<NllLossBackward>) average train loss tensor(0.1316, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09270566026605315 val_avg_loss: tensor(2.6244)\n",
      "epoch: 20 train_loss: tensor(0.1391, grad_fn=<NllLossBackward>) average train loss tensor(0.1302, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12249653423000914 val_avg_loss: tensor(2.4292)\n",
      "epoch: 21 train_loss: tensor(0.1391, grad_fn=<NllLossBackward>) average train loss tensor(0.1317, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10084653275521341 val_avg_loss: tensor(2.4137)\n",
      "epoch: 22 train_loss: tensor(0.1432, grad_fn=<NllLossBackward>) average train loss tensor(0.1306, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11453263722974368 val_avg_loss: tensor(2.5684)\n",
      "epoch: 23 train_loss: tensor(0.1525, grad_fn=<NllLossBackward>) average train loss tensor(0.1314, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09521281302539598 val_avg_loss: tensor(2.6244)\n",
      "epoch: 24 train_loss: tensor(0.1403, grad_fn=<NllLossBackward>) average train loss tensor(0.1309, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10836799103324189 val_avg_loss: tensor(2.5289)\n",
      "epoch: 25 train_loss: tensor(0.1584, grad_fn=<NllLossBackward>) average train loss tensor(0.1305, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10276376721824028 val_avg_loss: tensor(2.6100)\n",
      "epoch: 26 train_loss: tensor(0.1407, grad_fn=<NllLossBackward>) average train loss tensor(0.1300, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11102262336666371 val_avg_loss: tensor(2.5340)\n",
      "epoch: 27 train_loss: tensor(0.1341, grad_fn=<NllLossBackward>) average train loss tensor(0.1286, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14488393357519982 val_avg_loss: tensor(2.4171)\n",
      "epoch: 28 train_loss: tensor(0.1502, grad_fn=<NllLossBackward>) average train loss tensor(0.1281, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12482671150045718 val_avg_loss: tensor(2.5701)\n",
      "epoch: 29 train_loss: tensor(0.1598, grad_fn=<NllLossBackward>) average train loss tensor(0.1292, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12801227030056336 val_avg_loss: tensor(2.5414)\n",
      "epoch: 30 train_loss: tensor(0.1368, grad_fn=<NllLossBackward>) average train loss tensor(0.1283, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13756894670088193 val_avg_loss: tensor(2.4886)\n",
      "epoch: 31 train_loss: tensor(0.1400, grad_fn=<NllLossBackward>) average train loss tensor(0.1276, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1349438102822759 val_avg_loss: tensor(2.6197)\n",
      "epoch: 32 train_loss: tensor(0.1316, grad_fn=<NllLossBackward>) average train loss tensor(0.1262, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1675367961537327 val_avg_loss: tensor(2.4289)\n",
      "epoch: 33 train_loss: tensor(0.1397, grad_fn=<NllLossBackward>) average train loss tensor(0.1265, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15358522844586026 val_avg_loss: tensor(2.4934)\n",
      "epoch: 34 train_loss: tensor(0.1386, grad_fn=<NllLossBackward>) average train loss tensor(0.1298, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13868979146388224 val_avg_loss: tensor(2.4598)\n",
      "epoch: 35 train_loss: tensor(0.1452, grad_fn=<NllLossBackward>) average train loss tensor(0.1285, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1381883609120137 val_avg_loss: tensor(2.5260)\n",
      "epoch: 36 train_loss: tensor(0.1432, grad_fn=<NllLossBackward>) average train loss tensor(0.1293, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11087514379258473 val_avg_loss: tensor(2.5895)\n",
      "epoch: 37 train_loss: tensor(0.1449, grad_fn=<NllLossBackward>) average train loss tensor(0.1280, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10816151962953131 val_avg_loss: tensor(2.5880)\n",
      "epoch: 38 train_loss: tensor(0.1415, grad_fn=<NllLossBackward>) average train loss tensor(0.1277, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14314367460106775 val_avg_loss: tensor(2.4916)\n",
      "epoch: 39 train_loss: tensor(0.1495, grad_fn=<NllLossBackward>) average train loss tensor(0.1304, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10656874022947822 val_avg_loss: tensor(2.7371)\n",
      "epoch: 40 train_loss: tensor(0.1357, grad_fn=<NllLossBackward>) average train loss tensor(0.1275, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15078311653835944 val_avg_loss: tensor(2.4869)\n",
      "epoch: 41 train_loss: tensor(0.1327, grad_fn=<NllLossBackward>) average train loss tensor(0.1253, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16246349880541544 val_avg_loss: tensor(2.4175)\n",
      "epoch: 42 train_loss: tensor(0.1588, grad_fn=<NllLossBackward>) average train loss tensor(0.1276, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14786302097159543 val_avg_loss: tensor(2.5059)\n",
      "epoch: 43 train_loss: tensor(0.1404, grad_fn=<NllLossBackward>) average train loss tensor(0.1253, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13007698433766923 val_avg_loss: tensor(2.5751)\n",
      "epoch: 44 train_loss: tensor(0.1481, grad_fn=<NllLossBackward>) average train loss tensor(0.1280, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13164026782290653 val_avg_loss: tensor(2.4592)\n",
      "epoch: 45 train_loss: tensor(0.1489, grad_fn=<NllLossBackward>) average train loss tensor(0.1278, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.12960504970061645 val_avg_loss: tensor(2.6513)\n",
      "epoch: 46 train_loss: tensor(0.1428, grad_fn=<NllLossBackward>) average train loss tensor(0.1274, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12966404153024805 val_avg_loss: tensor(2.5490)\n",
      "epoch: 47 train_loss: tensor(0.1420, grad_fn=<NllLossBackward>) average train loss tensor(0.1288, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1468601598678583 val_avg_loss: tensor(2.4590)\n",
      "epoch: 48 train_loss: tensor(0.1369, grad_fn=<NllLossBackward>) average train loss tensor(0.1270, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13866029554906645 val_avg_loss: tensor(2.4702)\n",
      "epoch: 49 train_loss: tensor(0.1427, grad_fn=<NllLossBackward>) average train loss tensor(0.1258, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1468896557826741 val_avg_loss: tensor(2.4384)\n",
      "epoch: 50 train_loss: tensor(0.1455, grad_fn=<NllLossBackward>) average train loss tensor(0.1304, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10583134235908327 val_avg_loss: tensor(2.5593)\n",
      "epoch: 51 train_loss: tensor(0.1435, grad_fn=<NllLossBackward>) average train loss tensor(0.1290, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12700940919682624 val_avg_loss: tensor(2.5281)\n",
      "epoch: 52 train_loss: tensor(0.1443, grad_fn=<NllLossBackward>) average train loss tensor(0.1268, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14933781671238533 val_avg_loss: tensor(2.4405)\n",
      "epoch: 53 train_loss: tensor(0.1401, grad_fn=<NllLossBackward>) average train loss tensor(0.1258, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1573902014570982 val_avg_loss: tensor(2.5215)\n",
      "epoch: 54 train_loss: tensor(0.1351, grad_fn=<NllLossBackward>) average train loss tensor(0.1253, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1619030764239153 val_avg_loss: tensor(2.3828)\n",
      "epoch: 55 train_loss: tensor(0.1525, grad_fn=<NllLossBackward>) average train loss tensor(0.1273, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13753945078606614 val_avg_loss: tensor(2.6215)\n",
      "epoch: 56 train_loss: tensor(0.1343, grad_fn=<NllLossBackward>) average train loss tensor(0.1251, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14818747603456922 val_avg_loss: tensor(2.4829)\n",
      "epoch: 57 train_loss: tensor(0.1413, grad_fn=<NllLossBackward>) average train loss tensor(0.1254, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15093059611243842 val_avg_loss: tensor(2.5116)\n",
      "epoch: 58 train_loss: tensor(0.1460, grad_fn=<NllLossBackward>) average train loss tensor(0.1258, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12588856443382593 val_avg_loss: tensor(2.5618)\n",
      "epoch: 59 train_loss: tensor(0.1441, grad_fn=<NllLossBackward>) average train loss tensor(0.1280, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11246792319263782 val_avg_loss: tensor(2.4972)\n",
      "epoch: 60 train_loss: tensor(0.1392, grad_fn=<NllLossBackward>) average train loss tensor(0.1265, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13264312892664365 val_avg_loss: tensor(2.5735)\n",
      "epoch: 61 train_loss: tensor(0.1374, grad_fn=<NllLossBackward>) average train loss tensor(0.1253, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1570657463941244 val_avg_loss: tensor(2.4898)\n",
      "epoch: 62 train_loss: tensor(0.1530, grad_fn=<NllLossBackward>) average train loss tensor(0.1271, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11606642480016517 val_avg_loss: tensor(2.6241)\n",
      "epoch: 63 train_loss: tensor(0.1367, grad_fn=<NllLossBackward>) average train loss tensor(0.1261, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.148364451523464 val_avg_loss: tensor(2.4214)\n",
      "epoch: 64 train_loss: tensor(0.1427, grad_fn=<NllLossBackward>) average train loss tensor(0.1257, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15653481992744006 val_avg_loss: tensor(2.3745)\n",
      "epoch: 65 train_loss: tensor(0.1391, grad_fn=<NllLossBackward>) average train loss tensor(0.1269, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13169925965253812 val_avg_loss: tensor(2.3945)\n",
      "epoch: 66 train_loss: tensor(0.1352, grad_fn=<NllLossBackward>) average train loss tensor(0.1262, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15930743592012506 val_avg_loss: tensor(2.4884)\n",
      "epoch: 67 train_loss: tensor(0.1398, grad_fn=<NllLossBackward>) average train loss tensor(0.1257, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1409609769046987 val_avg_loss: tensor(2.4975)\n",
      "epoch: 68 train_loss: tensor(0.1339, grad_fn=<NllLossBackward>) average train loss tensor(0.1251, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18490988998023775 val_avg_loss: tensor(2.3045)\n",
      "epoch: 69 train_loss: tensor(0.1392, grad_fn=<NllLossBackward>) average train loss tensor(0.1223, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18552930419136948 val_avg_loss: tensor(2.4066)\n",
      "epoch: 70 train_loss: tensor(0.1383, grad_fn=<NllLossBackward>) average train loss tensor(0.1251, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1468896557826741 val_avg_loss: tensor(2.4964)\n",
      "epoch: 71 train_loss: tensor(0.1285, grad_fn=<NllLossBackward>) average train loss tensor(0.1239, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18552930419136948 val_avg_loss: tensor(2.4154)\n",
      "epoch: 72 train_loss: tensor(0.1361, grad_fn=<NllLossBackward>) average train loss tensor(0.1231, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.160192313364599 val_avg_loss: tensor(2.4766)\n",
      "epoch: 73 train_loss: tensor(0.1429, grad_fn=<NllLossBackward>) average train loss tensor(0.1245, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1542636344866236 val_avg_loss: tensor(2.3920)\n",
      "epoch: 74 train_loss: tensor(0.1366, grad_fn=<NllLossBackward>) average train loss tensor(0.1227, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16892310415007522 val_avg_loss: tensor(2.4135)\n",
      "epoch: 75 train_loss: tensor(0.1465, grad_fn=<NllLossBackward>) average train loss tensor(0.1219, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1808984455652892 val_avg_loss: tensor(2.6084)\n",
      "epoch: 76 train_loss: tensor(0.1398, grad_fn=<NllLossBackward>) average train loss tensor(0.1270, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13880777512314543 val_avg_loss: tensor(2.5266)\n",
      "epoch: 77 train_loss: tensor(0.1433, grad_fn=<NllLossBackward>) average train loss tensor(0.1263, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11252691502226941 val_avg_loss: tensor(2.5173)\n",
      "epoch: 78 train_loss: tensor(0.1327, grad_fn=<NllLossBackward>) average train loss tensor(0.1251, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17299354039465534 val_avg_loss: tensor(2.4831)\n",
      "epoch: 79 train_loss: tensor(0.1367, grad_fn=<NllLossBackward>) average train loss tensor(0.1235, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16591452083886382 val_avg_loss: tensor(2.4385)\n",
      "epoch: 80 train_loss: tensor(0.1289, grad_fn=<NllLossBackward>) average train loss tensor(0.1232, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19479102144353008 val_avg_loss: tensor(2.3238)\n",
      "epoch: 81 train_loss: tensor(0.1434, grad_fn=<NllLossBackward>) average train loss tensor(0.1226, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15659381175707165 val_avg_loss: tensor(2.6043)\n",
      "epoch: 82 train_loss: tensor(0.1501, grad_fn=<NllLossBackward>) average train loss tensor(0.1263, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14467746217148925 val_avg_loss: tensor(2.4522)\n",
      "epoch: 83 train_loss: tensor(0.1349, grad_fn=<NllLossBackward>) average train loss tensor(0.1276, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14497242131964722 val_avg_loss: tensor(2.4428)\n",
      "epoch: 84 train_loss: tensor(0.1367, grad_fn=<NllLossBackward>) average train loss tensor(0.1231, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15364422027549185 val_avg_loss: tensor(2.4265)\n",
      "epoch: 85 train_loss: tensor(0.1358, grad_fn=<NllLossBackward>) average train loss tensor(0.1252, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17160723239831283 val_avg_loss: tensor(2.3734)\n",
      "epoch: 86 train_loss: tensor(0.1418, grad_fn=<NllLossBackward>) average train loss tensor(0.1220, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1676842757278117 val_avg_loss: tensor(2.4263)\n",
      "epoch: 87 train_loss: tensor(0.1341, grad_fn=<NllLossBackward>) average train loss tensor(0.1225, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18019054360971007 val_avg_loss: tensor(2.3291)\n",
      "epoch: 88 train_loss: tensor(0.1282, grad_fn=<NllLossBackward>) average train loss tensor(0.1222, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17317051588355012 val_avg_loss: tensor(2.4837)\n",
      "epoch: 89 train_loss: tensor(0.1486, grad_fn=<NllLossBackward>) average train loss tensor(0.1230, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16443972509807392 val_avg_loss: tensor(2.4093)\n",
      "epoch: 90 train_loss: tensor(0.1272, grad_fn=<NllLossBackward>) average train loss tensor(0.1226, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17859776420965695 val_avg_loss: tensor(2.2966)\n",
      "epoch: 91 train_loss: tensor(0.1364, grad_fn=<NllLossBackward>) average train loss tensor(0.1228, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.19116302392118692 val_avg_loss: tensor(2.3821)\n",
      "epoch: 92 train_loss: tensor(0.1286, grad_fn=<NllLossBackward>) average train loss tensor(0.1228, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18440845942836917 val_avg_loss: tensor(2.3511)\n",
      "epoch: 93 train_loss: tensor(0.1392, grad_fn=<NllLossBackward>) average train loss tensor(0.1233, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18883284665073888 val_avg_loss: tensor(2.4771)\n",
      "epoch: 94 train_loss: tensor(0.1361, grad_fn=<NllLossBackward>) average train loss tensor(0.1229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16352535173878419 val_avg_loss: tensor(2.4365)\n",
      "epoch: 95 train_loss: tensor(0.1500, grad_fn=<NllLossBackward>) average train loss tensor(0.1247, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13264312892664365 val_avg_loss: tensor(2.6246)\n",
      "epoch: 96 train_loss: tensor(0.1281, grad_fn=<NllLossBackward>) average train loss tensor(0.1229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1944665663805563 val_avg_loss: tensor(2.2681)\n",
      "epoch: 97 train_loss: tensor(0.1324, grad_fn=<NllLossBackward>) average train loss tensor(0.1248, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15662330767188745 val_avg_loss: tensor(2.3468)\n",
      "epoch: 98 train_loss: tensor(0.1370, grad_fn=<NllLossBackward>) average train loss tensor(0.1223, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18296315960239506 val_avg_loss: tensor(2.3829)\n",
      "epoch: 99 train_loss: tensor(0.1247, grad_fn=<NllLossBackward>) average train loss tensor(0.1227, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1989499454325576 val_avg_loss: tensor(2.2950)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.19      0.31     41251\n",
      "           1       0.03      0.95      0.06      1128\n",
      "\n",
      "    accuracy                           0.21     42379\n",
      "   macro avg       0.51      0.57      0.19     42379\n",
      "weighted avg       0.97      0.21      0.31     42379\n",
      "\n",
      "train with weight tensor([  1., 500.])\n",
      "epoch: 0 train_loss: tensor(0.3072, grad_fn=<NllLossBackward>) average train loss tensor(0.2745, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.1993)\n",
      "epoch: 1 train_loss: tensor(0.2841, grad_fn=<NllLossBackward>) average train loss tensor(0.2397, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.1843)\n",
      "epoch: 2 train_loss: tensor(0.2693, grad_fn=<NllLossBackward>) average train loss tensor(0.2340, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.1370)\n",
      "epoch: 3 train_loss: tensor(0.2765, grad_fn=<NllLossBackward>) average train loss tensor(0.2306, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.026634811078665603 val_avg_loss: tensor(2.1310)\n",
      "epoch: 4 train_loss: tensor(0.2519, grad_fn=<NllLossBackward>) average train loss tensor(0.2279, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.04763590242751379 val_avg_loss: tensor(2.1049)\n",
      "epoch: 5 train_loss: tensor(0.2471, grad_fn=<NllLossBackward>) average train loss tensor(0.2245, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.0563077013833584 val_avg_loss: tensor(2.1467)\n",
      "epoch: 6 train_loss: tensor(0.2614, grad_fn=<NllLossBackward>) average train loss tensor(0.2228, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07356281155060024 val_avg_loss: tensor(2.0991)\n",
      "epoch: 7 train_loss: tensor(0.2367, grad_fn=<NllLossBackward>) average train loss tensor(0.2211, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07733828864702239 val_avg_loss: tensor(2.0875)\n",
      "epoch: 8 train_loss: tensor(0.2407, grad_fn=<NllLossBackward>) average train loss tensor(0.2181, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09544878034392237 val_avg_loss: tensor(2.0238)\n",
      "epoch: 9 train_loss: tensor(0.2382, grad_fn=<NllLossBackward>) average train loss tensor(0.2175, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09432793558092205 val_avg_loss: tensor(2.0733)\n",
      "epoch: 10 train_loss: tensor(0.2231, grad_fn=<NllLossBackward>) average train loss tensor(0.2144, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10748311358876796 val_avg_loss: tensor(2.1102)\n",
      "epoch: 11 train_loss: tensor(0.2544, grad_fn=<NllLossBackward>) average train loss tensor(0.2143, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10420906704421437 val_avg_loss: tensor(2.0428)\n",
      "epoch: 12 train_loss: tensor(0.2133, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1320827065451435 val_avg_loss: tensor(2.0036)\n",
      "epoch: 13 train_loss: tensor(0.2208, grad_fn=<NllLossBackward>) average train loss tensor(0.2095, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13585818364156566 val_avg_loss: tensor(2.0019)\n",
      "epoch: 14 train_loss: tensor(0.2247, grad_fn=<NllLossBackward>) average train loss tensor(0.2111, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13299707990443324 val_avg_loss: tensor(2.0843)\n",
      "epoch: 15 train_loss: tensor(0.2251, grad_fn=<NllLossBackward>) average train loss tensor(0.2080, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.148364451523464 val_avg_loss: tensor(1.9965)\n",
      "epoch: 16 train_loss: tensor(0.2301, grad_fn=<NllLossBackward>) average train loss tensor(0.2055, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13706751614901336 val_avg_loss: tensor(1.9602)\n",
      "epoch: 17 train_loss: tensor(0.2404, grad_fn=<NllLossBackward>) average train loss tensor(0.2058, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15827507890157214 val_avg_loss: tensor(2.0124)\n",
      "epoch: 18 train_loss: tensor(0.2223, grad_fn=<NllLossBackward>) average train loss tensor(0.2083, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12813025395982655 val_avg_loss: tensor(2.0609)\n",
      "epoch: 19 train_loss: tensor(0.2027, grad_fn=<NllLossBackward>) average train loss tensor(0.2046, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17491077485768222 val_avg_loss: tensor(1.9993)\n",
      "epoch: 20 train_loss: tensor(0.2166, grad_fn=<NllLossBackward>) average train loss tensor(0.2060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13166976373772232 val_avg_loss: tensor(2.0452)\n",
      "epoch: 21 train_loss: tensor(0.2110, grad_fn=<NllLossBackward>) average train loss tensor(0.2024, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19039613013597617 val_avg_loss: tensor(1.9320)\n",
      "epoch: 22 train_loss: tensor(0.2153, grad_fn=<NllLossBackward>) average train loss tensor(0.2019, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17143025690941804 val_avg_loss: tensor(1.9527)\n",
      "epoch: 23 train_loss: tensor(0.2178, grad_fn=<NllLossBackward>) average train loss tensor(0.2034, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1336459900303808 val_avg_loss: tensor(1.9743)\n",
      "epoch: 24 train_loss: tensor(0.2252, grad_fn=<NllLossBackward>) average train loss tensor(0.1996, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1615491254461257 val_avg_loss: tensor(2.0684)\n",
      "epoch: 25 train_loss: tensor(0.2088, grad_fn=<NllLossBackward>) average train loss tensor(0.2006, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18128189245789458 val_avg_loss: tensor(1.8689)\n",
      "epoch: 26 train_loss: tensor(0.2196, grad_fn=<NllLossBackward>) average train loss tensor(0.2027, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15382119576438663 val_avg_loss: tensor(2.0664)\n",
      "epoch: 27 train_loss: tensor(0.2098, grad_fn=<NllLossBackward>) average train loss tensor(0.1972, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1959118662065304 val_avg_loss: tensor(1.9897)\n",
      "epoch: 28 train_loss: tensor(0.2515, grad_fn=<NllLossBackward>) average train loss tensor(0.1997, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18968822818039702 val_avg_loss: tensor(1.9476)\n",
      "epoch: 29 train_loss: tensor(0.1923, grad_fn=<NllLossBackward>) average train loss tensor(0.1979, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22021650001474796 val_avg_loss: tensor(1.8574)\n",
      "epoch: 30 train_loss: tensor(0.2089, grad_fn=<NllLossBackward>) average train loss tensor(0.2041, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15827507890157214 val_avg_loss: tensor(1.9196)\n",
      "epoch: 31 train_loss: tensor(0.1920, grad_fn=<NllLossBackward>) average train loss tensor(0.1954, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20387576320679585 val_avg_loss: tensor(1.8988)\n",
      "epoch: 32 train_loss: tensor(0.2152, grad_fn=<NllLossBackward>) average train loss tensor(0.1959, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16859864908710143 val_avg_loss: tensor(2.0420)\n",
      "epoch: 33 train_loss: tensor(0.2018, grad_fn=<NllLossBackward>) average train loss tensor(0.1940, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21416983747750937 val_avg_loss: tensor(1.8922)\n",
      "epoch: 34 train_loss: tensor(0.2052, grad_fn=<NllLossBackward>) average train loss tensor(0.1960, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18856738341739668 val_avg_loss: tensor(2.0225)\n",
      "epoch: 35 train_loss: tensor(0.2029, grad_fn=<NllLossBackward>) average train loss tensor(0.1982, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.17293454856502374 val_avg_loss: tensor(1.8627)\n",
      "epoch: 36 train_loss: tensor(0.1768, grad_fn=<NllLossBackward>) average train loss tensor(0.1902, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23036309471138247 val_avg_loss: tensor(1.7941)\n",
      "epoch: 37 train_loss: tensor(0.1863, grad_fn=<NllLossBackward>) average train loss tensor(0.1883, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24425567058962333 val_avg_loss: tensor(1.7728)\n",
      "epoch: 38 train_loss: tensor(0.1892, grad_fn=<NllLossBackward>) average train loss tensor(0.1906, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2328112556410937 val_avg_loss: tensor(1.8272)\n",
      "epoch: 39 train_loss: tensor(0.1965, grad_fn=<NllLossBackward>) average train loss tensor(0.1934, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23697017963012124 val_avg_loss: tensor(1.8516)\n",
      "epoch: 40 train_loss: tensor(0.1978, grad_fn=<NllLossBackward>) average train loss tensor(0.1905, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22753148688906585 val_avg_loss: tensor(1.9036)\n",
      "epoch: 41 train_loss: tensor(0.2004, grad_fn=<NllLossBackward>) average train loss tensor(0.1920, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19378816033979293 val_avg_loss: tensor(1.9584)\n",
      "epoch: 42 train_loss: tensor(0.1877, grad_fn=<NllLossBackward>) average train loss tensor(0.1956, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19980532696221573 val_avg_loss: tensor(1.7803)\n",
      "epoch: 43 train_loss: tensor(0.1953, grad_fn=<NllLossBackward>) average train loss tensor(0.1919, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22773795829277646 val_avg_loss: tensor(1.7562)\n",
      "epoch: 44 train_loss: tensor(0.1891, grad_fn=<NllLossBackward>) average train loss tensor(0.1902, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2301861192224877 val_avg_loss: tensor(1.8235)\n",
      "epoch: 45 train_loss: tensor(0.1947, grad_fn=<NllLossBackward>) average train loss tensor(0.1906, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2331062147892517 val_avg_loss: tensor(1.7736)\n",
      "epoch: 46 train_loss: tensor(0.1982, grad_fn=<NllLossBackward>) average train loss tensor(0.1872, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22360853021856472 val_avg_loss: tensor(1.8466)\n",
      "epoch: 47 train_loss: tensor(0.1867, grad_fn=<NllLossBackward>) average train loss tensor(0.1892, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24988939031944077 val_avg_loss: tensor(1.8756)\n",
      "epoch: 48 train_loss: tensor(0.2026, grad_fn=<NllLossBackward>) average train loss tensor(0.1948, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19526295608058283 val_avg_loss: tensor(1.8734)\n",
      "epoch: 49 train_loss: tensor(0.1938, grad_fn=<NllLossBackward>) average train loss tensor(0.1869, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22582072382974958 val_avg_loss: tensor(1.8720)\n",
      "epoch: 50 train_loss: tensor(0.1860, grad_fn=<NllLossBackward>) average train loss tensor(0.1900, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23924136507093768 val_avg_loss: tensor(1.7695)\n",
      "epoch: 51 train_loss: tensor(0.1924, grad_fn=<NllLossBackward>) average train loss tensor(0.1880, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25803026280860103 val_avg_loss: tensor(1.8113)\n",
      "epoch: 52 train_loss: tensor(0.1679, grad_fn=<NllLossBackward>) average train loss tensor(0.1841, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26738046780520897 val_avg_loss: tensor(1.7418)\n",
      "epoch: 53 train_loss: tensor(0.1848, grad_fn=<NllLossBackward>) average train loss tensor(0.1841, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2722767896646314 val_avg_loss: tensor(1.5818)\n",
      "epoch: 54 train_loss: tensor(0.1759, grad_fn=<NllLossBackward>) average train loss tensor(0.1850, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.261392797097602 val_avg_loss: tensor(1.7114)\n",
      "epoch: 55 train_loss: tensor(0.1901, grad_fn=<NllLossBackward>) average train loss tensor(0.1854, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2313659558151196 val_avg_loss: tensor(1.8458)\n",
      "epoch: 56 train_loss: tensor(0.1892, grad_fn=<NllLossBackward>) average train loss tensor(0.1829, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24868005781199304 val_avg_loss: tensor(1.7938)\n",
      "epoch: 57 train_loss: tensor(0.1933, grad_fn=<NllLossBackward>) average train loss tensor(0.1876, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23779606524496358 val_avg_loss: tensor(1.8358)\n",
      "epoch: 58 train_loss: tensor(0.1979, grad_fn=<NllLossBackward>) average train loss tensor(0.1852, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24888652921570362 val_avg_loss: tensor(1.7905)\n",
      "epoch: 59 train_loss: tensor(0.1758, grad_fn=<NllLossBackward>) average train loss tensor(0.1834, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27127392856089433 val_avg_loss: tensor(1.7162)\n",
      "epoch: 60 train_loss: tensor(0.1705, grad_fn=<NllLossBackward>) average train loss tensor(0.1840, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2570274017048639 val_avg_loss: tensor(1.6921)\n",
      "epoch: 61 train_loss: tensor(0.2019, grad_fn=<NllLossBackward>) average train loss tensor(0.1862, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23062855794472464 val_avg_loss: tensor(1.8670)\n",
      "epoch: 62 train_loss: tensor(0.1783, grad_fn=<NllLossBackward>) average train loss tensor(0.1863, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24363625637849157 val_avg_loss: tensor(1.7705)\n",
      "epoch: 63 train_loss: tensor(0.1837, grad_fn=<NllLossBackward>) average train loss tensor(0.1857, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.249417455682388 val_avg_loss: tensor(1.7625)\n",
      "epoch: 64 train_loss: tensor(0.1834, grad_fn=<NllLossBackward>) average train loss tensor(0.1851, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23228032917440936 val_avg_loss: tensor(1.7554)\n",
      "epoch: 65 train_loss: tensor(0.1847, grad_fn=<NllLossBackward>) average train loss tensor(0.1828, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2537828510751261 val_avg_loss: tensor(1.7244)\n",
      "epoch: 66 train_loss: tensor(0.1675, grad_fn=<NllLossBackward>) average train loss tensor(0.1815, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28083060496121287 val_avg_loss: tensor(1.6909)\n",
      "epoch: 67 train_loss: tensor(0.1846, grad_fn=<NllLossBackward>) average train loss tensor(0.1805, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2831017904020293 val_avg_loss: tensor(1.6891)\n",
      "epoch: 68 train_loss: tensor(0.1742, grad_fn=<NllLossBackward>) average train loss tensor(0.1822, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2668200454237088 val_avg_loss: tensor(1.6875)\n",
      "epoch: 69 train_loss: tensor(0.1781, grad_fn=<NllLossBackward>) average train loss tensor(0.1815, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27699613603515916 val_avg_loss: tensor(1.7799)\n",
      "epoch: 70 train_loss: tensor(0.1756, grad_fn=<NllLossBackward>) average train loss tensor(0.1818, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27089048166828894 val_avg_loss: tensor(1.7515)\n",
      "epoch: 71 train_loss: tensor(0.1710, grad_fn=<NllLossBackward>) average train loss tensor(0.1811, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2807716131315813 val_avg_loss: tensor(1.7000)\n",
      "epoch: 72 train_loss: tensor(0.1802, grad_fn=<NllLossBackward>) average train loss tensor(0.1852, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24531752352299208 val_avg_loss: tensor(1.7772)\n",
      "epoch: 73 train_loss: tensor(0.2165, grad_fn=<NllLossBackward>) average train loss tensor(0.1830, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29436922986166414 val_avg_loss: tensor(1.6347)\n",
      "epoch: 74 train_loss: tensor(0.1720, grad_fn=<NllLossBackward>) average train loss tensor(0.1809, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2797687520278441 val_avg_loss: tensor(1.6820)\n",
      "epoch: 75 train_loss: tensor(0.1761, grad_fn=<NllLossBackward>) average train loss tensor(0.1760, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28466507388726664 val_avg_loss: tensor(1.7816)\n",
      "epoch: 76 train_loss: tensor(0.1803, grad_fn=<NllLossBackward>) average train loss tensor(0.1782, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28024068666489693 val_avg_loss: tensor(1.8075)\n",
      "epoch: 77 train_loss: tensor(0.1808, grad_fn=<NllLossBackward>) average train loss tensor(0.1767, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29466418900982216 val_avg_loss: tensor(1.6322)\n",
      "epoch: 78 train_loss: tensor(0.1818, grad_fn=<NllLossBackward>) average train loss tensor(0.1836, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2610093502049966 val_avg_loss: tensor(1.6953)\n",
      "epoch: 79 train_loss: tensor(0.1680, grad_fn=<NllLossBackward>) average train loss tensor(0.1816, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26788189835707754 val_avg_loss: tensor(1.7131)\n",
      "epoch: 80 train_loss: tensor(0.1834, grad_fn=<NllLossBackward>) average train loss tensor(0.1804, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3029230451582456 val_avg_loss: tensor(1.6131)\n",
      "epoch: 81 train_loss: tensor(0.1723, grad_fn=<NllLossBackward>) average train loss tensor(0.1766, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.29554906645429607 val_avg_loss: tensor(1.7430)\n",
      "epoch: 82 train_loss: tensor(0.1655, grad_fn=<NllLossBackward>) average train loss tensor(0.1826, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28183346606495 val_avg_loss: tensor(1.6410)\n",
      "epoch: 83 train_loss: tensor(0.1882, grad_fn=<NllLossBackward>) average train loss tensor(0.1769, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29150812612453175 val_avg_loss: tensor(1.7308)\n",
      "epoch: 84 train_loss: tensor(0.1712, grad_fn=<NllLossBackward>) average train loss tensor(0.1786, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.302716573754535 val_avg_loss: tensor(1.7116)\n",
      "epoch: 85 train_loss: tensor(0.1696, grad_fn=<NllLossBackward>) average train loss tensor(0.1834, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29183258118750555 val_avg_loss: tensor(1.6295)\n",
      "epoch: 86 train_loss: tensor(0.1663, grad_fn=<NllLossBackward>) average train loss tensor(0.1755, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3292334011739374 val_avg_loss: tensor(1.5960)\n",
      "epoch: 87 train_loss: tensor(0.1689, grad_fn=<NllLossBackward>) average train loss tensor(0.1768, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30206766362858745 val_avg_loss: tensor(1.6527)\n",
      "epoch: 88 train_loss: tensor(0.1687, grad_fn=<NllLossBackward>) average train loss tensor(0.1796, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30457481638793027 val_avg_loss: tensor(1.6834)\n",
      "epoch: 89 train_loss: tensor(0.1952, grad_fn=<NllLossBackward>) average train loss tensor(0.1791, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2942807421172168 val_avg_loss: tensor(1.6988)\n",
      "epoch: 90 train_loss: tensor(0.1636, grad_fn=<NllLossBackward>) average train loss tensor(0.1775, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32708019939238414 val_avg_loss: tensor(1.5719)\n",
      "epoch: 91 train_loss: tensor(0.1789, grad_fn=<NllLossBackward>) average train loss tensor(0.1743, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3131876235141433 val_avg_loss: tensor(1.7227)\n",
      "epoch: 92 train_loss: tensor(0.1815, grad_fn=<NllLossBackward>) average train loss tensor(0.1803, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28254136802052915 val_avg_loss: tensor(1.7400)\n",
      "epoch: 93 train_loss: tensor(0.1768, grad_fn=<NllLossBackward>) average train loss tensor(0.1794, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29528360322095387 val_avg_loss: tensor(1.6126)\n",
      "epoch: 94 train_loss: tensor(0.1644, grad_fn=<NllLossBackward>) average train loss tensor(0.1747, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3026575819249034 val_avg_loss: tensor(1.6328)\n",
      "epoch: 95 train_loss: tensor(0.1773, grad_fn=<NllLossBackward>) average train loss tensor(0.1780, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28708373890216204 val_avg_loss: tensor(1.6876)\n",
      "epoch: 96 train_loss: tensor(0.1778, grad_fn=<NllLossBackward>) average train loss tensor(0.1765, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2938383033949798 val_avg_loss: tensor(1.6817)\n",
      "epoch: 97 train_loss: tensor(0.1721, grad_fn=<NllLossBackward>) average train loss tensor(0.1779, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31967672477361886 val_avg_loss: tensor(1.5460)\n",
      "epoch: 98 train_loss: tensor(0.1811, grad_fn=<NllLossBackward>) average train loss tensor(0.1753, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2831607822316609 val_avg_loss: tensor(1.6555)\n",
      "epoch: 99 train_loss: tensor(0.1630, grad_fn=<NllLossBackward>) average train loss tensor(0.1726, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33861310208536116 val_avg_loss: tensor(1.5434)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.33      0.49     41251\n",
      "           1       0.03      0.88      0.07      1128\n",
      "\n",
      "    accuracy                           0.34     42379\n",
      "   macro avg       0.51      0.60      0.28     42379\n",
      "weighted avg       0.96      0.34      0.48     42379\n",
      "\n",
      "train with weight tensor([  1., 400.])\n",
      "epoch: 0 train_loss: tensor(0.3577, grad_fn=<NllLossBackward>) average train loss tensor(0.3114, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.0440)\n",
      "epoch: 1 train_loss: tensor(0.3130, grad_fn=<NllLossBackward>) average train loss tensor(0.2755, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.0081)\n",
      "epoch: 2 train_loss: tensor(0.2997, grad_fn=<NllLossBackward>) average train loss tensor(0.2710, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.030793735067693125 val_avg_loss: tensor(1.9893)\n",
      "epoch: 3 train_loss: tensor(0.2839, grad_fn=<NllLossBackward>) average train loss tensor(0.2651, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.02979087396395599 val_avg_loss: tensor(1.9990)\n",
      "epoch: 4 train_loss: tensor(0.2797, grad_fn=<NllLossBackward>) average train loss tensor(0.2630, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.04468631094593399 val_avg_loss: tensor(1.9406)\n",
      "epoch: 5 train_loss: tensor(0.2653, grad_fn=<NllLossBackward>) average train loss tensor(0.2591, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.05754652980562192 val_avg_loss: tensor(1.9127)\n",
      "epoch: 6 train_loss: tensor(0.2756, grad_fn=<NllLossBackward>) average train loss tensor(0.2573, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.057192578827832345 val_avg_loss: tensor(1.9573)\n",
      "epoch: 7 train_loss: tensor(0.2808, grad_fn=<NllLossBackward>) average train loss tensor(0.2529, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07002330177270448 val_avg_loss: tensor(1.8987)\n",
      "epoch: 8 train_loss: tensor(0.2632, grad_fn=<NllLossBackward>) average train loss tensor(0.2521, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10131846739226617 val_avg_loss: tensor(1.9333)\n",
      "epoch: 9 train_loss: tensor(0.2661, grad_fn=<NllLossBackward>) average train loss tensor(0.2490, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12739285608943163 val_avg_loss: tensor(1.8713)\n",
      "epoch: 10 train_loss: tensor(0.2679, grad_fn=<NllLossBackward>) average train loss tensor(0.2463, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11606642480016517 val_avg_loss: tensor(1.9555)\n",
      "epoch: 11 train_loss: tensor(0.2594, grad_fn=<NllLossBackward>) average train loss tensor(0.2449, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1514615225791228 val_avg_loss: tensor(1.9046)\n",
      "epoch: 12 train_loss: tensor(0.2461, grad_fn=<NllLossBackward>) average train loss tensor(0.2412, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17243311801315517 val_avg_loss: tensor(1.8379)\n",
      "epoch: 13 train_loss: tensor(0.2525, grad_fn=<NllLossBackward>) average train loss tensor(0.2417, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16128366221278354 val_avg_loss: tensor(1.8713)\n",
      "epoch: 14 train_loss: tensor(0.2457, grad_fn=<NllLossBackward>) average train loss tensor(0.2400, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14624074565672654 val_avg_loss: tensor(1.8895)\n",
      "epoch: 15 train_loss: tensor(0.2591, grad_fn=<NllLossBackward>) average train loss tensor(0.2376, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15809810341267735 val_avg_loss: tensor(1.8120)\n",
      "epoch: 16 train_loss: tensor(0.2429, grad_fn=<NllLossBackward>) average train loss tensor(0.2367, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18900982213963366 val_avg_loss: tensor(1.7822)\n",
      "epoch: 17 train_loss: tensor(0.2388, grad_fn=<NllLossBackward>) average train loss tensor(0.2345, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1720496711205498 val_avg_loss: tensor(1.8117)\n",
      "epoch: 18 train_loss: tensor(0.2377, grad_fn=<NllLossBackward>) average train loss tensor(0.2357, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17446833613544524 val_avg_loss: tensor(1.8143)\n",
      "epoch: 19 train_loss: tensor(0.2489, grad_fn=<NllLossBackward>) average train loss tensor(0.2354, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16028080110904638 val_avg_loss: tensor(1.8682)\n",
      "epoch: 20 train_loss: tensor(0.2508, grad_fn=<NllLossBackward>) average train loss tensor(0.2321, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18137038020234197 val_avg_loss: tensor(1.7000)\n",
      "epoch: 21 train_loss: tensor(0.2456, grad_fn=<NllLossBackward>) average train loss tensor(0.2293, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18331711058018466 val_avg_loss: tensor(1.8592)\n",
      "epoch: 22 train_loss: tensor(0.2350, grad_fn=<NllLossBackward>) average train loss tensor(0.2310, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1822552576468159 val_avg_loss: tensor(1.7770)\n",
      "epoch: 23 train_loss: tensor(0.2437, grad_fn=<NllLossBackward>) average train loss tensor(0.2273, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1917529422175029 val_avg_loss: tensor(1.7885)\n",
      "epoch: 24 train_loss: tensor(0.2283, grad_fn=<NllLossBackward>) average train loss tensor(0.2272, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22761997463351324 val_avg_loss: tensor(1.8230)\n",
      "epoch: 25 train_loss: tensor(0.2147, grad_fn=<NllLossBackward>) average train loss tensor(0.2270, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.22638114621124975 val_avg_loss: tensor(1.6877)\n",
      "epoch: 26 train_loss: tensor(0.2489, grad_fn=<NllLossBackward>) average train loss tensor(0.2277, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18293366368757927 val_avg_loss: tensor(1.9211)\n",
      "epoch: 27 train_loss: tensor(0.2145, grad_fn=<NllLossBackward>) average train loss tensor(0.2259, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21782733091466833 val_avg_loss: tensor(1.6895)\n",
      "epoch: 28 train_loss: tensor(0.2220, grad_fn=<NllLossBackward>) average train loss tensor(0.2243, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22576173200011798 val_avg_loss: tensor(1.7036)\n",
      "epoch: 29 train_loss: tensor(0.2336, grad_fn=<NllLossBackward>) average train loss tensor(0.2235, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22965519275580332 val_avg_loss: tensor(1.7088)\n",
      "epoch: 30 train_loss: tensor(0.2221, grad_fn=<NllLossBackward>) average train loss tensor(0.2252, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23390260448927824 val_avg_loss: tensor(1.6814)\n",
      "epoch: 31 train_loss: tensor(0.2240, grad_fn=<NllLossBackward>) average train loss tensor(0.2246, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2150252190071675 val_avg_loss: tensor(1.7670)\n",
      "epoch: 32 train_loss: tensor(0.2115, grad_fn=<NllLossBackward>) average train loss tensor(0.2237, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23965430787835884 val_avg_loss: tensor(1.6502)\n",
      "epoch: 33 train_loss: tensor(0.2233, grad_fn=<NllLossBackward>) average train loss tensor(0.2164, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25584756511223194 val_avg_loss: tensor(1.7004)\n",
      "epoch: 34 train_loss: tensor(0.2155, grad_fn=<NllLossBackward>) average train loss tensor(0.2177, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25988850544199626 val_avg_loss: tensor(1.7604)\n",
      "epoch: 35 train_loss: tensor(0.2111, grad_fn=<NllLossBackward>) average train loss tensor(0.2209, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2209538978851429 val_avg_loss: tensor(1.7266)\n",
      "epoch: 36 train_loss: tensor(0.2316, grad_fn=<NllLossBackward>) average train loss tensor(0.2198, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24163053417101732 val_avg_loss: tensor(1.7890)\n",
      "epoch: 37 train_loss: tensor(0.2191, grad_fn=<NllLossBackward>) average train loss tensor(0.2194, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25567058962333716 val_avg_loss: tensor(1.6300)\n",
      "epoch: 38 train_loss: tensor(0.2200, grad_fn=<NllLossBackward>) average train loss tensor(0.2181, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22440491991859127 val_avg_loss: tensor(1.6902)\n",
      "epoch: 39 train_loss: tensor(0.2145, grad_fn=<NllLossBackward>) average train loss tensor(0.2162, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27384007314986875 val_avg_loss: tensor(1.6842)\n",
      "epoch: 40 train_loss: tensor(0.2315, grad_fn=<NllLossBackward>) average train loss tensor(0.2155, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2615107807568652 val_avg_loss: tensor(1.7215)\n",
      "epoch: 41 train_loss: tensor(0.2357, grad_fn=<NllLossBackward>) average train loss tensor(0.2152, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.234669498274489 val_avg_loss: tensor(1.7223)\n",
      "epoch: 42 train_loss: tensor(0.2260, grad_fn=<NllLossBackward>) average train loss tensor(0.2155, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24269238710438604 val_avg_loss: tensor(1.6885)\n",
      "epoch: 43 train_loss: tensor(0.1991, grad_fn=<NllLossBackward>) average train loss tensor(0.2141, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2533994041825207 val_avg_loss: tensor(1.6609)\n",
      "epoch: 44 train_loss: tensor(0.2224, grad_fn=<NllLossBackward>) average train loss tensor(0.2145, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2533994041825207 val_avg_loss: tensor(1.7412)\n",
      "epoch: 45 train_loss: tensor(0.1987, grad_fn=<NllLossBackward>) average train loss tensor(0.2166, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2579417750641536 val_avg_loss: tensor(1.6279)\n",
      "epoch: 46 train_loss: tensor(0.2040, grad_fn=<NllLossBackward>) average train loss tensor(0.2153, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2626021296050497 val_avg_loss: tensor(1.5421)\n",
      "epoch: 47 train_loss: tensor(0.1946, grad_fn=<NllLossBackward>) average train loss tensor(0.2141, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26360499070878685 val_avg_loss: tensor(1.5897)\n",
      "epoch: 48 train_loss: tensor(0.2037, grad_fn=<NllLossBackward>) average train loss tensor(0.2136, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24859157006754565 val_avg_loss: tensor(1.5887)\n",
      "epoch: 49 train_loss: tensor(0.2044, grad_fn=<NllLossBackward>) average train loss tensor(0.2141, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2640769253458396 val_avg_loss: tensor(1.6081)\n",
      "epoch: 50 train_loss: tensor(0.2318, grad_fn=<NllLossBackward>) average train loss tensor(0.2156, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2416895260006489 val_avg_loss: tensor(1.8021)\n",
      "epoch: 51 train_loss: tensor(0.1937, grad_fn=<NllLossBackward>) average train loss tensor(0.2060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30540070200277264 val_avg_loss: tensor(1.5786)\n",
      "epoch: 52 train_loss: tensor(0.1941, grad_fn=<NllLossBackward>) average train loss tensor(0.2068, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29584402560245404 val_avg_loss: tensor(1.5624)\n",
      "epoch: 53 train_loss: tensor(0.2047, grad_fn=<NllLossBackward>) average train loss tensor(0.2111, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2923045158245583 val_avg_loss: tensor(1.5857)\n",
      "epoch: 54 train_loss: tensor(0.2152, grad_fn=<NllLossBackward>) average train loss tensor(0.2077, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30917617909919476 val_avg_loss: tensor(1.5368)\n",
      "epoch: 55 train_loss: tensor(0.1899, grad_fn=<NllLossBackward>) average train loss tensor(0.2062, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29407427071350617 val_avg_loss: tensor(1.5916)\n",
      "epoch: 56 train_loss: tensor(0.1930, grad_fn=<NllLossBackward>) average train loss tensor(0.2077, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2930714096097691 val_avg_loss: tensor(1.6060)\n",
      "epoch: 57 train_loss: tensor(0.2117, grad_fn=<NllLossBackward>) average train loss tensor(0.2101, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26513877827920834 val_avg_loss: tensor(1.8051)\n",
      "epoch: 58 train_loss: tensor(0.1901, grad_fn=<NllLossBackward>) average train loss tensor(0.2092, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26661357401999825 val_avg_loss: tensor(1.6541)\n",
      "epoch: 59 train_loss: tensor(0.2155, grad_fn=<NllLossBackward>) average train loss tensor(0.2056, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29283544229124264 val_avg_loss: tensor(1.6246)\n",
      "epoch: 60 train_loss: tensor(0.1996, grad_fn=<NllLossBackward>) average train loss tensor(0.2072, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3189098309884081 val_avg_loss: tensor(1.4936)\n",
      "epoch: 61 train_loss: tensor(0.1858, grad_fn=<NllLossBackward>) average train loss tensor(0.2120, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28139102734271304 val_avg_loss: tensor(1.4545)\n",
      "epoch: 62 train_loss: tensor(0.2117, grad_fn=<NllLossBackward>) average train loss tensor(0.2050, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27965076836858094 val_avg_loss: tensor(1.6068)\n",
      "epoch: 63 train_loss: tensor(0.2027, grad_fn=<NllLossBackward>) average train loss tensor(0.2053, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26912072677934107 val_avg_loss: tensor(1.6038)\n",
      "epoch: 64 train_loss: tensor(0.1899, grad_fn=<NllLossBackward>) average train loss tensor(0.2056, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3234816977848568 val_avg_loss: tensor(1.5467)\n",
      "epoch: 65 train_loss: tensor(0.1982, grad_fn=<NllLossBackward>) average train loss tensor(0.2068, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2979972273840073 val_avg_loss: tensor(1.5944)\n",
      "epoch: 66 train_loss: tensor(0.2095, grad_fn=<NllLossBackward>) average train loss tensor(0.2055, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2795622806241336 val_avg_loss: tensor(1.5755)\n",
      "epoch: 67 train_loss: tensor(0.1856, grad_fn=<NllLossBackward>) average train loss tensor(0.2043, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2836032209538979 val_avg_loss: tensor(1.6203)\n",
      "epoch: 68 train_loss: tensor(0.1848, grad_fn=<NllLossBackward>) average train loss tensor(0.2010, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32761112585906854 val_avg_loss: tensor(1.4735)\n",
      "epoch: 69 train_loss: tensor(0.1822, grad_fn=<NllLossBackward>) average train loss tensor(0.2041, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3566351060378138 val_avg_loss: tensor(1.4441)\n",
      "epoch: 70 train_loss: tensor(0.1941, grad_fn=<NllLossBackward>) average train loss tensor(0.2027, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3328319027814648 val_avg_loss: tensor(1.5079)\n",
      "epoch: 71 train_loss: tensor(0.1840, grad_fn=<NllLossBackward>) average train loss tensor(0.2023, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.32274429991446185 val_avg_loss: tensor(1.5270)\n",
      "epoch: 72 train_loss: tensor(0.2021, grad_fn=<NllLossBackward>) average train loss tensor(0.2032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27705512786479075 val_avg_loss: tensor(1.5695)\n",
      "epoch: 73 train_loss: tensor(0.1874, grad_fn=<NllLossBackward>) average train loss tensor(0.1995, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3236586732737516 val_avg_loss: tensor(1.5177)\n",
      "epoch: 74 train_loss: tensor(0.1972, grad_fn=<NllLossBackward>) average train loss tensor(0.1981, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34878919269681147 val_avg_loss: tensor(1.4826)\n",
      "epoch: 75 train_loss: tensor(0.2047, grad_fn=<NllLossBackward>) average train loss tensor(0.2036, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3173760434179866 val_avg_loss: tensor(1.5650)\n",
      "epoch: 76 train_loss: tensor(0.1819, grad_fn=<NllLossBackward>) average train loss tensor(0.1983, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35684157744152434 val_avg_loss: tensor(1.4077)\n",
      "epoch: 77 train_loss: tensor(0.1881, grad_fn=<NllLossBackward>) average train loss tensor(0.2062, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2938677993097956 val_avg_loss: tensor(1.5763)\n",
      "epoch: 78 train_loss: tensor(0.2215, grad_fn=<NllLossBackward>) average train loss tensor(0.2068, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.303306492050851 val_avg_loss: tensor(1.6199)\n",
      "epoch: 79 train_loss: tensor(0.1763, grad_fn=<NllLossBackward>) average train loss tensor(0.2032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31044450343627406 val_avg_loss: tensor(1.3875)\n",
      "epoch: 80 train_loss: tensor(0.1893, grad_fn=<NllLossBackward>) average train loss tensor(0.1987, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32407161608117274 val_avg_loss: tensor(1.5337)\n",
      "epoch: 81 train_loss: tensor(0.1836, grad_fn=<NllLossBackward>) average train loss tensor(0.1952, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3530661003451022 val_avg_loss: tensor(1.5136)\n",
      "epoch: 82 train_loss: tensor(0.1642, grad_fn=<NllLossBackward>) average train loss tensor(0.1958, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33616494115564993 val_avg_loss: tensor(1.4446)\n",
      "epoch: 83 train_loss: tensor(0.1760, grad_fn=<NllLossBackward>) average train loss tensor(0.2028, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3397339468483615 val_avg_loss: tensor(1.4298)\n",
      "epoch: 84 train_loss: tensor(0.1858, grad_fn=<NllLossBackward>) average train loss tensor(0.2010, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3148983865734596 val_avg_loss: tensor(1.5275)\n",
      "epoch: 85 train_loss: tensor(0.1933, grad_fn=<NllLossBackward>) average train loss tensor(0.2002, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31988319617732947 val_avg_loss: tensor(1.5172)\n",
      "epoch: 86 train_loss: tensor(0.1755, grad_fn=<NllLossBackward>) average train loss tensor(0.1972, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3633896705306315 val_avg_loss: tensor(1.4455)\n",
      "epoch: 87 train_loss: tensor(0.1788, grad_fn=<NllLossBackward>) average train loss tensor(0.1990, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3296168480665428 val_avg_loss: tensor(1.5047)\n",
      "epoch: 88 train_loss: tensor(0.2001, grad_fn=<NllLossBackward>) average train loss tensor(0.1989, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3398814264224405 val_avg_loss: tensor(1.4773)\n",
      "epoch: 89 train_loss: tensor(0.1531, grad_fn=<NllLossBackward>) average train loss tensor(0.1952, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3530071085154706 val_avg_loss: tensor(1.3953)\n",
      "epoch: 90 train_loss: tensor(0.1713, grad_fn=<NllLossBackward>) average train loss tensor(0.1979, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36383210925286846 val_avg_loss: tensor(1.3705)\n",
      "epoch: 91 train_loss: tensor(0.1864, grad_fn=<NllLossBackward>) average train loss tensor(0.1943, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3115948441140902 val_avg_loss: tensor(1.6039)\n",
      "epoch: 92 train_loss: tensor(0.2247, grad_fn=<NllLossBackward>) average train loss tensor(0.1973, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3115653481992744 val_avg_loss: tensor(1.5318)\n",
      "epoch: 93 train_loss: tensor(0.1943, grad_fn=<NllLossBackward>) average train loss tensor(0.2004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3089402117806684 val_avg_loss: tensor(1.5797)\n",
      "epoch: 94 train_loss: tensor(0.1595, grad_fn=<NllLossBackward>) average train loss tensor(0.1974, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3639795888269475 val_avg_loss: tensor(1.3963)\n",
      "epoch: 95 train_loss: tensor(0.1951, grad_fn=<NllLossBackward>) average train loss tensor(0.1962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3399404182520721 val_avg_loss: tensor(1.4763)\n",
      "epoch: 96 train_loss: tensor(0.1808, grad_fn=<NllLossBackward>) average train loss tensor(0.1974, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3128041766215379 val_avg_loss: tensor(1.4486)\n",
      "epoch: 97 train_loss: tensor(0.1762, grad_fn=<NllLossBackward>) average train loss tensor(0.1997, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3476683479338112 val_avg_loss: tensor(1.4380)\n",
      "epoch: 98 train_loss: tensor(0.1748, grad_fn=<NllLossBackward>) average train loss tensor(0.1927, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38259151107571604 val_avg_loss: tensor(1.4182)\n",
      "epoch: 99 train_loss: tensor(0.1841, grad_fn=<NllLossBackward>) average train loss tensor(0.1959, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3545114001710763 val_avg_loss: tensor(1.4339)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.35      0.51     41251\n",
      "           1       0.04      0.88      0.07      1128\n",
      "\n",
      "    accuracy                           0.36     42379\n",
      "   macro avg       0.51      0.61      0.29     42379\n",
      "weighted avg       0.96      0.36      0.50     42379\n",
      "\n",
      "train with weight tensor([  1., 300.])\n",
      "epoch: 0 train_loss: tensor(0.3965, grad_fn=<NllLossBackward>) average train loss tensor(0.3604, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(1.8194)\n",
      "epoch: 1 train_loss: tensor(0.3700, grad_fn=<NllLossBackward>) average train loss tensor(0.3257, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025631949974928472 val_avg_loss: tensor(1.7816)\n",
      "epoch: 2 train_loss: tensor(0.3629, grad_fn=<NllLossBackward>) average train loss tensor(0.3187, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.042562605079196535 val_avg_loss: tensor(1.7378)\n",
      "epoch: 3 train_loss: tensor(0.3477, grad_fn=<NllLossBackward>) average train loss tensor(0.3143, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.05088045305725157 val_avg_loss: tensor(1.7304)\n",
      "epoch: 4 train_loss: tensor(0.3447, grad_fn=<NllLossBackward>) average train loss tensor(0.3114, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.06807657139486181 val_avg_loss: tensor(1.7487)\n",
      "epoch: 5 train_loss: tensor(0.3276, grad_fn=<NllLossBackward>) average train loss tensor(0.3051, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09654012919210689 val_avg_loss: tensor(1.7265)\n",
      "epoch: 6 train_loss: tensor(0.3321, grad_fn=<NllLossBackward>) average train loss tensor(0.3012, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10016812671445005 val_avg_loss: tensor(1.7154)\n",
      "epoch: 7 train_loss: tensor(0.2986, grad_fn=<NllLossBackward>) average train loss tensor(0.2996, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11636138394832315 val_avg_loss: tensor(1.6959)\n",
      "epoch: 8 train_loss: tensor(0.3118, grad_fn=<NllLossBackward>) average train loss tensor(0.2967, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14284871545290978 val_avg_loss: tensor(1.7047)\n",
      "epoch: 9 train_loss: tensor(0.3265, grad_fn=<NllLossBackward>) average train loss tensor(0.2939, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09978467982184468 val_avg_loss: tensor(1.7370)\n",
      "epoch: 10 train_loss: tensor(0.2848, grad_fn=<NllLossBackward>) average train loss tensor(0.2896, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13866029554906645 val_avg_loss: tensor(1.6462)\n",
      "epoch: 11 train_loss: tensor(0.2837, grad_fn=<NllLossBackward>) average train loss tensor(0.2845, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19623632126950416 val_avg_loss: tensor(1.6500)\n",
      "epoch: 12 train_loss: tensor(0.2751, grad_fn=<NllLossBackward>) average train loss tensor(0.2862, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14585729876412118 val_avg_loss: tensor(1.6593)\n",
      "epoch: 13 train_loss: tensor(0.3122, grad_fn=<NllLossBackward>) average train loss tensor(0.2823, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20502610388461198 val_avg_loss: tensor(1.6193)\n",
      "epoch: 14 train_loss: tensor(0.2982, grad_fn=<NllLossBackward>) average train loss tensor(0.2809, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18213727398755272 val_avg_loss: tensor(1.5947)\n",
      "epoch: 15 train_loss: tensor(0.2630, grad_fn=<NllLossBackward>) average train loss tensor(0.2759, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.23635076541898947 val_avg_loss: tensor(1.5711)\n",
      "epoch: 16 train_loss: tensor(0.2825, grad_fn=<NllLossBackward>) average train loss tensor(0.2781, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22655812170014453 val_avg_loss: tensor(1.4857)\n",
      "epoch: 17 train_loss: tensor(0.2872, grad_fn=<NllLossBackward>) average train loss tensor(0.2740, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19691472731026752 val_avg_loss: tensor(1.6671)\n",
      "epoch: 18 train_loss: tensor(0.2772, grad_fn=<NllLossBackward>) average train loss tensor(0.2751, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19688523139545172 val_avg_loss: tensor(1.4973)\n",
      "epoch: 19 train_loss: tensor(0.2845, grad_fn=<NllLossBackward>) average train loss tensor(0.2738, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2316609149632776 val_avg_loss: tensor(1.4927)\n",
      "epoch: 20 train_loss: tensor(0.2650, grad_fn=<NllLossBackward>) average train loss tensor(0.2689, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2327227678966463 val_avg_loss: tensor(1.5039)\n",
      "epoch: 21 train_loss: tensor(0.2579, grad_fn=<NllLossBackward>) average train loss tensor(0.2683, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22590921157419697 val_avg_loss: tensor(1.5094)\n",
      "epoch: 22 train_loss: tensor(0.2678, grad_fn=<NllLossBackward>) average train loss tensor(0.2675, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22127835294811668 val_avg_loss: tensor(1.6213)\n",
      "epoch: 23 train_loss: tensor(0.2635, grad_fn=<NllLossBackward>) average train loss tensor(0.2656, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24853257823791405 val_avg_loss: tensor(1.5262)\n",
      "epoch: 24 train_loss: tensor(0.2573, grad_fn=<NllLossBackward>) average train loss tensor(0.2629, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2389464059227797 val_avg_loss: tensor(1.4856)\n",
      "epoch: 25 train_loss: tensor(0.2545, grad_fn=<NllLossBackward>) average train loss tensor(0.2625, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22646963395569714 val_avg_loss: tensor(1.5176)\n",
      "epoch: 26 train_loss: tensor(0.2693, grad_fn=<NllLossBackward>) average train loss tensor(0.2571, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2813025395982656 val_avg_loss: tensor(1.4807)\n",
      "epoch: 27 train_loss: tensor(0.2442, grad_fn=<NllLossBackward>) average train loss tensor(0.2582, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26499129870512933 val_avg_loss: tensor(1.5246)\n",
      "epoch: 28 train_loss: tensor(0.2342, grad_fn=<NllLossBackward>) average train loss tensor(0.2602, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28708373890216204 val_avg_loss: tensor(1.4343)\n",
      "epoch: 29 train_loss: tensor(0.2331, grad_fn=<NllLossBackward>) average train loss tensor(0.2573, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28534347992802994 val_avg_loss: tensor(1.4093)\n",
      "epoch: 30 train_loss: tensor(0.2529, grad_fn=<NllLossBackward>) average train loss tensor(0.2563, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2814795150871604 val_avg_loss: tensor(1.4189)\n",
      "epoch: 31 train_loss: tensor(0.2281, grad_fn=<NllLossBackward>) average train loss tensor(0.2546, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3249564935256467 val_avg_loss: tensor(1.3131)\n",
      "epoch: 32 train_loss: tensor(0.2460, grad_fn=<NllLossBackward>) average train loss tensor(0.2521, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.290741232339321 val_avg_loss: tensor(1.4169)\n",
      "epoch: 33 train_loss: tensor(0.2241, grad_fn=<NllLossBackward>) average train loss tensor(0.2511, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3141904846178804 val_avg_loss: tensor(1.3351)\n",
      "epoch: 34 train_loss: tensor(0.2159, grad_fn=<NllLossBackward>) average train loss tensor(0.2522, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2765536973129222 val_avg_loss: tensor(1.3768)\n",
      "epoch: 35 train_loss: tensor(0.2428, grad_fn=<NllLossBackward>) average train loss tensor(0.2518, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30053387605816595 val_avg_loss: tensor(1.3857)\n",
      "epoch: 36 train_loss: tensor(0.2538, grad_fn=<NllLossBackward>) average train loss tensor(0.2482, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2966699112172964 val_avg_loss: tensor(1.3782)\n",
      "epoch: 37 train_loss: tensor(0.2419, grad_fn=<NllLossBackward>) average train loss tensor(0.2511, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31504586614753854 val_avg_loss: tensor(1.3438)\n",
      "epoch: 38 train_loss: tensor(0.2194, grad_fn=<NllLossBackward>) average train loss tensor(0.2535, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31681562103648647 val_avg_loss: tensor(1.3452)\n",
      "epoch: 39 train_loss: tensor(0.2471, grad_fn=<NllLossBackward>) average train loss tensor(0.2491, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3188213432439607 val_avg_loss: tensor(1.3936)\n",
      "epoch: 40 train_loss: tensor(0.2377, grad_fn=<NllLossBackward>) average train loss tensor(0.2460, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30065185971742914 val_avg_loss: tensor(1.3617)\n",
      "epoch: 41 train_loss: tensor(0.1886, grad_fn=<NllLossBackward>) average train loss tensor(0.2390, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36678170073444827 val_avg_loss: tensor(1.2015)\n",
      "epoch: 42 train_loss: tensor(0.2157, grad_fn=<NllLossBackward>) average train loss tensor(0.2456, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3127451847919063 val_avg_loss: tensor(1.4063)\n",
      "epoch: 43 train_loss: tensor(0.2379, grad_fn=<NllLossBackward>) average train loss tensor(0.2437, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33002979087396395 val_avg_loss: tensor(1.3513)\n",
      "epoch: 44 train_loss: tensor(0.2152, grad_fn=<NllLossBackward>) average train loss tensor(0.2381, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3489956641005221 val_avg_loss: tensor(1.3104)\n",
      "epoch: 45 train_loss: tensor(0.2332, grad_fn=<NllLossBackward>) average train loss tensor(0.2402, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33412972303335986 val_avg_loss: tensor(1.2779)\n",
      "epoch: 46 train_loss: tensor(0.2325, grad_fn=<NllLossBackward>) average train loss tensor(0.2417, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3381411674483084 val_avg_loss: tensor(1.3633)\n",
      "epoch: 47 train_loss: tensor(0.2170, grad_fn=<NllLossBackward>) average train loss tensor(0.2485, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3398224345928089 val_avg_loss: tensor(1.2828)\n",
      "epoch: 48 train_loss: tensor(0.2029, grad_fn=<NllLossBackward>) average train loss tensor(0.2429, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3496445742264696 val_avg_loss: tensor(1.2125)\n",
      "epoch: 49 train_loss: tensor(0.2069, grad_fn=<NllLossBackward>) average train loss tensor(0.2334, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3813821785682683 val_avg_loss: tensor(1.3192)\n",
      "epoch: 50 train_loss: tensor(0.2199, grad_fn=<NllLossBackward>) average train loss tensor(0.2404, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3402353774002301 val_avg_loss: tensor(1.3095)\n",
      "epoch: 51 train_loss: tensor(0.2361, grad_fn=<NllLossBackward>) average train loss tensor(0.2370, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32657876884051557 val_avg_loss: tensor(1.3982)\n",
      "epoch: 52 train_loss: tensor(0.1961, grad_fn=<NllLossBackward>) average train loss tensor(0.2329, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3606760463675781 val_avg_loss: tensor(1.2807)\n",
      "epoch: 53 train_loss: tensor(0.2068, grad_fn=<NllLossBackward>) average train loss tensor(0.2321, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36695867622334305 val_avg_loss: tensor(1.2930)\n",
      "epoch: 54 train_loss: tensor(0.2122, grad_fn=<NllLossBackward>) average train loss tensor(0.2385, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35808040586378787 val_avg_loss: tensor(1.3331)\n",
      "epoch: 55 train_loss: tensor(0.2057, grad_fn=<NllLossBackward>) average train loss tensor(0.2337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34073680795209865 val_avg_loss: tensor(1.2856)\n",
      "epoch: 56 train_loss: tensor(0.2279, grad_fn=<NllLossBackward>) average train loss tensor(0.2320, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3616789074713152 val_avg_loss: tensor(1.3156)\n",
      "epoch: 57 train_loss: tensor(0.1985, grad_fn=<NllLossBackward>) average train loss tensor(0.2296, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3855411025572958 val_avg_loss: tensor(1.2750)\n",
      "epoch: 58 train_loss: tensor(0.2343, grad_fn=<NllLossBackward>) average train loss tensor(0.2372, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36710615579742206 val_avg_loss: tensor(1.2695)\n",
      "epoch: 59 train_loss: tensor(0.1845, grad_fn=<NllLossBackward>) average train loss tensor(0.2298, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3916172610093502 val_avg_loss: tensor(1.1996)\n",
      "epoch: 60 train_loss: tensor(0.1819, grad_fn=<NllLossBackward>) average train loss tensor(0.2272, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4153319765212518 val_avg_loss: tensor(1.1446)\n",
      "epoch: 61 train_loss: tensor(0.2109, grad_fn=<NllLossBackward>) average train loss tensor(0.2317, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.37203197357166035 val_avg_loss: tensor(1.2745)\n",
      "epoch: 62 train_loss: tensor(0.1987, grad_fn=<NllLossBackward>) average train loss tensor(0.2323, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3693478453234227 val_avg_loss: tensor(1.2417)\n",
      "epoch: 63 train_loss: tensor(0.1793, grad_fn=<NllLossBackward>) average train loss tensor(0.2239, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4110255729581453 val_avg_loss: tensor(1.2259)\n",
      "epoch: 64 train_loss: tensor(0.1932, grad_fn=<NllLossBackward>) average train loss tensor(0.2272, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39978762941332624 val_avg_loss: tensor(1.2483)\n",
      "epoch: 65 train_loss: tensor(0.2112, grad_fn=<NllLossBackward>) average train loss tensor(0.2319, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3651299295047636 val_avg_loss: tensor(1.2784)\n",
      "epoch: 66 train_loss: tensor(0.2125, grad_fn=<NllLossBackward>) average train loss tensor(0.2259, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39076187947969204 val_avg_loss: tensor(1.2066)\n",
      "epoch: 67 train_loss: tensor(0.1819, grad_fn=<NllLossBackward>) average train loss tensor(0.2262, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4171607232398313 val_avg_loss: tensor(1.1426)\n",
      "epoch: 68 train_loss: tensor(0.1806, grad_fn=<NllLossBackward>) average train loss tensor(0.2268, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41733769872872606 val_avg_loss: tensor(1.1987)\n",
      "epoch: 69 train_loss: tensor(0.1844, grad_fn=<NllLossBackward>) average train loss tensor(0.2221, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40312066778751143 val_avg_loss: tensor(1.1912)\n",
      "epoch: 70 train_loss: tensor(0.2285, grad_fn=<NllLossBackward>) average train loss tensor(0.2232, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.400554523198537 val_avg_loss: tensor(1.2156)\n",
      "epoch: 71 train_loss: tensor(0.1811, grad_fn=<NllLossBackward>) average train loss tensor(0.2248, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43530071085154703 val_avg_loss: tensor(1.1287)\n",
      "epoch: 72 train_loss: tensor(0.1865, grad_fn=<NllLossBackward>) average train loss tensor(0.2232, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42910656874022945 val_avg_loss: tensor(1.1320)\n",
      "epoch: 73 train_loss: tensor(0.1835, grad_fn=<NllLossBackward>) average train loss tensor(0.2199, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43538919859599445 val_avg_loss: tensor(1.1262)\n",
      "epoch: 74 train_loss: tensor(0.1897, grad_fn=<NllLossBackward>) average train loss tensor(0.2180, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4151844969471728 val_avg_loss: tensor(1.1744)\n",
      "epoch: 75 train_loss: tensor(0.2150, grad_fn=<NllLossBackward>) average train loss tensor(0.2230, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38928708373890214 val_avg_loss: tensor(1.2108)\n",
      "epoch: 76 train_loss: tensor(0.1993, grad_fn=<NllLossBackward>) average train loss tensor(0.2198, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4330000294959148 val_avg_loss: tensor(1.1694)\n",
      "epoch: 77 train_loss: tensor(0.1898, grad_fn=<NllLossBackward>) average train loss tensor(0.2183, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4273073179364658 val_avg_loss: tensor(1.1955)\n",
      "epoch: 78 train_loss: tensor(0.1929, grad_fn=<NllLossBackward>) average train loss tensor(0.2252, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4006135150281686 val_avg_loss: tensor(1.1903)\n",
      "epoch: 79 train_loss: tensor(0.1978, grad_fn=<NllLossBackward>) average train loss tensor(0.2229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39205969973158716 val_avg_loss: tensor(1.1936)\n",
      "epoch: 80 train_loss: tensor(0.1927, grad_fn=<NllLossBackward>) average train loss tensor(0.2233, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41208742589151404 val_avg_loss: tensor(1.2326)\n",
      "epoch: 81 train_loss: tensor(0.1864, grad_fn=<NllLossBackward>) average train loss tensor(0.2194, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42471167743267557 val_avg_loss: tensor(1.1996)\n",
      "epoch: 82 train_loss: tensor(0.1757, grad_fn=<NllLossBackward>) average train loss tensor(0.2209, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4441789812111023 val_avg_loss: tensor(1.1630)\n",
      "epoch: 83 train_loss: tensor(0.1877, grad_fn=<NllLossBackward>) average train loss tensor(0.2160, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.436185588296021 val_avg_loss: tensor(1.1498)\n",
      "epoch: 84 train_loss: tensor(0.1897, grad_fn=<NllLossBackward>) average train loss tensor(0.2167, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4348877680441259 val_avg_loss: tensor(1.1807)\n",
      "epoch: 85 train_loss: tensor(0.1793, grad_fn=<NllLossBackward>) average train loss tensor(0.2194, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4483674011149456 val_avg_loss: tensor(1.0688)\n",
      "epoch: 86 train_loss: tensor(0.1784, grad_fn=<NllLossBackward>) average train loss tensor(0.2184, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4269238710438604 val_avg_loss: tensor(1.1153)\n",
      "epoch: 87 train_loss: tensor(0.1892, grad_fn=<NllLossBackward>) average train loss tensor(0.2198, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43376692328112554 val_avg_loss: tensor(1.1890)\n",
      "epoch: 88 train_loss: tensor(0.1693, grad_fn=<NllLossBackward>) average train loss tensor(0.2113, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43556617408488923 val_avg_loss: tensor(1.2051)\n",
      "epoch: 89 train_loss: tensor(0.1701, grad_fn=<NllLossBackward>) average train loss tensor(0.2140, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4524083414447099 val_avg_loss: tensor(1.0926)\n",
      "epoch: 90 train_loss: tensor(0.1840, grad_fn=<NllLossBackward>) average train loss tensor(0.2113, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4847063681680087 val_avg_loss: tensor(1.0359)\n",
      "epoch: 91 train_loss: tensor(0.1760, grad_fn=<NllLossBackward>) average train loss tensor(0.2129, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4500191723446303 val_avg_loss: tensor(1.0450)\n",
      "epoch: 92 train_loss: tensor(0.1685, grad_fn=<NllLossBackward>) average train loss tensor(0.2097, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4776863404418488 val_avg_loss: tensor(1.1120)\n",
      "epoch: 93 train_loss: tensor(0.1944, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46930950063416216 val_avg_loss: tensor(1.0874)\n",
      "epoch: 94 train_loss: tensor(0.1943, grad_fn=<NllLossBackward>) average train loss tensor(0.2171, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4308763236291774 val_avg_loss: tensor(1.1684)\n",
      "epoch: 95 train_loss: tensor(0.1767, grad_fn=<NllLossBackward>) average train loss tensor(0.2165, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4600477833820016 val_avg_loss: tensor(1.0699)\n",
      "epoch: 96 train_loss: tensor(0.1985, grad_fn=<NllLossBackward>) average train loss tensor(0.2167, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4455947851222606 val_avg_loss: tensor(1.1239)\n",
      "epoch: 97 train_loss: tensor(0.2287, grad_fn=<NllLossBackward>) average train loss tensor(0.2174, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.433265492729257 val_avg_loss: tensor(1.1229)\n",
      "epoch: 98 train_loss: tensor(0.1814, grad_fn=<NllLossBackward>) average train loss tensor(0.2147, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4323511193699673 val_avg_loss: tensor(1.1605)\n",
      "epoch: 99 train_loss: tensor(0.1688, grad_fn=<NllLossBackward>) average train loss tensor(0.2091, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4734389287083739 val_avg_loss: tensor(1.0910)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.47      0.64     41251\n",
      "           1       0.04      0.77      0.07      1128\n",
      "\n",
      "    accuracy                           0.48     42379\n",
      "   macro avg       0.51      0.62      0.35     42379\n",
      "weighted avg       0.96      0.48      0.62     42379\n",
      "\n",
      "train with weight tensor([  1., 200.])\n",
      "epoch: 0 train_loss: tensor(0.4980, grad_fn=<NllLossBackward>) average train loss tensor(0.4362, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.059699731587175175 val_avg_loss: tensor(1.5086)\n",
      "epoch: 1 train_loss: tensor(0.4260, grad_fn=<NllLossBackward>) average train loss tensor(0.4032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.06256083532430759 val_avg_loss: tensor(1.4608)\n",
      "epoch: 2 train_loss: tensor(0.4265, grad_fn=<NllLossBackward>) average train loss tensor(0.3974, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07925552311004926 val_avg_loss: tensor(1.4625)\n",
      "epoch: 3 train_loss: tensor(0.4002, grad_fn=<NllLossBackward>) average train loss tensor(0.3899, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11621390437424417 val_avg_loss: tensor(1.4237)\n",
      "epoch: 4 train_loss: tensor(0.3797, grad_fn=<NllLossBackward>) average train loss tensor(0.3843, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13164026782290653 val_avg_loss: tensor(1.4138)\n",
      "epoch: 5 train_loss: tensor(0.3981, grad_fn=<NllLossBackward>) average train loss tensor(0.3778, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.14635872931598973 val_avg_loss: tensor(1.4520)\n",
      "epoch: 6 train_loss: tensor(0.3679, grad_fn=<NllLossBackward>) average train loss tensor(0.3698, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1720496711205498 val_avg_loss: tensor(1.4111)\n",
      "epoch: 7 train_loss: tensor(0.3580, grad_fn=<NllLossBackward>) average train loss tensor(0.3661, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17921717842078871 val_avg_loss: tensor(1.3539)\n",
      "epoch: 8 train_loss: tensor(0.3449, grad_fn=<NllLossBackward>) average train loss tensor(0.3603, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23776656933014778 val_avg_loss: tensor(1.3558)\n",
      "epoch: 9 train_loss: tensor(0.3505, grad_fn=<NllLossBackward>) average train loss tensor(0.3589, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22614517889272337 val_avg_loss: tensor(1.3700)\n",
      "epoch: 10 train_loss: tensor(0.3499, grad_fn=<NllLossBackward>) average train loss tensor(0.3585, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22487685455564405 val_avg_loss: tensor(1.3432)\n",
      "epoch: 11 train_loss: tensor(0.3273, grad_fn=<NllLossBackward>) average train loss tensor(0.3509, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23682270005604225 val_avg_loss: tensor(1.3117)\n",
      "epoch: 12 train_loss: tensor(0.3407, grad_fn=<NllLossBackward>) average train loss tensor(0.3467, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2431938176562546 val_avg_loss: tensor(1.2871)\n",
      "epoch: 13 train_loss: tensor(0.3407, grad_fn=<NllLossBackward>) average train loss tensor(0.3439, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25272099814175736 val_avg_loss: tensor(1.3426)\n",
      "epoch: 14 train_loss: tensor(0.3302, grad_fn=<NllLossBackward>) average train loss tensor(0.3430, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2526915022269416 val_avg_loss: tensor(1.2772)\n",
      "epoch: 15 train_loss: tensor(0.2971, grad_fn=<NllLossBackward>) average train loss tensor(0.3381, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2776450461611067 val_avg_loss: tensor(1.2191)\n",
      "epoch: 16 train_loss: tensor(0.3163, grad_fn=<NllLossBackward>) average train loss tensor(0.3368, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2779105093944489 val_avg_loss: tensor(1.2515)\n",
      "epoch: 17 train_loss: tensor(0.3119, grad_fn=<NllLossBackward>) average train loss tensor(0.3328, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29165560569861076 val_avg_loss: tensor(1.2742)\n",
      "epoch: 18 train_loss: tensor(0.3168, grad_fn=<NllLossBackward>) average train loss tensor(0.3315, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27274872430168423 val_avg_loss: tensor(1.2680)\n",
      "epoch: 19 train_loss: tensor(0.3059, grad_fn=<NllLossBackward>) average train loss tensor(0.3328, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2690617349497095 val_avg_loss: tensor(1.2997)\n",
      "epoch: 20 train_loss: tensor(0.3319, grad_fn=<NllLossBackward>) average train loss tensor(0.3260, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29460519718019057 val_avg_loss: tensor(1.2301)\n",
      "epoch: 21 train_loss: tensor(0.3127, grad_fn=<NllLossBackward>) average train loss tensor(0.3244, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30392590626198274 val_avg_loss: tensor(1.2118)\n",
      "epoch: 22 train_loss: tensor(0.3030, grad_fn=<NllLossBackward>) average train loss tensor(0.3204, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29584402560245404 val_avg_loss: tensor(1.2469)\n",
      "epoch: 23 train_loss: tensor(0.2829, grad_fn=<NllLossBackward>) average train loss tensor(0.3207, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31799545762911835 val_avg_loss: tensor(1.1841)\n",
      "epoch: 24 train_loss: tensor(0.2809, grad_fn=<NllLossBackward>) average train loss tensor(0.3167, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31958823702917144 val_avg_loss: tensor(1.1814)\n",
      "epoch: 25 train_loss: tensor(0.2772, grad_fn=<NllLossBackward>) average train loss tensor(0.3163, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35973217709347255 val_avg_loss: tensor(1.1413)\n",
      "epoch: 26 train_loss: tensor(0.2877, grad_fn=<NllLossBackward>) average train loss tensor(0.3137, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32864348287762146 val_avg_loss: tensor(1.1941)\n",
      "epoch: 27 train_loss: tensor(0.2725, grad_fn=<NllLossBackward>) average train loss tensor(0.3164, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3453676665781789 val_avg_loss: tensor(1.1311)\n",
      "epoch: 28 train_loss: tensor(0.2946, grad_fn=<NllLossBackward>) average train loss tensor(0.3078, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35097189039318055 val_avg_loss: tensor(1.2275)\n",
      "epoch: 29 train_loss: tensor(0.2535, grad_fn=<NllLossBackward>) average train loss tensor(0.3077, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39545172993540395 val_avg_loss: tensor(1.1031)\n",
      "epoch: 30 train_loss: tensor(0.3274, grad_fn=<NllLossBackward>) average train loss tensor(0.3051, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37914048904226766 val_avg_loss: tensor(1.1187)\n",
      "epoch: 31 train_loss: tensor(0.2964, grad_fn=<NllLossBackward>) average train loss tensor(0.3027, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37297584284576585 val_avg_loss: tensor(1.0889)\n",
      "epoch: 32 train_loss: tensor(0.2729, grad_fn=<NllLossBackward>) average train loss tensor(0.3006, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3858360617054538 val_avg_loss: tensor(1.0876)\n",
      "epoch: 33 train_loss: tensor(0.2946, grad_fn=<NllLossBackward>) average train loss tensor(0.2980, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38598354127953277 val_avg_loss: tensor(1.0837)\n",
      "epoch: 34 train_loss: tensor(0.2756, grad_fn=<NllLossBackward>) average train loss tensor(0.3002, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3943898770020352 val_avg_loss: tensor(1.1013)\n",
      "epoch: 35 train_loss: tensor(0.2471, grad_fn=<NllLossBackward>) average train loss tensor(0.2933, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41928442910656877 val_avg_loss: tensor(1.0637)\n",
      "epoch: 36 train_loss: tensor(0.2454, grad_fn=<NllLossBackward>) average train loss tensor(0.2953, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40285520455416923 val_avg_loss: tensor(1.0340)\n",
      "epoch: 37 train_loss: tensor(0.2411, grad_fn=<NllLossBackward>) average train loss tensor(0.2923, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4062177388431702 val_avg_loss: tensor(1.0723)\n",
      "epoch: 38 train_loss: tensor(0.2405, grad_fn=<NllLossBackward>) average train loss tensor(0.2881, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4156564315842256 val_avg_loss: tensor(1.0738)\n",
      "epoch: 39 train_loss: tensor(0.2825, grad_fn=<NllLossBackward>) average train loss tensor(0.2933, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3958646727428251 val_avg_loss: tensor(1.0748)\n",
      "epoch: 40 train_loss: tensor(0.2626, grad_fn=<NllLossBackward>) average train loss tensor(0.2946, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4227059552252013 val_avg_loss: tensor(1.0145)\n",
      "epoch: 41 train_loss: tensor(0.2468, grad_fn=<NllLossBackward>) average train loss tensor(0.2883, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4311417868625195 val_avg_loss: tensor(1.0213)\n",
      "epoch: 42 train_loss: tensor(0.2219, grad_fn=<NllLossBackward>) average train loss tensor(0.2810, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45338170663363125 val_avg_loss: tensor(0.9848)\n",
      "epoch: 43 train_loss: tensor(0.2353, grad_fn=<NllLossBackward>) average train loss tensor(0.2823, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40727959177653894 val_avg_loss: tensor(1.0478)\n",
      "epoch: 44 train_loss: tensor(0.2248, grad_fn=<NllLossBackward>) average train loss tensor(0.2824, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4611686281450019 val_avg_loss: tensor(0.9406)\n",
      "epoch: 45 train_loss: tensor(0.2522, grad_fn=<NllLossBackward>) average train loss tensor(0.2794, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45420759224847357 val_avg_loss: tensor(0.9883)\n",
      "epoch: 46 train_loss: tensor(0.2255, grad_fn=<NllLossBackward>) average train loss tensor(0.2846, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4500191723446303 val_avg_loss: tensor(0.9976)\n",
      "epoch: 47 train_loss: tensor(0.2383, grad_fn=<NllLossBackward>) average train loss tensor(0.2806, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44819042562605077 val_avg_loss: tensor(0.9977)\n",
      "epoch: 48 train_loss: tensor(0.2118, grad_fn=<NllLossBackward>) average train loss tensor(0.2777, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4718756452231366 val_avg_loss: tensor(0.9765)\n",
      "epoch: 49 train_loss: tensor(0.2321, grad_fn=<NllLossBackward>) average train loss tensor(0.2815, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43712945757012656 val_avg_loss: tensor(1.0026)\n",
      "epoch: 50 train_loss: tensor(0.2464, grad_fn=<NllLossBackward>) average train loss tensor(0.2778, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4206412411880954 val_avg_loss: tensor(1.0741)\n",
      "epoch: 51 train_loss: tensor(0.2290, grad_fn=<NllLossBackward>) average train loss tensor(0.2758, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.48724301684216736 val_avg_loss: tensor(0.9537)\n",
      "epoch: 52 train_loss: tensor(0.2437, grad_fn=<NllLossBackward>) average train loss tensor(0.2781, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4645901542636345 val_avg_loss: tensor(0.9724)\n",
      "epoch: 53 train_loss: tensor(0.2875, grad_fn=<NllLossBackward>) average train loss tensor(0.2729, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44299914461847034 val_avg_loss: tensor(1.0051)\n",
      "epoch: 54 train_loss: tensor(0.2236, grad_fn=<NllLossBackward>) average train loss tensor(0.2760, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47818777099371734 val_avg_loss: tensor(0.9127)\n",
      "epoch: 55 train_loss: tensor(0.2227, grad_fn=<NllLossBackward>) average train loss tensor(0.2713, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47709642214553283 val_avg_loss: tensor(0.9509)\n",
      "epoch: 56 train_loss: tensor(0.2072, grad_fn=<NllLossBackward>) average train loss tensor(0.2662, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5021089579093295 val_avg_loss: tensor(0.8981)\n",
      "epoch: 57 train_loss: tensor(0.2235, grad_fn=<NllLossBackward>) average train loss tensor(0.2664, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.492257322360853 val_avg_loss: tensor(0.9782)\n",
      "epoch: 58 train_loss: tensor(0.2233, grad_fn=<NllLossBackward>) average train loss tensor(0.2698, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4623779606524496 val_avg_loss: tensor(0.9891)\n",
      "epoch: 59 train_loss: tensor(0.2193, grad_fn=<NllLossBackward>) average train loss tensor(0.2683, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.49682918915730173 val_avg_loss: tensor(0.9012)\n",
      "epoch: 60 train_loss: tensor(0.2250, grad_fn=<NllLossBackward>) average train loss tensor(0.2658, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4972421319647229 val_avg_loss: tensor(0.9541)\n",
      "epoch: 61 train_loss: tensor(0.2075, grad_fn=<NllLossBackward>) average train loss tensor(0.2679, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.519275580332124 val_avg_loss: tensor(0.8858)\n",
      "epoch: 62 train_loss: tensor(0.2293, grad_fn=<NllLossBackward>) average train loss tensor(0.2658, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4805474441789812 val_avg_loss: tensor(0.9072)\n",
      "epoch: 63 train_loss: tensor(0.2095, grad_fn=<NllLossBackward>) average train loss tensor(0.2627, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5139073238356487 val_avg_loss: tensor(0.9038)\n",
      "epoch: 64 train_loss: tensor(0.1986, grad_fn=<NllLossBackward>) average train loss tensor(0.2602, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5295106627732059 val_avg_loss: tensor(0.8566)\n",
      "epoch: 65 train_loss: tensor(0.2203, grad_fn=<NllLossBackward>) average train loss tensor(0.2576, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5181252396543079 val_avg_loss: tensor(0.8913)\n",
      "epoch: 66 train_loss: tensor(0.1928, grad_fn=<NllLossBackward>) average train loss tensor(0.2499, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5488009910627378 val_avg_loss: tensor(0.8466)\n",
      "epoch: 67 train_loss: tensor(0.2076, grad_fn=<NllLossBackward>) average train loss tensor(0.2573, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5081261245317523 val_avg_loss: tensor(0.8884)\n",
      "epoch: 68 train_loss: tensor(0.1961, grad_fn=<NllLossBackward>) average train loss tensor(0.2622, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5131994218800696 val_avg_loss: tensor(0.8663)\n",
      "epoch: 69 train_loss: tensor(0.2161, grad_fn=<NllLossBackward>) average train loss tensor(0.2535, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5361767395215763 val_avg_loss: tensor(0.8479)\n",
      "epoch: 70 train_loss: tensor(0.2003, grad_fn=<NllLossBackward>) average train loss tensor(0.2567, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5211043270507035 val_avg_loss: tensor(0.8981)\n",
      "epoch: 71 train_loss: tensor(0.2166, grad_fn=<NllLossBackward>) average train loss tensor(0.2550, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5412205409550778 val_avg_loss: tensor(0.8257)\n",
      "epoch: 72 train_loss: tensor(0.1977, grad_fn=<NllLossBackward>) average train loss tensor(0.2523, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5793882547267204 val_avg_loss: tensor(0.7872)\n",
      "epoch: 73 train_loss: tensor(0.2818, grad_fn=<NllLossBackward>) average train loss tensor(0.2510, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5405421349143144 val_avg_loss: tensor(0.8824)\n",
      "epoch: 74 train_loss: tensor(0.2067, grad_fn=<NllLossBackward>) average train loss tensor(0.2547, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5136418606023065 val_avg_loss: tensor(0.8786)\n",
      "epoch: 75 train_loss: tensor(0.1873, grad_fn=<NllLossBackward>) average train loss tensor(0.2515, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5095419284429107 val_avg_loss: tensor(0.9074)\n",
      "epoch: 76 train_loss: tensor(0.1940, grad_fn=<NllLossBackward>) average train loss tensor(0.2512, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5359407722030499 val_avg_loss: tensor(0.8456)\n",
      "epoch: 77 train_loss: tensor(0.2022, grad_fn=<NllLossBackward>) average train loss tensor(0.2571, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5164734684246232 val_avg_loss: tensor(0.8665)\n",
      "epoch: 78 train_loss: tensor(0.2117, grad_fn=<NllLossBackward>) average train loss tensor(0.2490, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5263251039730997 val_avg_loss: tensor(0.8964)\n",
      "epoch: 79 train_loss: tensor(0.2174, grad_fn=<NllLossBackward>) average train loss tensor(0.2493, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5582101878889774 val_avg_loss: tensor(0.8138)\n",
      "epoch: 80 train_loss: tensor(0.2126, grad_fn=<NllLossBackward>) average train loss tensor(0.2479, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5506002418665015 val_avg_loss: tensor(0.8636)\n",
      "epoch: 81 train_loss: tensor(0.2128, grad_fn=<NllLossBackward>) average train loss tensor(0.2439, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5264430876323629 val_avg_loss: tensor(0.8532)\n",
      "epoch: 82 train_loss: tensor(0.1886, grad_fn=<NllLossBackward>) average train loss tensor(0.2446, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.578562369111878 val_avg_loss: tensor(0.7845)\n",
      "epoch: 83 train_loss: tensor(0.2089, grad_fn=<NllLossBackward>) average train loss tensor(0.2457, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5152641359171755 val_avg_loss: tensor(0.9075)\n",
      "epoch: 84 train_loss: tensor(0.1850, grad_fn=<NllLossBackward>) average train loss tensor(0.2460, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5866442497714066 val_avg_loss: tensor(0.8005)\n",
      "epoch: 85 train_loss: tensor(0.1978, grad_fn=<NllLossBackward>) average train loss tensor(0.2484, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5442291242662891 val_avg_loss: tensor(0.8716)\n",
      "epoch: 86 train_loss: tensor(0.1807, grad_fn=<NllLossBackward>) average train loss tensor(0.2383, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5573253104445034 val_avg_loss: tensor(0.8307)\n",
      "epoch: 87 train_loss: tensor(0.1905, grad_fn=<NllLossBackward>) average train loss tensor(0.2423, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5669409786744536 val_avg_loss: tensor(0.8026)\n",
      "epoch: 88 train_loss: tensor(0.1656, grad_fn=<NllLossBackward>) average train loss tensor(0.2429, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5906261982715394 val_avg_loss: tensor(0.7617)\n",
      "epoch: 89 train_loss: tensor(0.1890, grad_fn=<NllLossBackward>) average train loss tensor(0.2482, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5626345751113471 val_avg_loss: tensor(0.8110)\n",
      "epoch: 90 train_loss: tensor(0.2142, grad_fn=<NllLossBackward>) average train loss tensor(0.2441, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5299531014954428 val_avg_loss: tensor(0.8840)\n",
      "epoch: 91 train_loss: tensor(0.1916, grad_fn=<NllLossBackward>) average train loss tensor(0.2472, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5561749697666873 val_avg_loss: tensor(0.8311)\n",
      "epoch: 92 train_loss: tensor(0.1998, grad_fn=<NllLossBackward>) average train loss tensor(0.2439, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5241719021915465 val_avg_loss: tensor(0.8565)\n",
      "epoch: 93 train_loss: tensor(0.1721, grad_fn=<NllLossBackward>) average train loss tensor(0.2401, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5990030380792261 val_avg_loss: tensor(0.7523)\n",
      "epoch: 94 train_loss: tensor(0.1910, grad_fn=<NllLossBackward>) average train loss tensor(0.2426, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5421644102291833 val_avg_loss: tensor(0.8127)\n",
      "epoch: 95 train_loss: tensor(0.1732, grad_fn=<NllLossBackward>) average train loss tensor(0.2380, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5831047399935109 val_avg_loss: tensor(0.7790)\n",
      "epoch: 96 train_loss: tensor(0.2209, grad_fn=<NllLossBackward>) average train loss tensor(0.2348, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5744329410376663 val_avg_loss: tensor(0.8191)\n",
      "epoch: 97 train_loss: tensor(0.2001, grad_fn=<NllLossBackward>) average train loss tensor(0.2383, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.5441701324366576 val_avg_loss: tensor(0.8738)\n",
      "epoch: 98 train_loss: tensor(0.1962, grad_fn=<NllLossBackward>) average train loss tensor(0.2380, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5528124354776863 val_avg_loss: tensor(0.8742)\n",
      "epoch: 99 train_loss: tensor(0.1968, grad_fn=<NllLossBackward>) average train loss tensor(0.2411, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5534908415184497 val_avg_loss: tensor(0.8810)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.55      0.71     41251\n",
      "           1       0.04      0.70      0.08      1128\n",
      "\n",
      "    accuracy                           0.56     42379\n",
      "   macro avg       0.51      0.63      0.39     42379\n",
      "weighted avg       0.96      0.56      0.69     42379\n",
      "\n",
      "train with weight tensor([  1., 100.])\n",
      "epoch: 0 train_loss: tensor(0.6013, grad_fn=<NllLossBackward>) average train loss tensor(0.5626, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17697548889478806 val_avg_loss: tensor(1.0359)\n",
      "epoch: 1 train_loss: tensor(0.5615, grad_fn=<NllLossBackward>) average train loss tensor(0.5338, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25183612069728345 val_avg_loss: tensor(0.9874)\n",
      "epoch: 2 train_loss: tensor(0.5165, grad_fn=<NllLossBackward>) average train loss tensor(0.5241, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2615697725864968 val_avg_loss: tensor(1.0114)\n",
      "epoch: 3 train_loss: tensor(0.4740, grad_fn=<NllLossBackward>) average train loss tensor(0.5119, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2845470902280034 val_avg_loss: tensor(1.0065)\n",
      "epoch: 4 train_loss: tensor(0.4799, grad_fn=<NllLossBackward>) average train loss tensor(0.5057, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2995310149544288 val_avg_loss: tensor(0.9950)\n",
      "epoch: 5 train_loss: tensor(0.4678, grad_fn=<NllLossBackward>) average train loss tensor(0.4962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3291744093443058 val_avg_loss: tensor(0.9755)\n",
      "epoch: 6 train_loss: tensor(0.4548, grad_fn=<NllLossBackward>) average train loss tensor(0.4932, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3337167802259387 val_avg_loss: tensor(0.9419)\n",
      "epoch: 7 train_loss: tensor(0.4199, grad_fn=<NllLossBackward>) average train loss tensor(0.4819, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35256466979323364 val_avg_loss: tensor(0.9739)\n",
      "epoch: 8 train_loss: tensor(0.4483, grad_fn=<NllLossBackward>) average train loss tensor(0.4791, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3697607881308439 val_avg_loss: tensor(0.9203)\n",
      "epoch: 9 train_loss: tensor(0.4189, grad_fn=<NllLossBackward>) average train loss tensor(0.4731, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3425360587558623 val_avg_loss: tensor(0.9107)\n",
      "epoch: 10 train_loss: tensor(0.5075, grad_fn=<NllLossBackward>) average train loss tensor(0.4700, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35828687726749847 val_avg_loss: tensor(0.9471)\n",
      "epoch: 11 train_loss: tensor(0.4072, grad_fn=<NllLossBackward>) average train loss tensor(0.4584, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3829749579683214 val_avg_loss: tensor(0.9137)\n",
      "epoch: 12 train_loss: tensor(0.4080, grad_fn=<NllLossBackward>) average train loss tensor(0.4519, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.379228976786715 val_avg_loss: tensor(0.9123)\n",
      "epoch: 13 train_loss: tensor(0.3445, grad_fn=<NllLossBackward>) average train loss tensor(0.4542, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4274842934253606 val_avg_loss: tensor(0.8570)\n",
      "epoch: 14 train_loss: tensor(0.3753, grad_fn=<NllLossBackward>) average train loss tensor(0.4475, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40421201663569595 val_avg_loss: tensor(0.8723)\n",
      "epoch: 15 train_loss: tensor(0.3645, grad_fn=<NllLossBackward>) average train loss tensor(0.4389, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43544819042562605 val_avg_loss: tensor(0.8595)\n",
      "epoch: 16 train_loss: tensor(0.3690, grad_fn=<NllLossBackward>) average train loss tensor(0.4331, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44037400819986433 val_avg_loss: tensor(0.8107)\n",
      "epoch: 17 train_loss: tensor(0.4003, grad_fn=<NllLossBackward>) average train loss tensor(0.4351, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4059227796950122 val_avg_loss: tensor(0.8700)\n",
      "epoch: 18 train_loss: tensor(0.3537, grad_fn=<NllLossBackward>) average train loss tensor(0.4297, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4519953986372887 val_avg_loss: tensor(0.8097)\n",
      "epoch: 19 train_loss: tensor(0.3151, grad_fn=<NllLossBackward>) average train loss tensor(0.4223, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4911659735126685 val_avg_loss: tensor(0.7880)\n",
      "epoch: 20 train_loss: tensor(0.3742, grad_fn=<NllLossBackward>) average train loss tensor(0.4198, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4734389287083739 val_avg_loss: tensor(0.8027)\n",
      "epoch: 21 train_loss: tensor(0.3567, grad_fn=<NllLossBackward>) average train loss tensor(0.4124, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46105064448573874 val_avg_loss: tensor(0.8340)\n",
      "epoch: 22 train_loss: tensor(0.3882, grad_fn=<NllLossBackward>) average train loss tensor(0.4073, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4838509866383506 val_avg_loss: tensor(0.8258)\n",
      "epoch: 23 train_loss: tensor(0.3399, grad_fn=<NllLossBackward>) average train loss tensor(0.4066, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4720231247972156 val_avg_loss: tensor(0.8041)\n",
      "epoch: 24 train_loss: tensor(0.3513, grad_fn=<NllLossBackward>) average train loss tensor(0.4032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.505117541220541 val_avg_loss: tensor(0.7586)\n",
      "epoch: 25 train_loss: tensor(0.2963, grad_fn=<NllLossBackward>) average train loss tensor(0.4038, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.533876058165944 val_avg_loss: tensor(0.7008)\n",
      "epoch: 26 train_loss: tensor(0.2820, grad_fn=<NllLossBackward>) average train loss tensor(0.3966, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5312804176621538 val_avg_loss: tensor(0.7414)\n",
      "epoch: 27 train_loss: tensor(0.3255, grad_fn=<NllLossBackward>) average train loss tensor(0.3972, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5234640002359673 val_avg_loss: tensor(0.7511)\n",
      "epoch: 28 train_loss: tensor(0.3058, grad_fn=<NllLossBackward>) average train loss tensor(0.3874, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5268265345249683 val_avg_loss: tensor(0.7422)\n",
      "epoch: 29 train_loss: tensor(0.2723, grad_fn=<NllLossBackward>) average train loss tensor(0.3843, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5566469044037401 val_avg_loss: tensor(0.6981)\n",
      "epoch: 30 train_loss: tensor(0.3310, grad_fn=<NllLossBackward>) average train loss tensor(0.3796, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5098663835058844 val_avg_loss: tensor(0.7696)\n",
      "epoch: 31 train_loss: tensor(0.2486, grad_fn=<NllLossBackward>) average train loss tensor(0.3814, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.563519452555821 val_avg_loss: tensor(0.6822)\n",
      "epoch: 32 train_loss: tensor(0.2843, grad_fn=<NllLossBackward>) average train loss tensor(0.3776, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.562074152729847 val_avg_loss: tensor(0.6907)\n",
      "epoch: 33 train_loss: tensor(0.2814, grad_fn=<NllLossBackward>) average train loss tensor(0.3790, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5652597115299531 val_avg_loss: tensor(0.6578)\n",
      "epoch: 34 train_loss: tensor(0.2623, grad_fn=<NllLossBackward>) average train loss tensor(0.3633, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.588060053682565 val_avg_loss: tensor(0.6587)\n",
      "epoch: 35 train_loss: tensor(0.2663, grad_fn=<NllLossBackward>) average train loss tensor(0.3656, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5846680234787482 val_avg_loss: tensor(0.6627)\n",
      "epoch: 36 train_loss: tensor(0.2660, grad_fn=<NllLossBackward>) average train loss tensor(0.3671, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5932218387753296 val_avg_loss: tensor(0.6541)\n",
      "epoch: 37 train_loss: tensor(0.2427, grad_fn=<NllLossBackward>) average train loss tensor(0.3623, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.614930832079757 val_avg_loss: tensor(0.6252)\n",
      "epoch: 38 train_loss: tensor(0.2533, grad_fn=<NllLossBackward>) average train loss tensor(0.3564, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5915110757160134 val_avg_loss: tensor(0.6635)\n",
      "epoch: 39 train_loss: tensor(0.3050, grad_fn=<NllLossBackward>) average train loss tensor(0.3593, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5958764711087514 val_avg_loss: tensor(0.6623)\n",
      "epoch: 40 train_loss: tensor(0.2422, grad_fn=<NllLossBackward>) average train loss tensor(0.3583, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6172905052650208 val_avg_loss: tensor(0.6247)\n",
      "epoch: 41 train_loss: tensor(0.2722, grad_fn=<NllLossBackward>) average train loss tensor(0.3501, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.6008022888829897 val_avg_loss: tensor(0.6548)\n",
      "epoch: 42 train_loss: tensor(0.2510, grad_fn=<NllLossBackward>) average train loss tensor(0.3455, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6241335575022859 val_avg_loss: tensor(0.5999)\n",
      "epoch: 43 train_loss: tensor(0.2410, grad_fn=<NllLossBackward>) average train loss tensor(0.3439, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6059050821461227 val_avg_loss: tensor(0.6338)\n",
      "epoch: 44 train_loss: tensor(0.2822, grad_fn=<NllLossBackward>) average train loss tensor(0.3429, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6605315163849806 val_avg_loss: tensor(0.5740)\n",
      "epoch: 45 train_loss: tensor(0.1854, grad_fn=<NllLossBackward>) average train loss tensor(0.3401, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6675220481963248 val_avg_loss: tensor(0.5591)\n",
      "epoch: 46 train_loss: tensor(0.2205, grad_fn=<NllLossBackward>) average train loss tensor(0.3354, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6570804943515323 val_avg_loss: tensor(0.5602)\n",
      "epoch: 47 train_loss: tensor(0.2093, grad_fn=<NllLossBackward>) average train loss tensor(0.3368, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6421850573695543 val_avg_loss: tensor(0.5982)\n",
      "epoch: 48 train_loss: tensor(0.2650, grad_fn=<NllLossBackward>) average train loss tensor(0.3380, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6091201368610447 val_avg_loss: tensor(0.6271)\n",
      "epoch: 49 train_loss: tensor(0.2313, grad_fn=<NllLossBackward>) average train loss tensor(0.3274, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6769312450225644 val_avg_loss: tensor(0.5442)\n",
      "epoch: 50 train_loss: tensor(0.2410, grad_fn=<NllLossBackward>) average train loss tensor(0.3354, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6548093089107159 val_avg_loss: tensor(0.5785)\n",
      "epoch: 51 train_loss: tensor(0.2471, grad_fn=<NllLossBackward>) average train loss tensor(0.3302, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6465799486771082 val_avg_loss: tensor(0.5947)\n",
      "epoch: 52 train_loss: tensor(0.2314, grad_fn=<NllLossBackward>) average train loss tensor(0.3248, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6683479338111672 val_avg_loss: tensor(0.5669)\n",
      "epoch: 53 train_loss: tensor(0.2449, grad_fn=<NllLossBackward>) average train loss tensor(0.3293, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6585257941775065 val_avg_loss: tensor(0.5689)\n",
      "epoch: 54 train_loss: tensor(0.2164, grad_fn=<NllLossBackward>) average train loss tensor(0.3229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.659617143025691 val_avg_loss: tensor(0.5699)\n",
      "epoch: 55 train_loss: tensor(0.2298, grad_fn=<NllLossBackward>) average train loss tensor(0.3203, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6768722531929328 val_avg_loss: tensor(0.5619)\n",
      "epoch: 56 train_loss: tensor(0.3545, grad_fn=<NllLossBackward>) average train loss tensor(0.3193, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6592042002182698 val_avg_loss: tensor(0.5854)\n",
      "epoch: 57 train_loss: tensor(0.2196, grad_fn=<NllLossBackward>) average train loss tensor(0.3236, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.670117688700115 val_avg_loss: tensor(0.5758)\n",
      "epoch: 58 train_loss: tensor(0.1921, grad_fn=<NllLossBackward>) average train loss tensor(0.3136, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.698168303689939 val_avg_loss: tensor(0.5230)\n",
      "epoch: 59 train_loss: tensor(0.2108, grad_fn=<NllLossBackward>) average train loss tensor(0.3131, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6661947320296139 val_avg_loss: tensor(0.5657)\n",
      "epoch: 60 train_loss: tensor(0.2457, grad_fn=<NllLossBackward>) average train loss tensor(0.3080, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6868713683154883 val_avg_loss: tensor(0.5489)\n",
      "epoch: 61 train_loss: tensor(0.2316, grad_fn=<NllLossBackward>) average train loss tensor(0.3098, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7025926909123087 val_avg_loss: tensor(0.5277)\n",
      "epoch: 62 train_loss: tensor(0.2355, grad_fn=<NllLossBackward>) average train loss tensor(0.3104, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6846296787894877 val_avg_loss: tensor(0.5499)\n",
      "epoch: 63 train_loss: tensor(0.2020, grad_fn=<NllLossBackward>) average train loss tensor(0.3143, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7094652390643896 val_avg_loss: tensor(0.5206)\n",
      "epoch: 64 train_loss: tensor(0.2010, grad_fn=<NllLossBackward>) average train loss tensor(0.3011, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6956906468454119 val_avg_loss: tensor(0.5601)\n",
      "epoch: 65 train_loss: tensor(0.1819, grad_fn=<NllLossBackward>) average train loss tensor(0.3092, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7155708934312598 val_avg_loss: tensor(0.5037)\n",
      "epoch: 66 train_loss: tensor(0.2397, grad_fn=<NllLossBackward>) average train loss tensor(0.3059, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7177830870424446 val_avg_loss: tensor(0.5107)\n",
      "epoch: 67 train_loss: tensor(0.1951, grad_fn=<NllLossBackward>) average train loss tensor(0.3028, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7033595846975194 val_avg_loss: tensor(0.5336)\n",
      "epoch: 68 train_loss: tensor(0.2023, grad_fn=<NllLossBackward>) average train loss tensor(0.2910, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7261009350204997 val_avg_loss: tensor(0.5001)\n",
      "epoch: 69 train_loss: tensor(0.1852, grad_fn=<NllLossBackward>) average train loss tensor(0.3055, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6848951420228299 val_avg_loss: tensor(0.5258)\n",
      "epoch: 70 train_loss: tensor(0.2049, grad_fn=<NllLossBackward>) average train loss tensor(0.3012, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7213225968203404 val_avg_loss: tensor(0.5099)\n",
      "epoch: 71 train_loss: tensor(0.1810, grad_fn=<NllLossBackward>) average train loss tensor(0.2913, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7368964398430817 val_avg_loss: tensor(0.4851)\n",
      "epoch: 72 train_loss: tensor(0.2326, grad_fn=<NllLossBackward>) average train loss tensor(0.2981, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7228563843907618 val_avg_loss: tensor(0.5113)\n",
      "epoch: 73 train_loss: tensor(0.2453, grad_fn=<NllLossBackward>) average train loss tensor(0.2881, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.710704067486653 val_avg_loss: tensor(0.5020)\n",
      "epoch: 74 train_loss: tensor(0.2286, grad_fn=<NllLossBackward>) average train loss tensor(0.2962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7263663982538419 val_avg_loss: tensor(0.4872)\n",
      "epoch: 75 train_loss: tensor(0.2109, grad_fn=<NllLossBackward>) average train loss tensor(0.2868, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7568946700881928 val_avg_loss: tensor(0.4574)\n",
      "epoch: 76 train_loss: tensor(0.1924, grad_fn=<NllLossBackward>) average train loss tensor(0.2884, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7215585641388668 val_avg_loss: tensor(0.5082)\n",
      "epoch: 77 train_loss: tensor(0.2061, grad_fn=<NllLossBackward>) average train loss tensor(0.2870, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7230038639648408 val_avg_loss: tensor(0.5125)\n",
      "epoch: 78 train_loss: tensor(0.1731, grad_fn=<NllLossBackward>) average train loss tensor(0.2925, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7290210305872636 val_avg_loss: tensor(0.4973)\n",
      "epoch: 79 train_loss: tensor(0.1921, grad_fn=<NllLossBackward>) average train loss tensor(0.2912, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7460401734359792 val_avg_loss: tensor(0.4712)\n",
      "epoch: 80 train_loss: tensor(0.2030, grad_fn=<NllLossBackward>) average train loss tensor(0.2829, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7172226646609444 val_avg_loss: tensor(0.5103)\n",
      "epoch: 81 train_loss: tensor(0.1634, grad_fn=<NllLossBackward>) average train loss tensor(0.2805, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.765920420021827 val_avg_loss: tensor(0.4444)\n",
      "epoch: 82 train_loss: tensor(0.1738, grad_fn=<NllLossBackward>) average train loss tensor(0.2787, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7453912633100316 val_avg_loss: tensor(0.4735)\n",
      "epoch: 83 train_loss: tensor(0.1989, grad_fn=<NllLossBackward>) average train loss tensor(0.2812, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7374863581393977 val_avg_loss: tensor(0.4982)\n",
      "epoch: 84 train_loss: tensor(0.1686, grad_fn=<NllLossBackward>) average train loss tensor(0.2734, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7572486210659823 val_avg_loss: tensor(0.4555)\n",
      "epoch: 85 train_loss: tensor(0.2270, grad_fn=<NllLossBackward>) average train loss tensor(0.2772, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.741379818895083 val_avg_loss: tensor(0.4871)\n",
      "epoch: 86 train_loss: tensor(0.1835, grad_fn=<NllLossBackward>) average train loss tensor(0.2802, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7313217119428959 val_avg_loss: tensor(0.4846)\n",
      "epoch: 87 train_loss: tensor(0.1953, grad_fn=<NllLossBackward>) average train loss tensor(0.2789, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.7319706220688434 val_avg_loss: tensor(0.5057)\n",
      "epoch: 88 train_loss: tensor(0.1831, grad_fn=<NllLossBackward>) average train loss tensor(0.2826, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7511429666991122 val_avg_loss: tensor(0.4701)\n",
      "epoch: 89 train_loss: tensor(0.1615, grad_fn=<NllLossBackward>) average train loss tensor(0.2690, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7592543432734566 val_avg_loss: tensor(0.4531)\n",
      "epoch: 90 train_loss: tensor(0.1635, grad_fn=<NllLossBackward>) average train loss tensor(0.2655, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7813762793853052 val_avg_loss: tensor(0.4267)\n",
      "epoch: 91 train_loss: tensor(0.1766, grad_fn=<NllLossBackward>) average train loss tensor(0.2821, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7385187151579506 val_avg_loss: tensor(0.4789)\n",
      "epoch: 92 train_loss: tensor(0.1904, grad_fn=<NllLossBackward>) average train loss tensor(0.2634, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7601097248031148 val_avg_loss: tensor(0.4667)\n",
      "epoch: 93 train_loss: tensor(0.1721, grad_fn=<NllLossBackward>) average train loss tensor(0.2776, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7700793440108545 val_avg_loss: tensor(0.4490)\n",
      "epoch: 94 train_loss: tensor(0.1883, grad_fn=<NllLossBackward>) average train loss tensor(0.2665, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.759637790166062 val_avg_loss: tensor(0.4817)\n",
      "epoch: 95 train_loss: tensor(0.1616, grad_fn=<NllLossBackward>) average train loss tensor(0.2683, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7494616995546117 val_avg_loss: tensor(0.4690)\n",
      "epoch: 96 train_loss: tensor(0.1575, grad_fn=<NllLossBackward>) average train loss tensor(0.2665, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.747957407899006 val_avg_loss: tensor(0.4711)\n",
      "epoch: 97 train_loss: tensor(0.1574, grad_fn=<NllLossBackward>) average train loss tensor(0.2576, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.776126006548093 val_avg_loss: tensor(0.4392)\n",
      "epoch: 98 train_loss: tensor(0.1950, grad_fn=<NllLossBackward>) average train loss tensor(0.2664, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.720939149927735 val_avg_loss: tensor(0.5117)\n",
      "epoch: 99 train_loss: tensor(0.1623, grad_fn=<NllLossBackward>) average train loss tensor(0.2583, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7964781877709937 val_avg_loss: tensor(0.4104)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.88     41251\n",
      "           1       0.06      0.42      0.10      1128\n",
      "\n",
      "    accuracy                           0.80     42379\n",
      "   macro avg       0.52      0.61      0.49     42379\n",
      "weighted avg       0.96      0.80      0.86     42379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights_list = [torch.tensor([1, 1000]).float(), \n",
    "                torch.tensor([1, 500]).float(), \n",
    "                torch.tensor([1, 400]).float(),\n",
    "                torch.tensor([1, 300]).float(),\n",
    "                torch.tensor([1, 200]).float(),\n",
    "                torch.tensor([1, 100]).float()]\n",
    "\n",
    "\n",
    "for weight in weights_list:\n",
    "    print('train with weight', weight)\n",
    "    \n",
    "    model = WtsTrivialModel(d=128, drop=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "    writer = SummaryWriter('runs/trivial_d128_drop05_wd0005_weighted' + str(int(weight[1].item())))\n",
    "\n",
    "    fit_wts_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epochs=100,\n",
    "        writer=writer,\n",
    "        train_loader=wts_train_loader,\n",
    "        val_loader=wts_val_loader,\n",
    "        weight=weight\n",
    "    )\n",
    "    \n",
    "    predictions = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "            outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "            predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "    predictions = predictions.numpy()\n",
    "\n",
    "    print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC Score with weights (1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.664015070728567"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_wts_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.5179, grad_fn=<NllLossBackward>) average train loss tensor(0.4334, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025336990826770493 val_avg_loss: tensor(1.5276)\n",
      "epoch: 1 train_loss: tensor(0.4391, grad_fn=<NllLossBackward>) average train loss tensor(0.4035, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.06497950033920302 val_avg_loss: tensor(1.4921)\n",
      "epoch: 2 train_loss: tensor(0.4528, grad_fn=<NllLossBackward>) average train loss tensor(0.3961, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07486063180249536 val_avg_loss: tensor(1.4577)\n",
      "epoch: 3 train_loss: tensor(0.3817, grad_fn=<NllLossBackward>) average train loss tensor(0.3879, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10630327699613604 val_avg_loss: tensor(1.4352)\n",
      "epoch: 4 train_loss: tensor(0.3902, grad_fn=<NllLossBackward>) average train loss tensor(0.3842, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12473822375600979 val_avg_loss: tensor(1.4462)\n",
      "epoch: 5 train_loss: tensor(0.3996, grad_fn=<NllLossBackward>) average train loss tensor(0.3790, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15464708137922897 val_avg_loss: tensor(1.3935)\n",
      "epoch: 6 train_loss: tensor(0.3654, grad_fn=<NllLossBackward>) average train loss tensor(0.3726, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17871574786892017 val_avg_loss: tensor(1.4227)\n",
      "epoch: 7 train_loss: tensor(0.3690, grad_fn=<NllLossBackward>) average train loss tensor(0.3719, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15939592366457245 val_avg_loss: tensor(1.3732)\n",
      "epoch: 8 train_loss: tensor(0.3571, grad_fn=<NllLossBackward>) average train loss tensor(0.3635, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18948175677668644 val_avg_loss: tensor(1.4026)\n",
      "epoch: 9 train_loss: tensor(0.3457, grad_fn=<NllLossBackward>) average train loss tensor(0.3600, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22083591422587973 val_avg_loss: tensor(1.3224)\n",
      "epoch: 10 train_loss: tensor(0.3594, grad_fn=<NllLossBackward>) average train loss tensor(0.3553, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22083591422587973 val_avg_loss: tensor(1.3852)\n",
      "epoch: 11 train_loss: tensor(0.3404, grad_fn=<NllLossBackward>) average train loss tensor(0.3543, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21874170427395806 val_avg_loss: tensor(1.3694)\n",
      "epoch: 12 train_loss: tensor(0.3344, grad_fn=<NllLossBackward>) average train loss tensor(0.3486, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22900628262985576 val_avg_loss: tensor(1.3327)\n",
      "epoch: 13 train_loss: tensor(0.3455, grad_fn=<NllLossBackward>) average train loss tensor(0.3468, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2444326460785181 val_avg_loss: tensor(1.3166)\n",
      "epoch: 14 train_loss: tensor(0.3239, grad_fn=<NllLossBackward>) average train loss tensor(0.3418, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24832610683420347 val_avg_loss: tensor(1.3198)\n",
      "epoch: 15 train_loss: tensor(0.3455, grad_fn=<NllLossBackward>) average train loss tensor(0.3407, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26826534524968293 val_avg_loss: tensor(1.2638)\n",
      "epoch: 16 train_loss: tensor(0.3127, grad_fn=<NllLossBackward>) average train loss tensor(0.3363, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25693891396041646 val_avg_loss: tensor(1.3242)\n",
      "epoch: 17 train_loss: tensor(0.3119, grad_fn=<NllLossBackward>) average train loss tensor(0.3328, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2687077839719199 val_avg_loss: tensor(1.3067)\n",
      "epoch: 18 train_loss: tensor(0.2908, grad_fn=<NllLossBackward>) average train loss tensor(0.3322, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2909771996578474 val_avg_loss: tensor(1.2399)\n",
      "epoch: 19 train_loss: tensor(0.2935, grad_fn=<NllLossBackward>) average train loss tensor(0.3322, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30339497979529834 val_avg_loss: tensor(1.2436)\n",
      "epoch: 20 train_loss: tensor(0.2984, grad_fn=<NllLossBackward>) average train loss tensor(0.3248, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3021561513730348 val_avg_loss: tensor(1.2464)\n",
      "epoch: 21 train_loss: tensor(0.3240, grad_fn=<NllLossBackward>) average train loss tensor(0.3272, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2760522667610536 val_avg_loss: tensor(1.2774)\n",
      "epoch: 22 train_loss: tensor(0.2933, grad_fn=<NllLossBackward>) average train loss tensor(0.3257, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2969648703654544 val_avg_loss: tensor(1.2200)\n",
      "epoch: 23 train_loss: tensor(0.3069, grad_fn=<NllLossBackward>) average train loss tensor(0.3207, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2807716131315813 val_avg_loss: tensor(1.2777)\n",
      "epoch: 24 train_loss: tensor(0.2969, grad_fn=<NllLossBackward>) average train loss tensor(0.3201, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3172875556735392 val_avg_loss: tensor(1.1640)\n",
      "epoch: 25 train_loss: tensor(0.2779, grad_fn=<NllLossBackward>) average train loss tensor(0.3158, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33731528183346604 val_avg_loss: tensor(1.1739)\n",
      "epoch: 26 train_loss: tensor(0.2785, grad_fn=<NllLossBackward>) average train loss tensor(0.3063, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3370498186001239 val_avg_loss: tensor(1.2431)\n",
      "epoch: 27 train_loss: tensor(0.3018, grad_fn=<NllLossBackward>) average train loss tensor(0.3137, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3189688228180397 val_avg_loss: tensor(1.2233)\n",
      "epoch: 28 train_loss: tensor(0.2859, grad_fn=<NllLossBackward>) average train loss tensor(0.3101, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.353685514556234 val_avg_loss: tensor(1.1412)\n",
      "epoch: 29 train_loss: tensor(0.2538, grad_fn=<NllLossBackward>) average train loss tensor(0.3059, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.377488717812583 val_avg_loss: tensor(1.0891)\n",
      "epoch: 30 train_loss: tensor(0.2857, grad_fn=<NllLossBackward>) average train loss tensor(0.3066, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3488776804412589 val_avg_loss: tensor(1.1645)\n",
      "epoch: 31 train_loss: tensor(0.2747, grad_fn=<NllLossBackward>) average train loss tensor(0.3082, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3579034303748931 val_avg_loss: tensor(1.1579)\n",
      "epoch: 32 train_loss: tensor(0.2884, grad_fn=<NllLossBackward>) average train loss tensor(0.3112, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34982154971536444 val_avg_loss: tensor(1.1122)\n",
      "epoch: 33 train_loss: tensor(0.2590, grad_fn=<NllLossBackward>) average train loss tensor(0.3052, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36247529717134175 val_avg_loss: tensor(1.1093)\n",
      "epoch: 34 train_loss: tensor(0.2925, grad_fn=<NllLossBackward>) average train loss tensor(0.3052, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3484352417190219 val_avg_loss: tensor(1.2095)\n",
      "epoch: 35 train_loss: tensor(0.2785, grad_fn=<NllLossBackward>) average train loss tensor(0.2999, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38150016222753147 val_avg_loss: tensor(1.1650)\n",
      "epoch: 36 train_loss: tensor(0.2562, grad_fn=<NllLossBackward>) average train loss tensor(0.3000, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37916998495708343 val_avg_loss: tensor(1.1299)\n",
      "epoch: 37 train_loss: tensor(0.2598, grad_fn=<NllLossBackward>) average train loss tensor(0.2964, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.373978703949503 val_avg_loss: tensor(1.1399)\n",
      "epoch: 38 train_loss: tensor(0.2703, grad_fn=<NllLossBackward>) average train loss tensor(0.2992, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34365690351886263 val_avg_loss: tensor(1.1244)\n",
      "epoch: 39 train_loss: tensor(0.2697, grad_fn=<NllLossBackward>) average train loss tensor(0.2960, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3932395363242191 val_avg_loss: tensor(1.0877)\n",
      "epoch: 40 train_loss: tensor(0.2298, grad_fn=<NllLossBackward>) average train loss tensor(0.2943, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37312332241984486 val_avg_loss: tensor(1.0804)\n",
      "epoch: 41 train_loss: tensor(0.2691, grad_fn=<NllLossBackward>) average train loss tensor(0.2867, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4132082706545144 val_avg_loss: tensor(1.0779)\n",
      "epoch: 42 train_loss: tensor(0.2379, grad_fn=<NllLossBackward>) average train loss tensor(0.2850, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4494882458779459 val_avg_loss: tensor(0.9868)\n",
      "epoch: 43 train_loss: tensor(0.2668, grad_fn=<NllLossBackward>) average train loss tensor(0.2933, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3677550659233696 val_avg_loss: tensor(1.1596)\n",
      "epoch: 44 train_loss: tensor(0.2442, grad_fn=<NllLossBackward>) average train loss tensor(0.2879, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41733769872872606 val_avg_loss: tensor(1.0549)\n",
      "epoch: 45 train_loss: tensor(0.2246, grad_fn=<NllLossBackward>) average train loss tensor(0.2831, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45618381854113205 val_avg_loss: tensor(0.9702)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.2364, grad_fn=<NllLossBackward>) average train loss tensor(0.2874, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.442025779429549 val_avg_loss: tensor(0.9872)\n",
      "epoch: 47 train_loss: tensor(0.2486, grad_fn=<NllLossBackward>) average train loss tensor(0.2880, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4347402884700469 val_avg_loss: tensor(1.0183)\n",
      "epoch: 48 train_loss: tensor(0.2358, grad_fn=<NllLossBackward>) average train loss tensor(0.2749, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45928088959679086 val_avg_loss: tensor(1.0019)\n",
      "epoch: 49 train_loss: tensor(0.2290, grad_fn=<NllLossBackward>) average train loss tensor(0.2846, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4297259829513612 val_avg_loss: tensor(1.0017)\n",
      "epoch: 50 train_loss: tensor(0.2281, grad_fn=<NllLossBackward>) average train loss tensor(0.2786, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4459782320148659 val_avg_loss: tensor(0.9995)\n",
      "epoch: 51 train_loss: tensor(0.2469, grad_fn=<NllLossBackward>) average train loss tensor(0.2775, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4344748252367047 val_avg_loss: tensor(1.0557)\n",
      "epoch: 52 train_loss: tensor(0.2174, grad_fn=<NllLossBackward>) average train loss tensor(0.2754, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4599297997227384 val_avg_loss: tensor(0.9149)\n",
      "epoch: 53 train_loss: tensor(0.2054, grad_fn=<NllLossBackward>) average train loss tensor(0.2650, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48750848007550956 val_avg_loss: tensor(0.9637)\n",
      "epoch: 54 train_loss: tensor(0.2384, grad_fn=<NllLossBackward>) average train loss tensor(0.2715, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4409934224109961 val_avg_loss: tensor(1.0239)\n",
      "epoch: 55 train_loss: tensor(0.2219, grad_fn=<NllLossBackward>) average train loss tensor(0.2741, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47308497773058433 val_avg_loss: tensor(0.9335)\n",
      "epoch: 56 train_loss: tensor(0.2605, grad_fn=<NllLossBackward>) average train loss tensor(0.2752, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47659499159366425 val_avg_loss: tensor(0.9684)\n",
      "epoch: 57 train_loss: tensor(0.2294, grad_fn=<NllLossBackward>) average train loss tensor(0.2731, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4348582721293101 val_avg_loss: tensor(1.0017)\n",
      "epoch: 58 train_loss: tensor(0.2073, grad_fn=<NllLossBackward>) average train loss tensor(0.2667, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4821697194938501 val_avg_loss: tensor(0.9622)\n",
      "epoch: 59 train_loss: tensor(0.2310, grad_fn=<NllLossBackward>) average train loss tensor(0.2641, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48004601362711263 val_avg_loss: tensor(0.9736)\n",
      "epoch: 60 train_loss: tensor(0.2505, grad_fn=<NllLossBackward>) average train loss tensor(0.2716, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48883579624222046 val_avg_loss: tensor(0.9476)\n",
      "epoch: 61 train_loss: tensor(0.2048, grad_fn=<NllLossBackward>) average train loss tensor(0.2714, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4513464885113412 val_avg_loss: tensor(1.0047)\n",
      "epoch: 62 train_loss: tensor(0.2039, grad_fn=<NllLossBackward>) average train loss tensor(0.2697, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48429342536058756 val_avg_loss: tensor(0.9638)\n",
      "epoch: 63 train_loss: tensor(0.2089, grad_fn=<NllLossBackward>) average train loss tensor(0.2668, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46700881927852994 val_avg_loss: tensor(0.9418)\n",
      "epoch: 64 train_loss: tensor(0.2162, grad_fn=<NllLossBackward>) average train loss tensor(0.2684, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4673922661711353 val_avg_loss: tensor(0.9525)\n",
      "epoch: 65 train_loss: tensor(0.2104, grad_fn=<NllLossBackward>) average train loss tensor(0.2606, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5190986048432292 val_avg_loss: tensor(0.8822)\n",
      "epoch: 66 train_loss: tensor(0.1950, grad_fn=<NllLossBackward>) average train loss tensor(0.2547, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5208388638173613 val_avg_loss: tensor(0.8734)\n",
      "epoch: 67 train_loss: tensor(0.2303, grad_fn=<NllLossBackward>) average train loss tensor(0.2635, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.49677019732767014 val_avg_loss: tensor(0.8904)\n",
      "epoch: 68 train_loss: tensor(0.1946, grad_fn=<NllLossBackward>) average train loss tensor(0.2643, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48364451523464 val_avg_loss: tensor(0.9307)\n",
      "epoch: 69 train_loss: tensor(0.2054, grad_fn=<NllLossBackward>) average train loss tensor(0.2560, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4914314367460107 val_avg_loss: tensor(0.9293)\n",
      "epoch: 70 train_loss: tensor(0.2159, grad_fn=<NllLossBackward>) average train loss tensor(0.2550, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5029643394389877 val_avg_loss: tensor(0.9277)\n",
      "epoch: 71 train_loss: tensor(0.2044, grad_fn=<NllLossBackward>) average train loss tensor(0.2559, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5146447217060437 val_avg_loss: tensor(0.9105)\n",
      "epoch: 72 train_loss: tensor(0.2045, grad_fn=<NllLossBackward>) average train loss tensor(0.2617, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47571011414919034 val_avg_loss: tensor(0.9659)\n",
      "epoch: 73 train_loss: tensor(0.2013, grad_fn=<NllLossBackward>) average train loss tensor(0.2571, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5321652951066277 val_avg_loss: tensor(0.8317)\n",
      "epoch: 74 train_loss: tensor(0.1926, grad_fn=<NllLossBackward>) average train loss tensor(0.2508, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5286847771583636 val_avg_loss: tensor(0.8871)\n",
      "epoch: 75 train_loss: tensor(0.1695, grad_fn=<NllLossBackward>) average train loss tensor(0.2553, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5316933604695749 val_avg_loss: tensor(0.8579)\n",
      "epoch: 76 train_loss: tensor(0.2070, grad_fn=<NllLossBackward>) average train loss tensor(0.2510, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5214582780284931 val_avg_loss: tensor(0.8994)\n",
      "epoch: 77 train_loss: tensor(0.1825, grad_fn=<NllLossBackward>) average train loss tensor(0.2522, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5299825974102587 val_avg_loss: tensor(0.8214)\n",
      "epoch: 78 train_loss: tensor(0.1841, grad_fn=<NllLossBackward>) average train loss tensor(0.2531, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5380939739846031 val_avg_loss: tensor(0.8345)\n",
      "epoch: 79 train_loss: tensor(0.1880, grad_fn=<NllLossBackward>) average train loss tensor(0.2503, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5399817125328142 val_avg_loss: tensor(0.8688)\n",
      "epoch: 80 train_loss: tensor(0.1968, grad_fn=<NllLossBackward>) average train loss tensor(0.2475, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5303070524732325 val_avg_loss: tensor(0.8759)\n",
      "epoch: 81 train_loss: tensor(0.2189, grad_fn=<NllLossBackward>) average train loss tensor(0.2473, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5652302156151373 val_avg_loss: tensor(0.8298)\n",
      "epoch: 82 train_loss: tensor(0.1848, grad_fn=<NllLossBackward>) average train loss tensor(0.2461, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5431967672477361 val_avg_loss: tensor(0.8747)\n",
      "epoch: 83 train_loss: tensor(0.1878, grad_fn=<NllLossBackward>) average train loss tensor(0.2453, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5328731970622069 val_avg_loss: tensor(0.8812)\n",
      "epoch: 84 train_loss: tensor(0.2274, grad_fn=<NllLossBackward>) average train loss tensor(0.2453, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5434032386514468 val_avg_loss: tensor(0.8678)\n",
      "epoch: 85 train_loss: tensor(0.1902, grad_fn=<NllLossBackward>) average train loss tensor(0.2468, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5038492168834616 val_avg_loss: tensor(0.9350)\n",
      "epoch: 86 train_loss: tensor(0.1875, grad_fn=<NllLossBackward>) average train loss tensor(0.2481, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5534613456036339 val_avg_loss: tensor(0.8001)\n",
      "epoch: 87 train_loss: tensor(0.1905, grad_fn=<NllLossBackward>) average train loss tensor(0.2442, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5428723121847624 val_avg_loss: tensor(0.8412)\n",
      "epoch: 88 train_loss: tensor(0.1766, grad_fn=<NllLossBackward>) average train loss tensor(0.2472, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.556941863551898 val_avg_loss: tensor(0.8171)\n",
      "epoch: 89 train_loss: tensor(0.1859, grad_fn=<NllLossBackward>) average train loss tensor(0.2445, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5371796006253134 val_avg_loss: tensor(0.9055)\n",
      "epoch: 90 train_loss: tensor(0.2042, grad_fn=<NllLossBackward>) average train loss tensor(0.2432, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5482700645960534 val_avg_loss: tensor(0.8316)\n",
      "epoch: 91 train_loss: tensor(0.1916, grad_fn=<NllLossBackward>) average train loss tensor(0.2404, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5495973807627643 val_avg_loss: tensor(0.8557)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 train_loss: tensor(0.1984, grad_fn=<NllLossBackward>) average train loss tensor(0.2394, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5348789192696811 val_avg_loss: tensor(0.9184)\n",
      "epoch: 93 train_loss: tensor(0.1619, grad_fn=<NllLossBackward>) average train loss tensor(0.2417, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5772055570303513 val_avg_loss: tensor(0.7990)\n",
      "epoch: 94 train_loss: tensor(0.1903, grad_fn=<NllLossBackward>) average train loss tensor(0.2420, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5399522166179984 val_avg_loss: tensor(0.8480)\n",
      "epoch: 95 train_loss: tensor(0.1933, grad_fn=<NllLossBackward>) average train loss tensor(0.2429, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5180957437394921 val_avg_loss: tensor(0.8942)\n",
      "epoch: 96 train_loss: tensor(0.1803, grad_fn=<NllLossBackward>) average train loss tensor(0.2414, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5637849157891632 val_avg_loss: tensor(0.8178)\n",
      "epoch: 97 train_loss: tensor(0.1576, grad_fn=<NllLossBackward>) average train loss tensor(0.2367, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.581187505530484 val_avg_loss: tensor(0.7581)\n",
      "epoch: 98 train_loss: tensor(0.1897, grad_fn=<NllLossBackward>) average train loss tensor(0.2365, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.575494793971035 val_avg_loss: tensor(0.8450)\n",
      "epoch: 99 train_loss: tensor(0.1860, grad_fn=<NllLossBackward>) average train loss tensor(0.2360, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5621921363891101 val_avg_loss: tensor(0.8339)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.56      0.72     41251\n",
      "           1       0.04      0.69      0.08      1128\n",
      "\n",
      "    accuracy                           0.57     42379\n",
      "   macro avg       0.51      0.63      0.40     42379\n",
      "weighted avg       0.96      0.57      0.70     42379\n",
      "\n",
      "roc auc score: 0.6797052985691643\n"
     ]
    }
   ],
   "source": [
    "model = WtsTrivialModel(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 200]).float()\n",
    ")\n",
    "    \n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.4383, grad_fn=<NllLossBackward>) average train loss tensor(0.3738, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(1.8072)\n",
      "epoch: 1 train_loss: tensor(0.3928, grad_fn=<NllLossBackward>) average train loss tensor(0.3268, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(1.8049)\n",
      "epoch: 2 train_loss: tensor(0.3436, grad_fn=<NllLossBackward>) average train loss tensor(0.3198, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.04279857239772292 val_avg_loss: tensor(1.7705)\n",
      "epoch: 3 train_loss: tensor(0.3544, grad_fn=<NllLossBackward>) average train loss tensor(0.3151, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07987493732118102 val_avg_loss: tensor(1.7618)\n",
      "epoch: 4 train_loss: tensor(0.3098, grad_fn=<NllLossBackward>) average train loss tensor(0.3112, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.0755390378432587 val_avg_loss: tensor(1.7231)\n",
      "epoch: 5 train_loss: tensor(0.3114, grad_fn=<NllLossBackward>) average train loss tensor(0.3056, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09503583753650119 val_avg_loss: tensor(1.7221)\n",
      "epoch: 6 train_loss: tensor(0.3345, grad_fn=<NllLossBackward>) average train loss tensor(0.3054, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10137745922189777 val_avg_loss: tensor(1.7297)\n",
      "epoch: 7 train_loss: tensor(0.3224, grad_fn=<NllLossBackward>) average train loss tensor(0.2999, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12842521310798455 val_avg_loss: tensor(1.7310)\n",
      "epoch: 8 train_loss: tensor(0.3158, grad_fn=<NllLossBackward>) average train loss tensor(0.2941, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14326165826033094 val_avg_loss: tensor(1.7028)\n",
      "epoch: 9 train_loss: tensor(0.3191, grad_fn=<NllLossBackward>) average train loss tensor(0.2934, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1367430610860396 val_avg_loss: tensor(1.7748)\n",
      "epoch: 10 train_loss: tensor(0.2885, grad_fn=<NllLossBackward>) average train loss tensor(0.2944, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13910273427130343 val_avg_loss: tensor(1.6327)\n",
      "epoch: 11 train_loss: tensor(0.2771, grad_fn=<NllLossBackward>) average train loss tensor(0.2860, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18744653865439637 val_avg_loss: tensor(1.6389)\n",
      "epoch: 12 train_loss: tensor(0.3047, grad_fn=<NllLossBackward>) average train loss tensor(0.2853, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17550069315399816 val_avg_loss: tensor(1.7353)\n",
      "epoch: 13 train_loss: tensor(0.3016, grad_fn=<NllLossBackward>) average train loss tensor(0.2877, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13759844261569773 val_avg_loss: tensor(1.6970)\n",
      "epoch: 14 train_loss: tensor(0.2767, grad_fn=<NllLossBackward>) average train loss tensor(0.2812, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18930478128779163 val_avg_loss: tensor(1.6312)\n",
      "epoch: 15 train_loss: tensor(0.3047, grad_fn=<NllLossBackward>) average train loss tensor(0.2810, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17600212370586674 val_avg_loss: tensor(1.7232)\n",
      "epoch: 16 train_loss: tensor(0.2922, grad_fn=<NllLossBackward>) average train loss tensor(0.2765, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20850662183287616 val_avg_loss: tensor(1.6548)\n",
      "epoch: 17 train_loss: tensor(0.2828, grad_fn=<NllLossBackward>) average train loss tensor(0.2766, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19449606229537209 val_avg_loss: tensor(1.6132)\n",
      "epoch: 18 train_loss: tensor(0.2716, grad_fn=<NllLossBackward>) average train loss tensor(0.2766, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17440934430581365 val_avg_loss: tensor(1.6723)\n",
      "epoch: 19 train_loss: tensor(0.2864, grad_fn=<NllLossBackward>) average train loss tensor(0.2742, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21717842078872077 val_avg_loss: tensor(1.5600)\n",
      "epoch: 20 train_loss: tensor(0.2897, grad_fn=<NllLossBackward>) average train loss tensor(0.2715, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2241099607704333 val_avg_loss: tensor(1.5076)\n",
      "epoch: 21 train_loss: tensor(0.2642, grad_fn=<NllLossBackward>) average train loss tensor(0.2698, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23900539775241128 val_avg_loss: tensor(1.5826)\n",
      "epoch: 22 train_loss: tensor(0.2605, grad_fn=<NllLossBackward>) average train loss tensor(0.2675, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24844409049346666 val_avg_loss: tensor(1.6032)\n",
      "epoch: 23 train_loss: tensor(0.2577, grad_fn=<NllLossBackward>) average train loss tensor(0.2650, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2648438191310504 val_avg_loss: tensor(1.4932)\n",
      "epoch: 24 train_loss: tensor(0.2472, grad_fn=<NllLossBackward>) average train loss tensor(0.2649, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24853257823791405 val_avg_loss: tensor(1.5812)\n",
      "epoch: 25 train_loss: tensor(0.2749, grad_fn=<NllLossBackward>) average train loss tensor(0.2663, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23104150075214583 val_avg_loss: tensor(1.5684)\n",
      "epoch: 26 train_loss: tensor(0.2575, grad_fn=<NllLossBackward>) average train loss tensor(0.2675, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24729374981565053 val_avg_loss: tensor(1.5413)\n",
      "epoch: 27 train_loss: tensor(0.2586, grad_fn=<NllLossBackward>) average train loss tensor(0.2602, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26496180279031356 val_avg_loss: tensor(1.5098)\n",
      "epoch: 28 train_loss: tensor(0.2689, grad_fn=<NllLossBackward>) average train loss tensor(0.2616, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2573518567678377 val_avg_loss: tensor(1.5342)\n",
      "epoch: 29 train_loss: tensor(0.2410, grad_fn=<NllLossBackward>) average train loss tensor(0.2646, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2541073061380999 val_avg_loss: tensor(1.4665)\n",
      "epoch: 30 train_loss: tensor(0.2373, grad_fn=<NllLossBackward>) average train loss tensor(0.2596, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2686782880571041 val_avg_loss: tensor(1.5056)\n",
      "epoch: 31 train_loss: tensor(0.2328, grad_fn=<NllLossBackward>) average train loss tensor(0.2591, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.299796478187771 val_avg_loss: tensor(1.3472)\n",
      "epoch: 32 train_loss: tensor(0.2682, grad_fn=<NllLossBackward>) average train loss tensor(0.2540, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2835147332094505 val_avg_loss: tensor(1.5239)\n",
      "epoch: 33 train_loss: tensor(0.2690, grad_fn=<NllLossBackward>) average train loss tensor(0.2564, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27366309766097396 val_avg_loss: tensor(1.5201)\n",
      "epoch: 34 train_loss: tensor(0.2790, grad_fn=<NllLossBackward>) average train loss tensor(0.2569, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28109606819455507 val_avg_loss: tensor(1.4517)\n",
      "epoch: 35 train_loss: tensor(0.2323, grad_fn=<NllLossBackward>) average train loss tensor(0.2513, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3061380998731676 val_avg_loss: tensor(1.4585)\n",
      "epoch: 36 train_loss: tensor(0.2409, grad_fn=<NllLossBackward>) average train loss tensor(0.2542, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29587352151726987 val_avg_loss: tensor(1.3948)\n",
      "epoch: 37 train_loss: tensor(0.2340, grad_fn=<NllLossBackward>) average train loss tensor(0.2526, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30643305902132556 val_avg_loss: tensor(1.3906)\n",
      "epoch: 38 train_loss: tensor(0.2399, grad_fn=<NllLossBackward>) average train loss tensor(0.2521, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2825118721057134 val_avg_loss: tensor(1.4291)\n",
      "epoch: 39 train_loss: tensor(0.2303, grad_fn=<NllLossBackward>) average train loss tensor(0.2453, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3158127599327493 val_avg_loss: tensor(1.4331)\n",
      "epoch: 40 train_loss: tensor(0.2579, grad_fn=<NllLossBackward>) average train loss tensor(0.2507, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2716573754534997 val_avg_loss: tensor(1.5891)\n",
      "epoch: 41 train_loss: tensor(0.2550, grad_fn=<NllLossBackward>) average train loss tensor(0.2529, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2995900067840604 val_avg_loss: tensor(1.4622)\n",
      "epoch: 42 train_loss: tensor(0.2180, grad_fn=<NllLossBackward>) average train loss tensor(0.2442, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32118101642922453 val_avg_loss: tensor(1.4164)\n",
      "epoch: 43 train_loss: tensor(0.2223, grad_fn=<NllLossBackward>) average train loss tensor(0.2449, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3280240686664897 val_avg_loss: tensor(1.4281)\n",
      "epoch: 44 train_loss: tensor(0.2489, grad_fn=<NllLossBackward>) average train loss tensor(0.2426, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30743592012506266 val_avg_loss: tensor(1.5695)\n",
      "epoch: 45 train_loss: tensor(0.2558, grad_fn=<NllLossBackward>) average train loss tensor(0.2445, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.28985635489484707 val_avg_loss: tensor(1.4446)\n",
      "epoch: 46 train_loss: tensor(0.2377, grad_fn=<NllLossBackward>) average train loss tensor(0.2437, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31062147892516884 val_avg_loss: tensor(1.4951)\n",
      "epoch: 47 train_loss: tensor(0.2079, grad_fn=<NllLossBackward>) average train loss tensor(0.2439, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3378167123853346 val_avg_loss: tensor(1.3291)\n",
      "epoch: 48 train_loss: tensor(0.2355, grad_fn=<NllLossBackward>) average train loss tensor(0.2455, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33607645341120257 val_avg_loss: tensor(1.3915)\n",
      "epoch: 49 train_loss: tensor(0.2134, grad_fn=<NllLossBackward>) average train loss tensor(0.2426, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35486535114886586 val_avg_loss: tensor(1.3737)\n",
      "epoch: 50 train_loss: tensor(0.2200, grad_fn=<NllLossBackward>) average train loss tensor(0.2410, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33067870099991153 val_avg_loss: tensor(1.3847)\n",
      "epoch: 51 train_loss: tensor(0.2188, grad_fn=<NllLossBackward>) average train loss tensor(0.2457, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.341415213992862 val_avg_loss: tensor(1.2986)\n",
      "epoch: 52 train_loss: tensor(0.2143, grad_fn=<NllLossBackward>) average train loss tensor(0.2412, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3344246821815179 val_avg_loss: tensor(1.3269)\n",
      "epoch: 53 train_loss: tensor(0.2305, grad_fn=<NllLossBackward>) average train loss tensor(0.2451, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3394094917853877 val_avg_loss: tensor(1.4026)\n",
      "epoch: 54 train_loss: tensor(0.2077, grad_fn=<NllLossBackward>) average train loss tensor(0.2410, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32413060791080434 val_avg_loss: tensor(1.3746)\n",
      "epoch: 55 train_loss: tensor(0.2189, grad_fn=<NllLossBackward>) average train loss tensor(0.2367, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3636846296787895 val_avg_loss: tensor(1.3243)\n",
      "epoch: 56 train_loss: tensor(0.2270, grad_fn=<NllLossBackward>) average train loss tensor(0.2350, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33489661681857064 val_avg_loss: tensor(1.4283)\n",
      "epoch: 57 train_loss: tensor(0.2352, grad_fn=<NllLossBackward>) average train loss tensor(0.2376, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35634014688965576 val_avg_loss: tensor(1.3308)\n",
      "epoch: 58 train_loss: tensor(0.2390, grad_fn=<NllLossBackward>) average train loss tensor(0.2436, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3314750906999381 val_avg_loss: tensor(1.3718)\n",
      "epoch: 59 train_loss: tensor(0.2187, grad_fn=<NllLossBackward>) average train loss tensor(0.2352, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34687195823378464 val_avg_loss: tensor(1.3349)\n",
      "epoch: 60 train_loss: tensor(0.2245, grad_fn=<NllLossBackward>) average train loss tensor(0.2352, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3757189629236351 val_avg_loss: tensor(1.3011)\n",
      "epoch: 61 train_loss: tensor(0.2122, grad_fn=<NllLossBackward>) average train loss tensor(0.2321, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38533463115358524 val_avg_loss: tensor(1.2613)\n",
      "epoch: 62 train_loss: tensor(0.1811, grad_fn=<NllLossBackward>) average train loss tensor(0.2300, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4140046603545409 val_avg_loss: tensor(1.2163)\n",
      "epoch: 63 train_loss: tensor(0.2171, grad_fn=<NllLossBackward>) average train loss tensor(0.2348, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38925758782408637 val_avg_loss: tensor(1.2447)\n",
      "epoch: 64 train_loss: tensor(0.1963, grad_fn=<NllLossBackward>) average train loss tensor(0.2311, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3698197799604755 val_avg_loss: tensor(1.3371)\n",
      "epoch: 65 train_loss: tensor(0.1996, grad_fn=<NllLossBackward>) average train loss tensor(0.2324, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3665162375011061 val_avg_loss: tensor(1.3237)\n",
      "epoch: 66 train_loss: tensor(0.2180, grad_fn=<NllLossBackward>) average train loss tensor(0.2317, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3872518656166121 val_avg_loss: tensor(1.2734)\n",
      "epoch: 67 train_loss: tensor(0.2087, grad_fn=<NllLossBackward>) average train loss tensor(0.2306, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38155915405716306 val_avg_loss: tensor(1.3009)\n",
      "epoch: 68 train_loss: tensor(0.2091, grad_fn=<NllLossBackward>) average train loss tensor(0.2317, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36586732737515854 val_avg_loss: tensor(1.3847)\n",
      "epoch: 69 train_loss: tensor(0.1992, grad_fn=<NllLossBackward>) average train loss tensor(0.2278, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39406542193906147 val_avg_loss: tensor(1.2438)\n",
      "epoch: 70 train_loss: tensor(0.1880, grad_fn=<NllLossBackward>) average train loss tensor(0.2277, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39961065392443146 val_avg_loss: tensor(1.2355)\n",
      "epoch: 71 train_loss: tensor(0.1920, grad_fn=<NllLossBackward>) average train loss tensor(0.2316, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.420700233017727 val_avg_loss: tensor(1.1254)\n",
      "epoch: 72 train_loss: tensor(0.3543, grad_fn=<NllLossBackward>) average train loss tensor(0.2296, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3591127628823408 val_avg_loss: tensor(1.3340)\n",
      "epoch: 73 train_loss: tensor(0.2031, grad_fn=<NllLossBackward>) average train loss tensor(0.2364, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3664277497566587 val_avg_loss: tensor(1.2735)\n",
      "epoch: 74 train_loss: tensor(0.2060, grad_fn=<NllLossBackward>) average train loss tensor(0.2358, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3839778190720585 val_avg_loss: tensor(1.2718)\n",
      "epoch: 75 train_loss: tensor(0.2099, grad_fn=<NllLossBackward>) average train loss tensor(0.2278, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3663687579270271 val_avg_loss: tensor(1.2635)\n",
      "epoch: 76 train_loss: tensor(0.2040, grad_fn=<NllLossBackward>) average train loss tensor(0.2269, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3993156947762735 val_avg_loss: tensor(1.1998)\n",
      "epoch: 77 train_loss: tensor(0.2272, grad_fn=<NllLossBackward>) average train loss tensor(0.2354, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3455446420670737 val_avg_loss: tensor(1.3531)\n",
      "epoch: 78 train_loss: tensor(0.1968, grad_fn=<NllLossBackward>) average train loss tensor(0.2310, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3635076541898947 val_avg_loss: tensor(1.3368)\n",
      "epoch: 79 train_loss: tensor(0.1907, grad_fn=<NllLossBackward>) average train loss tensor(0.2287, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40972775270625017 val_avg_loss: tensor(1.1893)\n",
      "epoch: 80 train_loss: tensor(0.1985, grad_fn=<NllLossBackward>) average train loss tensor(0.2233, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3965725746984043 val_avg_loss: tensor(1.2927)\n",
      "epoch: 81 train_loss: tensor(0.1988, grad_fn=<NllLossBackward>) average train loss tensor(0.2252, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38958204288706016 val_avg_loss: tensor(1.2573)\n",
      "epoch: 82 train_loss: tensor(0.1841, grad_fn=<NllLossBackward>) average train loss tensor(0.2214, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4229124266289119 val_avg_loss: tensor(1.1917)\n",
      "epoch: 83 train_loss: tensor(0.1990, grad_fn=<NllLossBackward>) average train loss tensor(0.2265, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3780786361088989 val_avg_loss: tensor(1.2744)\n",
      "epoch: 84 train_loss: tensor(0.1866, grad_fn=<NllLossBackward>) average train loss tensor(0.2233, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40173435979116895 val_avg_loss: tensor(1.2791)\n",
      "epoch: 85 train_loss: tensor(0.1856, grad_fn=<NllLossBackward>) average train loss tensor(0.2249, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4111730525322243 val_avg_loss: tensor(1.2303)\n",
      "epoch: 86 train_loss: tensor(0.1858, grad_fn=<NllLossBackward>) average train loss tensor(0.2222, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4336194437070466 val_avg_loss: tensor(1.1998)\n",
      "epoch: 87 train_loss: tensor(0.1992, grad_fn=<NllLossBackward>) average train loss tensor(0.2207, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4466566380556293 val_avg_loss: tensor(1.1975)\n",
      "epoch: 88 train_loss: tensor(0.1907, grad_fn=<NllLossBackward>) average train loss tensor(0.2247, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4275432852549922 val_avg_loss: tensor(1.2222)\n",
      "epoch: 89 train_loss: tensor(0.2108, grad_fn=<NllLossBackward>) average train loss tensor(0.2266, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4083414447099077 val_avg_loss: tensor(1.2593)\n",
      "epoch: 90 train_loss: tensor(0.1755, grad_fn=<NllLossBackward>) average train loss tensor(0.2199, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.403474618765301 val_avg_loss: tensor(1.2537)\n",
      "epoch: 91 train_loss: tensor(0.2084, grad_fn=<NllLossBackward>) average train loss tensor(0.2254, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.397486948057694 val_avg_loss: tensor(1.2477)\n",
      "epoch: 92 train_loss: tensor(0.1650, grad_fn=<NllLossBackward>) average train loss tensor(0.2142, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4518774149780255 val_avg_loss: tensor(1.2052)\n",
      "epoch: 93 train_loss: tensor(0.2082, grad_fn=<NllLossBackward>) average train loss tensor(0.2212, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42668790372533405 val_avg_loss: tensor(1.2524)\n",
      "epoch: 94 train_loss: tensor(0.1912, grad_fn=<NllLossBackward>) average train loss tensor(0.2170, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4168952600064891 val_avg_loss: tensor(1.2893)\n",
      "epoch: 95 train_loss: tensor(0.1957, grad_fn=<NllLossBackward>) average train loss tensor(0.2162, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47340943279355807 val_avg_loss: tensor(1.0927)\n",
      "epoch: 96 train_loss: tensor(0.1794, grad_fn=<NllLossBackward>) average train loss tensor(0.2183, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4673627702563195 val_avg_loss: tensor(1.0801)\n",
      "epoch: 97 train_loss: tensor(0.1864, grad_fn=<NllLossBackward>) average train loss tensor(0.2133, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4335014600477834 val_avg_loss: tensor(1.1605)\n",
      "epoch: 98 train_loss: tensor(0.1730, grad_fn=<NllLossBackward>) average train loss tensor(0.2166, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45636079403002683 val_avg_loss: tensor(1.1467)\n",
      "epoch: 99 train_loss: tensor(0.2054, grad_fn=<NllLossBackward>) average train loss tensor(0.2182, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4288706014217031 val_avg_loss: tensor(1.2509)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.42      0.59     41251\n",
      "           1       0.04      0.82      0.07      1128\n",
      "\n",
      "    accuracy                           0.43     42379\n",
      "   macro avg       0.51      0.62      0.33     42379\n",
      "weighted avg       0.96      0.43      0.58     42379\n",
      "\n",
      "roc auc score: 0.6855628988835172\n"
     ]
    }
   ],
   "source": [
    "model = WtsTrivialModel(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 300]).float()\n",
    ")\n",
    "    \n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.3634, grad_fn=<NllLossBackward>) average train loss tensor(0.3120, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.0965)\n",
      "epoch: 1 train_loss: tensor(0.3425, grad_fn=<NllLossBackward>) average train loss tensor(0.2763, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.0036)\n",
      "epoch: 2 train_loss: tensor(0.3149, grad_fn=<NllLossBackward>) average train loss tensor(0.2693, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.0176)\n",
      "epoch: 3 train_loss: tensor(0.3172, grad_fn=<NllLossBackward>) average train loss tensor(0.2669, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.02657581924903401 val_avg_loss: tensor(2.0324)\n",
      "epoch: 4 train_loss: tensor(0.2759, grad_fn=<NllLossBackward>) average train loss tensor(0.2626, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.05238474471285727 val_avg_loss: tensor(1.9943)\n",
      "epoch: 5 train_loss: tensor(0.2817, grad_fn=<NllLossBackward>) average train loss tensor(0.2595, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.065274459487361 val_avg_loss: tensor(1.9590)\n",
      "epoch: 6 train_loss: tensor(0.2824, grad_fn=<NllLossBackward>) average train loss tensor(0.2567, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07146860159867859 val_avg_loss: tensor(2.0362)\n",
      "epoch: 7 train_loss: tensor(0.2985, grad_fn=<NllLossBackward>) average train loss tensor(0.2536, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07919653128041766 val_avg_loss: tensor(1.9572)\n",
      "epoch: 8 train_loss: tensor(0.2672, grad_fn=<NllLossBackward>) average train loss tensor(0.2502, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1007875409255818 val_avg_loss: tensor(1.9951)\n",
      "epoch: 9 train_loss: tensor(0.2436, grad_fn=<NllLossBackward>) average train loss tensor(0.2496, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12270300563371973 val_avg_loss: tensor(1.8587)\n",
      "epoch: 10 train_loss: tensor(0.2609, grad_fn=<NllLossBackward>) average train loss tensor(0.2472, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13538624900451288 val_avg_loss: tensor(1.9260)\n",
      "epoch: 11 train_loss: tensor(0.2386, grad_fn=<NllLossBackward>) average train loss tensor(0.2451, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13420641241188094 val_avg_loss: tensor(1.8899)\n",
      "epoch: 12 train_loss: tensor(0.2700, grad_fn=<NllLossBackward>) average train loss tensor(0.2442, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1350617939415391 val_avg_loss: tensor(1.8726)\n",
      "epoch: 13 train_loss: tensor(0.2546, grad_fn=<NllLossBackward>) average train loss tensor(0.2413, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13612364687490783 val_avg_loss: tensor(1.9165)\n",
      "epoch: 14 train_loss: tensor(0.2384, grad_fn=<NllLossBackward>) average train loss tensor(0.2382, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15452909771996579 val_avg_loss: tensor(1.9202)\n",
      "epoch: 15 train_loss: tensor(0.2450, grad_fn=<NllLossBackward>) average train loss tensor(0.2389, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1720496711205498 val_avg_loss: tensor(1.7930)\n",
      "epoch: 16 train_loss: tensor(0.2331, grad_fn=<NllLossBackward>) average train loss tensor(0.2353, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1616671091053889 val_avg_loss: tensor(1.8136)\n",
      "epoch: 17 train_loss: tensor(0.2410, grad_fn=<NllLossBackward>) average train loss tensor(0.2373, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17470430345397162 val_avg_loss: tensor(1.7536)\n",
      "epoch: 18 train_loss: tensor(0.2296, grad_fn=<NllLossBackward>) average train loss tensor(0.2327, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19154647081379228 val_avg_loss: tensor(1.7763)\n",
      "epoch: 19 train_loss: tensor(0.2463, grad_fn=<NllLossBackward>) average train loss tensor(0.2340, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18883284665073888 val_avg_loss: tensor(1.8144)\n",
      "epoch: 20 train_loss: tensor(0.2356, grad_fn=<NllLossBackward>) average train loss tensor(0.2325, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1972981742028729 val_avg_loss: tensor(1.8584)\n",
      "epoch: 21 train_loss: tensor(0.2408, grad_fn=<NllLossBackward>) average train loss tensor(0.2311, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15113706751614903 val_avg_loss: tensor(1.9529)\n",
      "epoch: 22 train_loss: tensor(0.2296, grad_fn=<NllLossBackward>) average train loss tensor(0.2282, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1901896587322656 val_avg_loss: tensor(1.7511)\n",
      "epoch: 23 train_loss: tensor(0.2204, grad_fn=<NllLossBackward>) average train loss tensor(0.2262, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22177978349998526 val_avg_loss: tensor(1.7110)\n",
      "epoch: 24 train_loss: tensor(0.2387, grad_fn=<NllLossBackward>) average train loss tensor(0.2304, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19567589888800402 val_avg_loss: tensor(1.8069)\n",
      "epoch: 25 train_loss: tensor(0.2058, grad_fn=<NllLossBackward>) average train loss tensor(0.2261, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2282098929298292 val_avg_loss: tensor(1.7098)\n",
      "epoch: 26 train_loss: tensor(0.2386, grad_fn=<NllLossBackward>) average train loss tensor(0.2272, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19408311948795093 val_avg_loss: tensor(1.7443)\n",
      "epoch: 27 train_loss: tensor(0.2186, grad_fn=<NllLossBackward>) average train loss tensor(0.2231, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24387222369701797 val_avg_loss: tensor(1.6778)\n",
      "epoch: 28 train_loss: tensor(0.2425, grad_fn=<NllLossBackward>) average train loss tensor(0.2252, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22092440197032712 val_avg_loss: tensor(1.8171)\n",
      "epoch: 29 train_loss: tensor(0.2265, grad_fn=<NllLossBackward>) average train loss tensor(0.2267, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20850662183287616 val_avg_loss: tensor(1.7603)\n",
      "epoch: 30 train_loss: tensor(0.2295, grad_fn=<NllLossBackward>) average train loss tensor(0.2235, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2226056691148276 val_avg_loss: tensor(1.7487)\n",
      "epoch: 31 train_loss: tensor(0.2022, grad_fn=<NllLossBackward>) average train loss tensor(0.2232, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22422794442969649 val_avg_loss: tensor(1.6262)\n",
      "epoch: 32 train_loss: tensor(0.2364, grad_fn=<NllLossBackward>) average train loss tensor(0.2195, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2359083266967525 val_avg_loss: tensor(1.7681)\n",
      "epoch: 33 train_loss: tensor(0.2314, grad_fn=<NllLossBackward>) average train loss tensor(0.2216, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2017815532548742 val_avg_loss: tensor(1.8480)\n",
      "epoch: 34 train_loss: tensor(0.2257, grad_fn=<NllLossBackward>) average train loss tensor(0.2203, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2545497448603368 val_avg_loss: tensor(1.6759)\n",
      "epoch: 35 train_loss: tensor(0.2215, grad_fn=<NllLossBackward>) average train loss tensor(0.2163, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23210335368551455 val_avg_loss: tensor(1.7610)\n",
      "epoch: 36 train_loss: tensor(0.2180, grad_fn=<NllLossBackward>) average train loss tensor(0.2185, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22163230392590627 val_avg_loss: tensor(1.7896)\n",
      "epoch: 37 train_loss: tensor(0.2137, grad_fn=<NllLossBackward>) average train loss tensor(0.2201, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24646786420080818 val_avg_loss: tensor(1.7160)\n",
      "epoch: 38 train_loss: tensor(0.2087, grad_fn=<NllLossBackward>) average train loss tensor(0.2122, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26926820635342 val_avg_loss: tensor(1.6397)\n",
      "epoch: 39 train_loss: tensor(0.2191, grad_fn=<NllLossBackward>) average train loss tensor(0.2200, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22815090110019762 val_avg_loss: tensor(1.8083)\n",
      "epoch: 40 train_loss: tensor(0.2106, grad_fn=<NllLossBackward>) average train loss tensor(0.2204, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22965519275580332 val_avg_loss: tensor(1.7331)\n",
      "epoch: 41 train_loss: tensor(0.2182, grad_fn=<NllLossBackward>) average train loss tensor(0.2154, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26915022269415684 val_avg_loss: tensor(1.7046)\n",
      "epoch: 42 train_loss: tensor(0.2261, grad_fn=<NllLossBackward>) average train loss tensor(0.2195, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2331947025336991 val_avg_loss: tensor(1.7002)\n",
      "epoch: 43 train_loss: tensor(0.1980, grad_fn=<NllLossBackward>) average train loss tensor(0.2160, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.264460372238445 val_avg_loss: tensor(1.6356)\n",
      "epoch: 44 train_loss: tensor(0.2162, grad_fn=<NllLossBackward>) average train loss tensor(0.2149, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25540512638999496 val_avg_loss: tensor(1.7103)\n",
      "epoch: 45 train_loss: tensor(0.2120, grad_fn=<NllLossBackward>) average train loss tensor(0.2102, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.2687372798867357 val_avg_loss: tensor(1.6537)\n",
      "epoch: 46 train_loss: tensor(0.2060, grad_fn=<NllLossBackward>) average train loss tensor(0.2080, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2880276081762676 val_avg_loss: tensor(1.6281)\n",
      "epoch: 47 train_loss: tensor(0.1985, grad_fn=<NllLossBackward>) average train loss tensor(0.2143, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2794442969648704 val_avg_loss: tensor(1.6457)\n",
      "epoch: 48 train_loss: tensor(0.2017, grad_fn=<NllLossBackward>) average train loss tensor(0.2125, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2531339409491785 val_avg_loss: tensor(1.6799)\n",
      "epoch: 49 train_loss: tensor(0.2062, grad_fn=<NllLossBackward>) average train loss tensor(0.2118, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23838598354127954 val_avg_loss: tensor(1.6391)\n",
      "epoch: 50 train_loss: tensor(0.1915, grad_fn=<NllLossBackward>) average train loss tensor(0.2071, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28560894316137214 val_avg_loss: tensor(1.6339)\n",
      "epoch: 51 train_loss: tensor(0.1921, grad_fn=<NllLossBackward>) average train loss tensor(0.2107, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2685308084830251 val_avg_loss: tensor(1.6134)\n",
      "epoch: 52 train_loss: tensor(0.2243, grad_fn=<NllLossBackward>) average train loss tensor(0.2153, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24726425390083473 val_avg_loss: tensor(1.7619)\n",
      "epoch: 53 train_loss: tensor(0.1970, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.285196000353951 val_avg_loss: tensor(1.6635)\n",
      "epoch: 54 train_loss: tensor(0.2079, grad_fn=<NllLossBackward>) average train loss tensor(0.2092, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2909771996578474 val_avg_loss: tensor(1.6028)\n",
      "epoch: 55 train_loss: tensor(0.1862, grad_fn=<NllLossBackward>) average train loss tensor(0.2077, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2861103737132407 val_avg_loss: tensor(1.6214)\n",
      "epoch: 56 train_loss: tensor(0.2174, grad_fn=<NllLossBackward>) average train loss tensor(0.2124, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.269858124649736 val_avg_loss: tensor(1.6070)\n",
      "epoch: 57 train_loss: tensor(0.1809, grad_fn=<NllLossBackward>) average train loss tensor(0.2041, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2999144618470342 val_avg_loss: tensor(1.5957)\n",
      "epoch: 58 train_loss: tensor(0.2184, grad_fn=<NllLossBackward>) average train loss tensor(0.2083, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2877916408577412 val_avg_loss: tensor(1.5855)\n",
      "epoch: 59 train_loss: tensor(0.2810, grad_fn=<NllLossBackward>) average train loss tensor(0.2075, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28127304368344985 val_avg_loss: tensor(1.6263)\n",
      "epoch: 60 train_loss: tensor(0.2026, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2823348966168186 val_avg_loss: tensor(1.5206)\n",
      "epoch: 61 train_loss: tensor(0.1856, grad_fn=<NllLossBackward>) average train loss tensor(0.2035, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31392502138453826 val_avg_loss: tensor(1.5260)\n",
      "epoch: 62 train_loss: tensor(0.1793, grad_fn=<NllLossBackward>) average train loss tensor(0.2057, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3023626227767454 val_avg_loss: tensor(1.5578)\n",
      "epoch: 63 train_loss: tensor(0.1997, grad_fn=<NllLossBackward>) average train loss tensor(0.2061, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3102380320325635 val_avg_loss: tensor(1.6672)\n",
      "epoch: 64 train_loss: tensor(0.2016, grad_fn=<NllLossBackward>) average train loss tensor(0.2065, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2861103737132407 val_avg_loss: tensor(1.5755)\n",
      "epoch: 65 train_loss: tensor(0.1835, grad_fn=<NllLossBackward>) average train loss tensor(0.2025, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31967672477361886 val_avg_loss: tensor(1.4851)\n",
      "epoch: 66 train_loss: tensor(0.1845, grad_fn=<NllLossBackward>) average train loss tensor(0.2045, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31242072972893253 val_avg_loss: tensor(1.4910)\n",
      "epoch: 67 train_loss: tensor(0.2065, grad_fn=<NllLossBackward>) average train loss tensor(0.2070, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27661268914255377 val_avg_loss: tensor(1.6315)\n",
      "epoch: 68 train_loss: tensor(0.1783, grad_fn=<NllLossBackward>) average train loss tensor(0.2009, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31811344128838154 val_avg_loss: tensor(1.5242)\n",
      "epoch: 69 train_loss: tensor(0.1841, grad_fn=<NllLossBackward>) average train loss tensor(0.2041, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30646255493614133 val_avg_loss: tensor(1.6051)\n",
      "epoch: 70 train_loss: tensor(0.2077, grad_fn=<NllLossBackward>) average train loss tensor(0.2032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2951361236468749 val_avg_loss: tensor(1.6238)\n",
      "epoch: 71 train_loss: tensor(0.1850, grad_fn=<NllLossBackward>) average train loss tensor(0.2051, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.305636669321299 val_avg_loss: tensor(1.4767)\n",
      "epoch: 72 train_loss: tensor(0.1917, grad_fn=<NllLossBackward>) average train loss tensor(0.2079, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29743680500250713 val_avg_loss: tensor(1.5057)\n",
      "epoch: 73 train_loss: tensor(0.1814, grad_fn=<NllLossBackward>) average train loss tensor(0.2024, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31454443559567 val_avg_loss: tensor(1.5962)\n",
      "epoch: 74 train_loss: tensor(0.1867, grad_fn=<NllLossBackward>) average train loss tensor(0.1967, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3176415066513288 val_avg_loss: tensor(1.5598)\n",
      "epoch: 75 train_loss: tensor(0.1917, grad_fn=<NllLossBackward>) average train loss tensor(0.2022, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3147214110845648 val_avg_loss: tensor(1.5263)\n",
      "epoch: 76 train_loss: tensor(0.1952, grad_fn=<NllLossBackward>) average train loss tensor(0.2026, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31044450343627406 val_avg_loss: tensor(1.4970)\n",
      "epoch: 77 train_loss: tensor(0.1926, grad_fn=<NllLossBackward>) average train loss tensor(0.1993, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31967672477361886 val_avg_loss: tensor(1.5233)\n",
      "epoch: 78 train_loss: tensor(0.1786, grad_fn=<NllLossBackward>) average train loss tensor(0.1997, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3276406217738843 val_avg_loss: tensor(1.5356)\n",
      "epoch: 79 train_loss: tensor(0.2015, grad_fn=<NllLossBackward>) average train loss tensor(0.2031, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3133645990030381 val_avg_loss: tensor(1.6023)\n",
      "epoch: 80 train_loss: tensor(0.1977, grad_fn=<NllLossBackward>) average train loss tensor(0.1981, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3176710025661446 val_avg_loss: tensor(1.6091)\n",
      "epoch: 81 train_loss: tensor(0.2528, grad_fn=<NllLossBackward>) average train loss tensor(0.1959, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33778721647051885 val_avg_loss: tensor(1.5185)\n",
      "epoch: 82 train_loss: tensor(0.1817, grad_fn=<NllLossBackward>) average train loss tensor(0.2067, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31115240539185324 val_avg_loss: tensor(1.5997)\n",
      "epoch: 83 train_loss: tensor(0.1975, grad_fn=<NllLossBackward>) average train loss tensor(0.2012, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31519334572161756 val_avg_loss: tensor(1.5337)\n",
      "epoch: 84 train_loss: tensor(0.1817, grad_fn=<NllLossBackward>) average train loss tensor(0.2043, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3113588767955638 val_avg_loss: tensor(1.4910)\n",
      "epoch: 85 train_loss: tensor(0.1849, grad_fn=<NllLossBackward>) average train loss tensor(0.1963, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35088340264873313 val_avg_loss: tensor(1.4950)\n",
      "epoch: 86 train_loss: tensor(0.1865, grad_fn=<NllLossBackward>) average train loss tensor(0.1966, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3531545880895496 val_avg_loss: tensor(1.4975)\n",
      "epoch: 87 train_loss: tensor(0.1937, grad_fn=<NllLossBackward>) average train loss tensor(0.2006, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3189393269032239 val_avg_loss: tensor(1.4625)\n",
      "epoch: 88 train_loss: tensor(0.2255, grad_fn=<NllLossBackward>) average train loss tensor(0.1957, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34666548683007403 val_avg_loss: tensor(1.5654)\n",
      "epoch: 89 train_loss: tensor(0.1750, grad_fn=<NllLossBackward>) average train loss tensor(0.1998, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3236586732737516 val_avg_loss: tensor(1.5738)\n",
      "epoch: 90 train_loss: tensor(0.1718, grad_fn=<NllLossBackward>) average train loss tensor(0.1961, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3275226381146211 val_avg_loss: tensor(1.5154)\n",
      "epoch: 91 train_loss: tensor(0.1774, grad_fn=<NllLossBackward>) average train loss tensor(0.1996, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.3596436893490252 val_avg_loss: tensor(1.4422)\n",
      "epoch: 92 train_loss: tensor(0.1874, grad_fn=<NllLossBackward>) average train loss tensor(0.1992, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33713830634457126 val_avg_loss: tensor(1.4707)\n",
      "epoch: 93 train_loss: tensor(0.1986, grad_fn=<NllLossBackward>) average train loss tensor(0.2054, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28510751260950357 val_avg_loss: tensor(1.5390)\n",
      "epoch: 94 train_loss: tensor(0.1808, grad_fn=<NllLossBackward>) average train loss tensor(0.1982, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34610506444857386 val_avg_loss: tensor(1.4209)\n",
      "epoch: 95 train_loss: tensor(0.1622, grad_fn=<NllLossBackward>) average train loss tensor(0.1994, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3464590154263634 val_avg_loss: tensor(1.4567)\n",
      "epoch: 96 train_loss: tensor(0.1893, grad_fn=<NllLossBackward>) average train loss tensor(0.1962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3500870129487066 val_avg_loss: tensor(1.4367)\n",
      "epoch: 97 train_loss: tensor(0.1803, grad_fn=<NllLossBackward>) average train loss tensor(0.1931, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3690528861752647 val_avg_loss: tensor(1.4144)\n",
      "epoch: 98 train_loss: tensor(0.1790, grad_fn=<NllLossBackward>) average train loss tensor(0.1949, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3623278175972628 val_avg_loss: tensor(1.4174)\n",
      "epoch: 99 train_loss: tensor(0.1823, grad_fn=<NllLossBackward>) average train loss tensor(0.1890, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3788455298941097 val_avg_loss: tensor(1.5418)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.37      0.54     41251\n",
      "           1       0.04      0.86      0.07      1128\n",
      "\n",
      "    accuracy                           0.38     42379\n",
      "   macro avg       0.51      0.61      0.30     42379\n",
      "weighted avg       0.96      0.38      0.52     42379\n",
      "\n",
      "roc auc score: 0.6911814602904103\n"
     ]
    }
   ],
   "source": [
    "model = WtsTrivialModel(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 400]).float()\n",
    ")\n",
    "    \n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.3379, grad_fn=<NllLossBackward>) average train loss tensor(0.2808, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.2439)\n",
      "epoch: 1 train_loss: tensor(0.2648, grad_fn=<NllLossBackward>) average train loss tensor(0.2388, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.2173)\n",
      "epoch: 2 train_loss: tensor(0.2664, grad_fn=<NllLossBackward>) average train loss tensor(0.2341, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.025277998997138895 val_avg_loss: tensor(2.1798)\n",
      "epoch: 3 train_loss: tensor(0.2613, grad_fn=<NllLossBackward>) average train loss tensor(0.2313, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.02775565584166593 val_avg_loss: tensor(2.2178)\n",
      "epoch: 4 train_loss: tensor(0.2437, grad_fn=<NllLossBackward>) average train loss tensor(0.2281, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.04344748252367047 val_avg_loss: tensor(2.1501)\n",
      "epoch: 5 train_loss: tensor(0.2589, grad_fn=<NllLossBackward>) average train loss tensor(0.2251, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.041589239890275194 val_avg_loss: tensor(2.1042)\n",
      "epoch: 6 train_loss: tensor(0.2600, grad_fn=<NllLossBackward>) average train loss tensor(0.2229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.047842373831224375 val_avg_loss: tensor(2.1159)\n",
      "epoch: 7 train_loss: tensor(0.2351, grad_fn=<NllLossBackward>) average train loss tensor(0.2200, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.07494911954694275 val_avg_loss: tensor(2.1102)\n",
      "epoch: 8 train_loss: tensor(0.2291, grad_fn=<NllLossBackward>) average train loss tensor(0.2195, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.08530218564728785 val_avg_loss: tensor(2.1200)\n",
      "epoch: 9 train_loss: tensor(0.2603, grad_fn=<NllLossBackward>) average train loss tensor(0.2168, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.09848685956994956 val_avg_loss: tensor(2.1171)\n",
      "epoch: 10 train_loss: tensor(0.2250, grad_fn=<NllLossBackward>) average train loss tensor(0.2151, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1068047075480046 val_avg_loss: tensor(2.0236)\n",
      "epoch: 11 train_loss: tensor(0.2328, grad_fn=<NllLossBackward>) average train loss tensor(0.2141, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11385423118898033 val_avg_loss: tensor(2.1073)\n",
      "epoch: 12 train_loss: tensor(0.2273, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13116833318585375 val_avg_loss: tensor(2.0343)\n",
      "epoch: 13 train_loss: tensor(0.2370, grad_fn=<NllLossBackward>) average train loss tensor(0.2112, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15573843022741352 val_avg_loss: tensor(2.0398)\n",
      "epoch: 14 train_loss: tensor(0.2338, grad_fn=<NllLossBackward>) average train loss tensor(0.2097, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14352712149367314 val_avg_loss: tensor(2.0096)\n",
      "epoch: 15 train_loss: tensor(0.2202, grad_fn=<NllLossBackward>) average train loss tensor(0.2096, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13892575878240865 val_avg_loss: tensor(1.9713)\n",
      "epoch: 16 train_loss: tensor(0.2141, grad_fn=<NllLossBackward>) average train loss tensor(0.2079, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13270212075627524 val_avg_loss: tensor(1.9837)\n",
      "epoch: 17 train_loss: tensor(0.2238, grad_fn=<NllLossBackward>) average train loss tensor(0.2059, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14335014600477833 val_avg_loss: tensor(2.0917)\n",
      "epoch: 18 train_loss: tensor(0.2141, grad_fn=<NllLossBackward>) average train loss tensor(0.2061, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15169748989764917 val_avg_loss: tensor(1.9811)\n",
      "epoch: 19 train_loss: tensor(0.2287, grad_fn=<NllLossBackward>) average train loss tensor(0.2032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17432085656136626 val_avg_loss: tensor(1.9795)\n",
      "epoch: 20 train_loss: tensor(0.2128, grad_fn=<NllLossBackward>) average train loss tensor(0.2036, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16783175530189068 val_avg_loss: tensor(1.9833)\n",
      "epoch: 21 train_loss: tensor(0.2100, grad_fn=<NllLossBackward>) average train loss tensor(0.2031, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14500191723446304 val_avg_loss: tensor(1.9801)\n",
      "epoch: 22 train_loss: tensor(0.1906, grad_fn=<NllLossBackward>) average train loss tensor(0.2017, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19659027224729375 val_avg_loss: tensor(1.8526)\n",
      "epoch: 23 train_loss: tensor(0.2032, grad_fn=<NllLossBackward>) average train loss tensor(0.1995, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2064419077957703 val_avg_loss: tensor(1.8714)\n",
      "epoch: 24 train_loss: tensor(0.2107, grad_fn=<NllLossBackward>) average train loss tensor(0.1984, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21679497389611538 val_avg_loss: tensor(1.8901)\n",
      "epoch: 25 train_loss: tensor(0.2114, grad_fn=<NllLossBackward>) average train loss tensor(0.1985, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1884788956729493 val_avg_loss: tensor(1.9930)\n",
      "epoch: 26 train_loss: tensor(0.2175, grad_fn=<NllLossBackward>) average train loss tensor(0.1983, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16237501106096805 val_avg_loss: tensor(1.9639)\n",
      "epoch: 27 train_loss: tensor(0.2141, grad_fn=<NllLossBackward>) average train loss tensor(0.1999, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19384715216942452 val_avg_loss: tensor(1.8645)\n",
      "epoch: 28 train_loss: tensor(0.2109, grad_fn=<NllLossBackward>) average train loss tensor(0.2002, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.157272217797835 val_avg_loss: tensor(1.8831)\n",
      "epoch: 29 train_loss: tensor(0.2057, grad_fn=<NllLossBackward>) average train loss tensor(0.2005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16210954782762588 val_avg_loss: tensor(1.9462)\n",
      "epoch: 30 train_loss: tensor(0.2131, grad_fn=<NllLossBackward>) average train loss tensor(0.1976, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2000707901955579 val_avg_loss: tensor(1.9058)\n",
      "epoch: 31 train_loss: tensor(0.2053, grad_fn=<NllLossBackward>) average train loss tensor(0.1989, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19367017668052974 val_avg_loss: tensor(1.9125)\n",
      "epoch: 32 train_loss: tensor(0.1878, grad_fn=<NllLossBackward>) average train loss tensor(0.1963, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1943485827212931 val_avg_loss: tensor(1.9126)\n",
      "epoch: 33 train_loss: tensor(0.2164, grad_fn=<NllLossBackward>) average train loss tensor(0.1963, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18706309176179098 val_avg_loss: tensor(1.9185)\n",
      "epoch: 34 train_loss: tensor(0.2539, grad_fn=<NllLossBackward>) average train loss tensor(0.1980, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20390525912161167 val_avg_loss: tensor(1.9464)\n",
      "epoch: 35 train_loss: tensor(0.1937, grad_fn=<NllLossBackward>) average train loss tensor(0.1998, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17081084269828628 val_avg_loss: tensor(1.9223)\n",
      "epoch: 36 train_loss: tensor(0.1963, grad_fn=<NllLossBackward>) average train loss tensor(0.1971, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23068754977435627 val_avg_loss: tensor(1.7872)\n",
      "epoch: 37 train_loss: tensor(0.1976, grad_fn=<NllLossBackward>) average train loss tensor(0.1935, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.231306963985488 val_avg_loss: tensor(1.7895)\n",
      "epoch: 38 train_loss: tensor(0.2092, grad_fn=<NllLossBackward>) average train loss tensor(0.1920, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.237294634693095 val_avg_loss: tensor(1.8936)\n",
      "epoch: 39 train_loss: tensor(0.2109, grad_fn=<NllLossBackward>) average train loss tensor(0.1932, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22593870748901276 val_avg_loss: tensor(1.9309)\n",
      "epoch: 40 train_loss: tensor(0.2017, grad_fn=<NllLossBackward>) average train loss tensor(0.1943, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21989204495177417 val_avg_loss: tensor(1.8718)\n",
      "epoch: 41 train_loss: tensor(0.1919, grad_fn=<NllLossBackward>) average train loss tensor(0.1928, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20977494616995546 val_avg_loss: tensor(1.8376)\n",
      "epoch: 42 train_loss: tensor(0.1908, grad_fn=<NllLossBackward>) average train loss tensor(0.1897, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21824027372208948 val_avg_loss: tensor(1.8351)\n",
      "epoch: 43 train_loss: tensor(0.2078, grad_fn=<NllLossBackward>) average train loss tensor(0.1916, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20281391027342713 val_avg_loss: tensor(1.9179)\n",
      "epoch: 44 train_loss: tensor(0.1881, grad_fn=<NllLossBackward>) average train loss tensor(0.1908, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22210423856295902 val_avg_loss: tensor(1.8129)\n",
      "epoch: 45 train_loss: tensor(0.1935, grad_fn=<NllLossBackward>) average train loss tensor(0.1887, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.21446479662566734 val_avg_loss: tensor(1.8073)\n",
      "epoch: 46 train_loss: tensor(0.1985, grad_fn=<NllLossBackward>) average train loss tensor(0.1887, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22877031531132938 val_avg_loss: tensor(1.6811)\n",
      "epoch: 47 train_loss: tensor(0.1881, grad_fn=<NllLossBackward>) average train loss tensor(0.1918, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22517181370380202 val_avg_loss: tensor(1.8571)\n",
      "epoch: 48 train_loss: tensor(0.1799, grad_fn=<NllLossBackward>) average train loss tensor(0.1896, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22449340766303866 val_avg_loss: tensor(1.7792)\n",
      "epoch: 49 train_loss: tensor(0.1805, grad_fn=<NllLossBackward>) average train loss tensor(0.1862, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27109695307199955 val_avg_loss: tensor(1.6459)\n",
      "epoch: 50 train_loss: tensor(0.1972, grad_fn=<NllLossBackward>) average train loss tensor(0.1904, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2255847565112232 val_avg_loss: tensor(1.8182)\n",
      "epoch: 51 train_loss: tensor(0.1888, grad_fn=<NllLossBackward>) average train loss tensor(0.1871, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24281037076364922 val_avg_loss: tensor(1.8256)\n",
      "epoch: 52 train_loss: tensor(0.2081, grad_fn=<NllLossBackward>) average train loss tensor(0.1869, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23670471639677904 val_avg_loss: tensor(1.8037)\n",
      "epoch: 53 train_loss: tensor(0.1745, grad_fn=<NllLossBackward>) average train loss tensor(0.1886, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24195498923399109 val_avg_loss: tensor(1.7191)\n",
      "epoch: 54 train_loss: tensor(0.1759, grad_fn=<NllLossBackward>) average train loss tensor(0.1892, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23991977111170104 val_avg_loss: tensor(1.7779)\n",
      "epoch: 55 train_loss: tensor(0.1777, grad_fn=<NllLossBackward>) average train loss tensor(0.1886, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.264017933516208 val_avg_loss: tensor(1.7289)\n",
      "epoch: 56 train_loss: tensor(0.1790, grad_fn=<NllLossBackward>) average train loss tensor(0.1849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26643659853110346 val_avg_loss: tensor(1.6641)\n",
      "epoch: 57 train_loss: tensor(0.1930, grad_fn=<NllLossBackward>) average train loss tensor(0.1877, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2195380939739846 val_avg_loss: tensor(1.8779)\n",
      "epoch: 58 train_loss: tensor(0.1725, grad_fn=<NllLossBackward>) average train loss tensor(0.1820, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2793853051352388 val_avg_loss: tensor(1.6985)\n",
      "epoch: 59 train_loss: tensor(0.1990, grad_fn=<NllLossBackward>) average train loss tensor(0.1835, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2610093502049966 val_avg_loss: tensor(1.7143)\n",
      "epoch: 60 train_loss: tensor(0.2113, grad_fn=<NllLossBackward>) average train loss tensor(0.1886, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2044951774179276 val_avg_loss: tensor(2.0297)\n",
      "epoch: 61 train_loss: tensor(0.1946, grad_fn=<NllLossBackward>) average train loss tensor(0.1886, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23797304073385836 val_avg_loss: tensor(1.7842)\n",
      "epoch: 62 train_loss: tensor(0.1727, grad_fn=<NllLossBackward>) average train loss tensor(0.1842, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2600359850160753 val_avg_loss: tensor(1.6102)\n",
      "epoch: 63 train_loss: tensor(0.1717, grad_fn=<NllLossBackward>) average train loss tensor(0.1822, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28286582308350294 val_avg_loss: tensor(1.7248)\n",
      "epoch: 64 train_loss: tensor(0.1717, grad_fn=<NllLossBackward>) average train loss tensor(0.1811, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27794000530926466 val_avg_loss: tensor(1.6414)\n",
      "epoch: 65 train_loss: tensor(0.1907, grad_fn=<NllLossBackward>) average train loss tensor(0.1878, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25136418606023064 val_avg_loss: tensor(1.7282)\n",
      "epoch: 66 train_loss: tensor(0.1703, grad_fn=<NllLossBackward>) average train loss tensor(0.1834, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2613338052679704 val_avg_loss: tensor(1.6734)\n",
      "epoch: 67 train_loss: tensor(0.1827, grad_fn=<NllLossBackward>) average train loss tensor(0.1843, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26295608058283926 val_avg_loss: tensor(1.7636)\n",
      "epoch: 68 train_loss: tensor(0.1779, grad_fn=<NllLossBackward>) average train loss tensor(0.1823, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27519688523139546 val_avg_loss: tensor(1.8155)\n",
      "epoch: 69 train_loss: tensor(0.1834, grad_fn=<NllLossBackward>) average train loss tensor(0.1821, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25331091643807335 val_avg_loss: tensor(1.8588)\n",
      "epoch: 70 train_loss: tensor(0.1914, grad_fn=<NllLossBackward>) average train loss tensor(0.1826, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26059640739757545 val_avg_loss: tensor(1.8130)\n",
      "epoch: 71 train_loss: tensor(0.1799, grad_fn=<NllLossBackward>) average train loss tensor(0.1829, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2627496091791287 val_avg_loss: tensor(1.6988)\n",
      "epoch: 72 train_loss: tensor(0.1936, grad_fn=<NllLossBackward>) average train loss tensor(0.1835, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25310444503436275 val_avg_loss: tensor(1.8010)\n",
      "epoch: 73 train_loss: tensor(0.1873, grad_fn=<NllLossBackward>) average train loss tensor(0.1836, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28413414742058224 val_avg_loss: tensor(1.6946)\n",
      "epoch: 74 train_loss: tensor(0.1782, grad_fn=<NllLossBackward>) average train loss tensor(0.1835, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2590921157419697 val_avg_loss: tensor(1.8028)\n",
      "epoch: 75 train_loss: tensor(0.1643, grad_fn=<NllLossBackward>) average train loss tensor(0.1808, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2853139840132142 val_avg_loss: tensor(1.6567)\n",
      "epoch: 76 train_loss: tensor(0.1678, grad_fn=<NllLossBackward>) average train loss tensor(0.1764, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3071999528065363 val_avg_loss: tensor(1.6268)\n",
      "epoch: 77 train_loss: tensor(0.1728, grad_fn=<NllLossBackward>) average train loss tensor(0.1799, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2970533581099018 val_avg_loss: tensor(1.7143)\n",
      "epoch: 78 train_loss: tensor(0.1758, grad_fn=<NllLossBackward>) average train loss tensor(0.1850, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24496357254520248 val_avg_loss: tensor(1.7086)\n",
      "epoch: 79 train_loss: tensor(0.1837, grad_fn=<NllLossBackward>) average train loss tensor(0.1844, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2713919122201575 val_avg_loss: tensor(1.6655)\n",
      "epoch: 80 train_loss: tensor(0.1755, grad_fn=<NllLossBackward>) average train loss tensor(0.1806, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28324926997610833 val_avg_loss: tensor(1.7054)\n",
      "epoch: 81 train_loss: tensor(0.1721, grad_fn=<NllLossBackward>) average train loss tensor(0.1787, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2713329203905259 val_avg_loss: tensor(1.7579)\n",
      "epoch: 82 train_loss: tensor(0.1876, grad_fn=<NllLossBackward>) average train loss tensor(0.1826, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26525676193847153 val_avg_loss: tensor(1.7506)\n",
      "epoch: 83 train_loss: tensor(0.1782, grad_fn=<NllLossBackward>) average train loss tensor(0.1783, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.291036191487479 val_avg_loss: tensor(1.7335)\n",
      "epoch: 84 train_loss: tensor(0.1787, grad_fn=<NllLossBackward>) average train loss tensor(0.1793, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3037784266879037 val_avg_loss: tensor(1.6779)\n",
      "epoch: 85 train_loss: tensor(0.1831, grad_fn=<NllLossBackward>) average train loss tensor(0.1783, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28658230835029347 val_avg_loss: tensor(1.8008)\n",
      "epoch: 86 train_loss: tensor(0.1770, grad_fn=<NllLossBackward>) average train loss tensor(0.1805, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27251275698315786 val_avg_loss: tensor(1.8020)\n",
      "epoch: 87 train_loss: tensor(0.1859, grad_fn=<NllLossBackward>) average train loss tensor(0.1796, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25151166563430966 val_avg_loss: tensor(1.6953)\n",
      "epoch: 88 train_loss: tensor(0.1622, grad_fn=<NllLossBackward>) average train loss tensor(0.1808, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2719523346016577 val_avg_loss: tensor(1.7064)\n",
      "epoch: 89 train_loss: tensor(0.1858, grad_fn=<NllLossBackward>) average train loss tensor(0.1810, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25938707489012774 val_avg_loss: tensor(1.7319)\n",
      "epoch: 90 train_loss: tensor(0.1654, grad_fn=<NllLossBackward>) average train loss tensor(0.1818, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29136064655045274 val_avg_loss: tensor(1.6663)\n",
      "epoch: 91 train_loss: tensor(0.1664, grad_fn=<NllLossBackward>) average train loss tensor(0.1785, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.2836032209538979 val_avg_loss: tensor(1.7061)\n",
      "epoch: 92 train_loss: tensor(0.1680, grad_fn=<NllLossBackward>) average train loss tensor(0.1750, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3167566292068549 val_avg_loss: tensor(1.5680)\n",
      "epoch: 93 train_loss: tensor(0.1662, grad_fn=<NllLossBackward>) average train loss tensor(0.1754, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.305636669321299 val_avg_loss: tensor(1.6694)\n",
      "epoch: 94 train_loss: tensor(0.1699, grad_fn=<NllLossBackward>) average train loss tensor(0.1807, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2905052650207946 val_avg_loss: tensor(1.6601)\n",
      "epoch: 95 train_loss: tensor(0.1674, grad_fn=<NllLossBackward>) average train loss tensor(0.1772, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2994130312951656 val_avg_loss: tensor(1.5759)\n",
      "epoch: 96 train_loss: tensor(0.1710, grad_fn=<NllLossBackward>) average train loss tensor(0.1787, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30436834498421966 val_avg_loss: tensor(1.5823)\n",
      "epoch: 97 train_loss: tensor(0.1704, grad_fn=<NllLossBackward>) average train loss tensor(0.1768, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27876589092410703 val_avg_loss: tensor(1.7015)\n",
      "epoch: 98 train_loss: tensor(0.1807, grad_fn=<NllLossBackward>) average train loss tensor(0.1780, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.305931628469457 val_avg_loss: tensor(1.6575)\n",
      "epoch: 99 train_loss: tensor(0.1578, grad_fn=<NllLossBackward>) average train loss tensor(0.1776, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30796684659174706 val_avg_loss: tensor(1.6327)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.29      0.45     41251\n",
      "           1       0.03      0.89      0.06      1128\n",
      "\n",
      "    accuracy                           0.31     42379\n",
      "   macro avg       0.51      0.59      0.26     42379\n",
      "weighted avg       0.96      0.31      0.44     42379\n",
      "\n",
      "roc auc score: 0.6897490105978088\n"
     ]
    }
   ],
   "source": [
    "model = WtsTrivialModel(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 500]).float()\n",
    ")\n",
    "    \n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.2815, grad_fn=<NllLossBackward>) average train loss tensor(0.5123, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.2874)\n",
      "epoch: 1 train_loss: tensor(0.1039, grad_fn=<NllLossBackward>) average train loss tensor(0.1661, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1152)\n",
      "epoch: 2 train_loss: tensor(0.0923, grad_fn=<NllLossBackward>) average train loss tensor(0.1213, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1122)\n",
      "epoch: 3 train_loss: tensor(0.0904, grad_fn=<NllLossBackward>) average train loss tensor(0.1183, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1114)\n",
      "epoch: 4 train_loss: tensor(0.0848, grad_fn=<NllLossBackward>) average train loss tensor(0.1169, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1105)\n",
      "epoch: 5 train_loss: tensor(0.0903, grad_fn=<NllLossBackward>) average train loss tensor(0.1160, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1103)\n",
      "epoch: 6 train_loss: tensor(0.0862, grad_fn=<NllLossBackward>) average train loss tensor(0.1153, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1102)\n",
      "epoch: 7 train_loss: tensor(0.0813, grad_fn=<NllLossBackward>) average train loss tensor(0.1143, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1105)\n",
      "epoch: 8 train_loss: tensor(0.0826, grad_fn=<NllLossBackward>) average train loss tensor(0.1136, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1109)\n",
      "epoch: 9 train_loss: tensor(0.0824, grad_fn=<NllLossBackward>) average train loss tensor(0.1130, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1109)\n",
      "epoch: 10 train_loss: tensor(0.0784, grad_fn=<NllLossBackward>) average train loss tensor(0.1128, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1110)\n",
      "epoch: 11 train_loss: tensor(0.0786, grad_fn=<NllLossBackward>) average train loss tensor(0.1122, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1111)\n",
      "epoch: 12 train_loss: tensor(0.0741, grad_fn=<NllLossBackward>) average train loss tensor(0.1114, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1117)\n",
      "epoch: 13 train_loss: tensor(0.0777, grad_fn=<NllLossBackward>) average train loss tensor(0.1113, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1117)\n",
      "epoch: 14 train_loss: tensor(0.0796, grad_fn=<NllLossBackward>) average train loss tensor(0.1111, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1127)\n",
      "epoch: 15 train_loss: tensor(0.0764, grad_fn=<NllLossBackward>) average train loss tensor(0.1099, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1131)\n",
      "epoch: 16 train_loss: tensor(0.0781, grad_fn=<NllLossBackward>) average train loss tensor(0.1103, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1121)\n",
      "epoch: 17 train_loss: tensor(0.0785, grad_fn=<NllLossBackward>) average train loss tensor(0.1094, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1119)\n",
      "epoch: 18 train_loss: tensor(0.0783, grad_fn=<NllLossBackward>) average train loss tensor(0.1092, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1124)\n",
      "epoch: 19 train_loss: tensor(0.0733, grad_fn=<NllLossBackward>) average train loss tensor(0.1089, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1129)\n",
      "epoch: 20 train_loss: tensor(0.0739, grad_fn=<NllLossBackward>) average train loss tensor(0.1086, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1120)\n",
      "epoch: 21 train_loss: tensor(0.0773, grad_fn=<NllLossBackward>) average train loss tensor(0.1077, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1128)\n",
      "epoch: 22 train_loss: tensor(0.0738, grad_fn=<NllLossBackward>) average train loss tensor(0.1076, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1122)\n",
      "epoch: 23 train_loss: tensor(0.0717, grad_fn=<NllLossBackward>) average train loss tensor(0.1068, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1133)\n",
      "epoch: 24 train_loss: tensor(0.0778, grad_fn=<NllLossBackward>) average train loss tensor(0.1070, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1131)\n",
      "epoch: 25 train_loss: tensor(0.0740, grad_fn=<NllLossBackward>) average train loss tensor(0.1072, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1136)\n",
      "epoch: 26 train_loss: tensor(0.0733, grad_fn=<NllLossBackward>) average train loss tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1124)\n",
      "epoch: 27 train_loss: tensor(0.0737, grad_fn=<NllLossBackward>) average train loss tensor(0.1059, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1135)\n",
      "epoch: 28 train_loss: tensor(0.0679, grad_fn=<NllLossBackward>) average train loss tensor(0.1054, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1142)\n",
      "epoch: 29 train_loss: tensor(0.0681, grad_fn=<NllLossBackward>) average train loss tensor(0.1047, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1150)\n",
      "epoch: 30 train_loss: tensor(0.0788, grad_fn=<NllLossBackward>) average train loss tensor(0.1042, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1155)\n",
      "epoch: 31 train_loss: tensor(0.0758, grad_fn=<NllLossBackward>) average train loss tensor(0.1048, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1132)\n",
      "epoch: 32 train_loss: tensor(0.0661, grad_fn=<NllLossBackward>) average train loss tensor(0.1033, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1151)\n",
      "epoch: 33 train_loss: tensor(0.0708, grad_fn=<NllLossBackward>) average train loss tensor(0.1037, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1144)\n",
      "epoch: 34 train_loss: tensor(0.0645, grad_fn=<NllLossBackward>) average train loss tensor(0.1028, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1165)\n",
      "epoch: 35 train_loss: tensor(0.0647, grad_fn=<NllLossBackward>) average train loss tensor(0.1023, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1173)\n",
      "epoch: 36 train_loss: tensor(0.0660, grad_fn=<NllLossBackward>) average train loss tensor(0.1024, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1159)\n",
      "epoch: 37 train_loss: tensor(0.0690, grad_fn=<NllLossBackward>) average train loss tensor(0.1018, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1157)\n",
      "epoch: 38 train_loss: tensor(0.0640, grad_fn=<NllLossBackward>) average train loss tensor(0.1009, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1158)\n",
      "epoch: 39 train_loss: tensor(0.0588, grad_fn=<NllLossBackward>) average train loss tensor(0.1005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1161)\n",
      "epoch: 40 train_loss: tensor(0.0608, grad_fn=<NllLossBackward>) average train loss tensor(0.1007, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1174)\n",
      "epoch: 41 train_loss: tensor(0.0625, grad_fn=<NllLossBackward>) average train loss tensor(0.1002, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1175)\n",
      "epoch: 42 train_loss: tensor(0.0654, grad_fn=<NllLossBackward>) average train loss tensor(0.0997, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1180)\n",
      "epoch: 43 train_loss: tensor(0.0662, grad_fn=<NllLossBackward>) average train loss tensor(0.0989, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1171)\n",
      "epoch: 44 train_loss: tensor(0.0645, grad_fn=<NllLossBackward>) average train loss tensor(0.0993, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1178)\n",
      "epoch: 45 train_loss: tensor(0.0657, grad_fn=<NllLossBackward>) average train loss tensor(0.0980, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1157)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.0587, grad_fn=<NllLossBackward>) average train loss tensor(0.0978, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1185)\n",
      "epoch: 47 train_loss: tensor(0.0645, grad_fn=<NllLossBackward>) average train loss tensor(0.0979, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1205)\n",
      "epoch: 48 train_loss: tensor(0.0546, grad_fn=<NllLossBackward>) average train loss tensor(0.0971, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1205)\n",
      "epoch: 49 train_loss: tensor(0.0652, grad_fn=<NllLossBackward>) average train loss tensor(0.0962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1208)\n",
      "epoch: 50 train_loss: tensor(0.0542, grad_fn=<NllLossBackward>) average train loss tensor(0.0958, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1195)\n",
      "epoch: 51 train_loss: tensor(0.0546, grad_fn=<NllLossBackward>) average train loss tensor(0.0964, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1208)\n",
      "epoch: 52 train_loss: tensor(0.0599, grad_fn=<NllLossBackward>) average train loss tensor(0.0957, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1210)\n",
      "epoch: 53 train_loss: tensor(0.0552, grad_fn=<NllLossBackward>) average train loss tensor(0.0947, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1196)\n",
      "epoch: 54 train_loss: tensor(0.0550, grad_fn=<NllLossBackward>) average train loss tensor(0.0947, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1198)\n",
      "epoch: 55 train_loss: tensor(0.0488, grad_fn=<NllLossBackward>) average train loss tensor(0.0940, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1239)\n",
      "epoch: 56 train_loss: tensor(0.0598, grad_fn=<NllLossBackward>) average train loss tensor(0.0943, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1222)\n",
      "epoch: 57 train_loss: tensor(0.0527, grad_fn=<NllLossBackward>) average train loss tensor(0.0941, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1195)\n",
      "epoch: 58 train_loss: tensor(0.0541, grad_fn=<NllLossBackward>) average train loss tensor(0.0934, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1231)\n",
      "epoch: 59 train_loss: tensor(0.0547, grad_fn=<NllLossBackward>) average train loss tensor(0.0930, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1238)\n",
      "epoch: 60 train_loss: tensor(0.0579, grad_fn=<NllLossBackward>) average train loss tensor(0.0930, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1221)\n",
      "epoch: 61 train_loss: tensor(0.0521, grad_fn=<NllLossBackward>) average train loss tensor(0.0925, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1210)\n",
      "epoch: 62 train_loss: tensor(0.0537, grad_fn=<NllLossBackward>) average train loss tensor(0.0926, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1248)\n",
      "epoch: 63 train_loss: tensor(0.0484, grad_fn=<NllLossBackward>) average train loss tensor(0.0917, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1227)\n",
      "epoch: 64 train_loss: tensor(0.0562, grad_fn=<NllLossBackward>) average train loss tensor(0.0916, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1215)\n",
      "epoch: 65 train_loss: tensor(0.0469, grad_fn=<NllLossBackward>) average train loss tensor(0.0910, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746335132584137 val_avg_loss: tensor(0.1267)\n",
      "epoch: 66 train_loss: tensor(0.0498, grad_fn=<NllLossBackward>) average train loss tensor(0.0908, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1248)\n",
      "epoch: 67 train_loss: tensor(0.0466, grad_fn=<NllLossBackward>) average train loss tensor(0.0913, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1238)\n",
      "epoch: 68 train_loss: tensor(0.0454, grad_fn=<NllLossBackward>) average train loss tensor(0.0902, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1253)\n",
      "epoch: 69 train_loss: tensor(0.0496, grad_fn=<NllLossBackward>) average train loss tensor(0.0897, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1244)\n",
      "epoch: 70 train_loss: tensor(0.0398, grad_fn=<NllLossBackward>) average train loss tensor(0.0901, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1249)\n",
      "epoch: 71 train_loss: tensor(0.0463, grad_fn=<NllLossBackward>) average train loss tensor(0.0887, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1239)\n",
      "epoch: 72 train_loss: tensor(0.0419, grad_fn=<NllLossBackward>) average train loss tensor(0.0895, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1246)\n",
      "epoch: 73 train_loss: tensor(0.0475, grad_fn=<NllLossBackward>) average train loss tensor(0.0890, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1242)\n",
      "epoch: 74 train_loss: tensor(0.0475, grad_fn=<NllLossBackward>) average train loss tensor(0.0883, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1275)\n",
      "epoch: 75 train_loss: tensor(0.0505, grad_fn=<NllLossBackward>) average train loss tensor(0.0888, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1275)\n",
      "epoch: 76 train_loss: tensor(0.0406, grad_fn=<NllLossBackward>) average train loss tensor(0.0886, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1222)\n",
      "epoch: 77 train_loss: tensor(0.0511, grad_fn=<NllLossBackward>) average train loss tensor(0.0881, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1248)\n",
      "epoch: 78 train_loss: tensor(0.0418, grad_fn=<NllLossBackward>) average train loss tensor(0.0885, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1271)\n",
      "epoch: 79 train_loss: tensor(0.0433, grad_fn=<NllLossBackward>) average train loss tensor(0.0871, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746040173435979 val_avg_loss: tensor(0.1285)\n",
      "epoch: 80 train_loss: tensor(0.0421, grad_fn=<NllLossBackward>) average train loss tensor(0.0871, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746335132584137 val_avg_loss: tensor(0.1268)\n",
      "epoch: 81 train_loss: tensor(0.0436, grad_fn=<NllLossBackward>) average train loss tensor(0.0868, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1272)\n",
      "epoch: 82 train_loss: tensor(0.0580, grad_fn=<NllLossBackward>) average train loss tensor(0.0880, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1248)\n",
      "epoch: 83 train_loss: tensor(0.0440, grad_fn=<NllLossBackward>) average train loss tensor(0.0866, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1262)\n",
      "epoch: 84 train_loss: tensor(0.0465, grad_fn=<NllLossBackward>) average train loss tensor(0.0863, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746630091732295 val_avg_loss: tensor(0.1259)\n",
      "epoch: 85 train_loss: tensor(0.0436, grad_fn=<NllLossBackward>) average train loss tensor(0.0861, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1294)\n",
      "epoch: 86 train_loss: tensor(0.0367, grad_fn=<NllLossBackward>) average train loss tensor(0.0849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1299)\n",
      "epoch: 87 train_loss: tensor(0.0429, grad_fn=<NllLossBackward>) average train loss tensor(0.0854, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1305)\n",
      "epoch: 88 train_loss: tensor(0.0364, grad_fn=<NllLossBackward>) average train loss tensor(0.0857, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1275)\n",
      "epoch: 89 train_loss: tensor(0.0416, grad_fn=<NllLossBackward>) average train loss tensor(0.0849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1272)\n",
      "epoch: 90 train_loss: tensor(0.0494, grad_fn=<NllLossBackward>) average train loss tensor(0.0858, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1250)\n",
      "epoch: 91 train_loss: tensor(0.0500, grad_fn=<NllLossBackward>) average train loss tensor(0.0849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1259)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 train_loss: tensor(0.0409, grad_fn=<NllLossBackward>) average train loss tensor(0.0849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1280)\n",
      "epoch: 93 train_loss: tensor(0.0352, grad_fn=<NllLossBackward>) average train loss tensor(0.0839, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1268)\n",
      "epoch: 94 train_loss: tensor(0.0416, grad_fn=<NllLossBackward>) average train loss tensor(0.0845, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1319)\n",
      "epoch: 95 train_loss: tensor(0.0433, grad_fn=<NllLossBackward>) average train loss tensor(0.0853, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1279)\n",
      "epoch: 96 train_loss: tensor(0.0420, grad_fn=<NllLossBackward>) average train loss tensor(0.0839, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1295)\n",
      "epoch: 97 train_loss: tensor(0.0416, grad_fn=<NllLossBackward>) average train loss tensor(0.0839, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9746925050880453 val_avg_loss: tensor(0.1275)\n",
      "epoch: 98 train_loss: tensor(0.0392, grad_fn=<NllLossBackward>) average train loss tensor(0.0838, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1297)\n",
      "epoch: 99 train_loss: tensor(0.0466, grad_fn=<NllLossBackward>) average train loss tensor(0.0832, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1275)\n"
     ]
    }
   ],
   "source": [
    "model = WtsNormModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/norm_bn_d128_drop05_wd0005')\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.6199, grad_fn=<NllLossBackward>) average train loss tensor(0.5532, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22036397958882695 val_avg_loss: tensor(1.1939)\n",
      "epoch: 1 train_loss: tensor(0.4589, grad_fn=<NllLossBackward>) average train loss tensor(0.4401, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19414211131758252 val_avg_loss: tensor(1.3056)\n",
      "epoch: 2 train_loss: tensor(0.4277, grad_fn=<NllLossBackward>) average train loss tensor(0.4055, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18169483526531574 val_avg_loss: tensor(1.4024)\n",
      "epoch: 3 train_loss: tensor(0.4529, grad_fn=<NllLossBackward>) average train loss tensor(0.3922, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16564905760552165 val_avg_loss: tensor(1.3765)\n",
      "epoch: 4 train_loss: tensor(0.4084, grad_fn=<NllLossBackward>) average train loss tensor(0.3849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20741527298469162 val_avg_loss: tensor(1.3747)\n",
      "epoch: 5 train_loss: tensor(0.4195, grad_fn=<NllLossBackward>) average train loss tensor(0.3780, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22885880305577677 val_avg_loss: tensor(1.3597)\n",
      "epoch: 6 train_loss: tensor(0.3913, grad_fn=<NllLossBackward>) average train loss tensor(0.3706, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25891514025307494 val_avg_loss: tensor(1.3372)\n",
      "epoch: 7 train_loss: tensor(0.3945, grad_fn=<NllLossBackward>) average train loss tensor(0.3642, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.257882783234522 val_avg_loss: tensor(1.3131)\n",
      "epoch: 8 train_loss: tensor(0.3702, grad_fn=<NllLossBackward>) average train loss tensor(0.3582, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.278618411350028 val_avg_loss: tensor(1.2650)\n",
      "epoch: 9 train_loss: tensor(0.4097, grad_fn=<NllLossBackward>) average train loss tensor(0.3525, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3083797893991682 val_avg_loss: tensor(1.2738)\n",
      "epoch: 10 train_loss: tensor(0.3651, grad_fn=<NllLossBackward>) average train loss tensor(0.3461, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34206412411880954 val_avg_loss: tensor(1.2210)\n",
      "epoch: 11 train_loss: tensor(0.3525, grad_fn=<NllLossBackward>) average train loss tensor(0.3456, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3353980473704392 val_avg_loss: tensor(1.2326)\n",
      "epoch: 12 train_loss: tensor(0.3532, grad_fn=<NllLossBackward>) average train loss tensor(0.3380, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35097189039318055 val_avg_loss: tensor(1.2440)\n",
      "epoch: 13 train_loss: tensor(0.3896, grad_fn=<NllLossBackward>) average train loss tensor(0.3366, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36902339026044895 val_avg_loss: tensor(1.2616)\n",
      "epoch: 14 train_loss: tensor(0.3550, grad_fn=<NllLossBackward>) average train loss tensor(0.3303, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3694363330678701 val_avg_loss: tensor(1.2593)\n",
      "epoch: 15 train_loss: tensor(0.3669, grad_fn=<NllLossBackward>) average train loss tensor(0.3250, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4141226440138041 val_avg_loss: tensor(1.1944)\n",
      "epoch: 16 train_loss: tensor(0.3442, grad_fn=<NllLossBackward>) average train loss tensor(0.3274, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38329941303129517 val_avg_loss: tensor(1.2187)\n",
      "epoch: 17 train_loss: tensor(0.3565, grad_fn=<NllLossBackward>) average train loss tensor(0.3248, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4047724390171961 val_avg_loss: tensor(1.2140)\n",
      "epoch: 18 train_loss: tensor(0.3354, grad_fn=<NllLossBackward>) average train loss tensor(0.3191, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41601038256201517 val_avg_loss: tensor(1.2067)\n",
      "epoch: 19 train_loss: tensor(0.3437, grad_fn=<NllLossBackward>) average train loss tensor(0.3144, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43948913075539037 val_avg_loss: tensor(1.1613)\n",
      "epoch: 20 train_loss: tensor(0.3303, grad_fn=<NllLossBackward>) average train loss tensor(0.3129, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44414948529628645 val_avg_loss: tensor(1.1509)\n",
      "epoch: 21 train_loss: tensor(0.3296, grad_fn=<NllLossBackward>) average train loss tensor(0.3110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45925139368197504 val_avg_loss: tensor(1.1427)\n",
      "epoch: 22 train_loss: tensor(0.3191, grad_fn=<NllLossBackward>) average train loss tensor(0.3093, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4377193758664425 val_avg_loss: tensor(1.1648)\n",
      "epoch: 23 train_loss: tensor(0.3301, grad_fn=<NllLossBackward>) average train loss tensor(0.3066, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4748547326195322 val_avg_loss: tensor(1.1183)\n",
      "epoch: 24 train_loss: tensor(0.3099, grad_fn=<NllLossBackward>) average train loss tensor(0.3067, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4884228534347993 val_avg_loss: tensor(1.0747)\n",
      "epoch: 25 train_loss: tensor(0.3483, grad_fn=<NllLossBackward>) average train loss tensor(0.3018, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4831135887679556 val_avg_loss: tensor(1.1395)\n",
      "epoch: 26 train_loss: tensor(0.3296, grad_fn=<NllLossBackward>) average train loss tensor(0.3064, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5182432233135711 val_avg_loss: tensor(1.0616)\n",
      "epoch: 27 train_loss: tensor(0.3245, grad_fn=<NllLossBackward>) average train loss tensor(0.2993, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4964162463498805 val_avg_loss: tensor(1.1214)\n",
      "epoch: 28 train_loss: tensor(0.3161, grad_fn=<NllLossBackward>) average train loss tensor(0.2952, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5028168598649088 val_avg_loss: tensor(1.0920)\n",
      "epoch: 29 train_loss: tensor(0.3315, grad_fn=<NllLossBackward>) average train loss tensor(0.2959, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4806949237530602 val_avg_loss: tensor(1.1088)\n",
      "epoch: 30 train_loss: tensor(0.3040, grad_fn=<NllLossBackward>) average train loss tensor(0.2897, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48420493761614014 val_avg_loss: tensor(1.1210)\n",
      "epoch: 31 train_loss: tensor(0.3719, grad_fn=<NllLossBackward>) average train loss tensor(0.2998, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4571276878152376 val_avg_loss: tensor(1.1391)\n",
      "epoch: 32 train_loss: tensor(0.3006, grad_fn=<NllLossBackward>) average train loss tensor(0.2973, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.501017609061145 val_avg_loss: tensor(1.0649)\n",
      "epoch: 33 train_loss: tensor(0.3204, grad_fn=<NllLossBackward>) average train loss tensor(0.2928, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5082146122761998 val_avg_loss: tensor(1.0824)\n",
      "epoch: 34 train_loss: tensor(0.2973, grad_fn=<NllLossBackward>) average train loss tensor(0.2849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5245258531693361 val_avg_loss: tensor(1.0726)\n",
      "epoch: 35 train_loss: tensor(0.2916, grad_fn=<NllLossBackward>) average train loss tensor(0.2879, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5308674748547326 val_avg_loss: tensor(1.0218)\n",
      "epoch: 36 train_loss: tensor(0.2922, grad_fn=<NllLossBackward>) average train loss tensor(0.2800, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5358817803734183 val_avg_loss: tensor(1.0489)\n",
      "epoch: 37 train_loss: tensor(0.2976, grad_fn=<NllLossBackward>) average train loss tensor(0.2825, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5091289856354895 val_avg_loss: tensor(1.0756)\n",
      "epoch: 38 train_loss: tensor(0.3159, grad_fn=<NllLossBackward>) average train loss tensor(0.2822, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.526000648910126 val_avg_loss: tensor(1.0196)\n",
      "epoch: 39 train_loss: tensor(0.2940, grad_fn=<NllLossBackward>) average train loss tensor(0.2786, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5361177476919446 val_avg_loss: tensor(1.0626)\n",
      "epoch: 40 train_loss: tensor(0.2900, grad_fn=<NllLossBackward>) average train loss tensor(0.2796, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5187151579506238 val_avg_loss: tensor(1.0509)\n",
      "epoch: 41 train_loss: tensor(0.3157, grad_fn=<NllLossBackward>) average train loss tensor(0.2829, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5151756481727281 val_avg_loss: tensor(1.0541)\n",
      "epoch: 42 train_loss: tensor(0.2812, grad_fn=<NllLossBackward>) average train loss tensor(0.2796, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5228445860248355 val_avg_loss: tensor(1.0359)\n",
      "epoch: 43 train_loss: tensor(0.2825, grad_fn=<NllLossBackward>) average train loss tensor(0.2748, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5369141373919712 val_avg_loss: tensor(1.0423)\n",
      "epoch: 44 train_loss: tensor(0.2840, grad_fn=<NllLossBackward>) average train loss tensor(0.2691, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5584756511223196 val_avg_loss: tensor(0.9726)\n",
      "epoch: 45 train_loss: tensor(0.2908, grad_fn=<NllLossBackward>) average train loss tensor(0.2722, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5560864820222399 val_avg_loss: tensor(0.9735)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.2789, grad_fn=<NllLossBackward>) average train loss tensor(0.2736, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5647877768929004 val_avg_loss: tensor(0.9518)\n",
      "epoch: 47 train_loss: tensor(0.2872, grad_fn=<NllLossBackward>) average train loss tensor(0.2709, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5649057605521636 val_avg_loss: tensor(0.9513)\n",
      "epoch: 48 train_loss: tensor(0.2629, grad_fn=<NllLossBackward>) average train loss tensor(0.2683, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5682977907559803 val_avg_loss: tensor(0.9404)\n",
      "epoch: 49 train_loss: tensor(0.2857, grad_fn=<NllLossBackward>) average train loss tensor(0.2723, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5650827360410583 val_avg_loss: tensor(0.9587)\n",
      "epoch: 50 train_loss: tensor(0.2730, grad_fn=<NllLossBackward>) average train loss tensor(0.2699, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5286552812435478 val_avg_loss: tensor(1.0362)\n",
      "epoch: 51 train_loss: tensor(0.2648, grad_fn=<NllLossBackward>) average train loss tensor(0.2641, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5877355986195912 val_avg_loss: tensor(0.9106)\n",
      "epoch: 52 train_loss: tensor(0.2755, grad_fn=<NllLossBackward>) average train loss tensor(0.2639, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5923959531604873 val_avg_loss: tensor(0.8825)\n",
      "epoch: 53 train_loss: tensor(0.2743, grad_fn=<NllLossBackward>) average train loss tensor(0.2620, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5597439754593989 val_avg_loss: tensor(0.9706)\n",
      "epoch: 54 train_loss: tensor(0.2723, grad_fn=<NllLossBackward>) average train loss tensor(0.2655, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5589770816741881 val_avg_loss: tensor(0.9599)\n",
      "epoch: 55 train_loss: tensor(0.2628, grad_fn=<NllLossBackward>) average train loss tensor(0.2647, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.592572928649382 val_avg_loss: tensor(0.9191)\n",
      "epoch: 56 train_loss: tensor(0.2617, grad_fn=<NllLossBackward>) average train loss tensor(0.2576, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5889744270418547 val_avg_loss: tensor(0.9050)\n",
      "epoch: 57 train_loss: tensor(0.2775, grad_fn=<NllLossBackward>) average train loss tensor(0.2663, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5453499690292894 val_avg_loss: tensor(0.9878)\n",
      "epoch: 58 train_loss: tensor(0.2712, grad_fn=<NllLossBackward>) average train loss tensor(0.2599, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5947261304309354 val_avg_loss: tensor(0.9079)\n",
      "epoch: 59 train_loss: tensor(0.2591, grad_fn=<NllLossBackward>) average train loss tensor(0.2627, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5803911158304574 val_avg_loss: tensor(0.9250)\n",
      "epoch: 60 train_loss: tensor(0.2738, grad_fn=<NllLossBackward>) average train loss tensor(0.2529, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6113323304722296 val_avg_loss: tensor(0.8742)\n",
      "epoch: 61 train_loss: tensor(0.2494, grad_fn=<NllLossBackward>) average train loss tensor(0.2591, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6027490192608324 val_avg_loss: tensor(0.8711)\n",
      "epoch: 62 train_loss: tensor(0.2681, grad_fn=<NllLossBackward>) average train loss tensor(0.2575, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5977052178273309 val_avg_loss: tensor(0.8943)\n",
      "epoch: 63 train_loss: tensor(0.2809, grad_fn=<NllLossBackward>) average train loss tensor(0.2620, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.607910804353597 val_avg_loss: tensor(0.8793)\n",
      "epoch: 64 train_loss: tensor(0.2789, grad_fn=<NllLossBackward>) average train loss tensor(0.2572, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6154617585464414 val_avg_loss: tensor(0.8715)\n",
      "epoch: 65 train_loss: tensor(0.2710, grad_fn=<NllLossBackward>) average train loss tensor(0.2565, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6247529717134177 val_avg_loss: tensor(0.8535)\n",
      "epoch: 66 train_loss: tensor(0.2544, grad_fn=<NllLossBackward>) average train loss tensor(0.2550, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6110078754092558 val_avg_loss: tensor(0.8616)\n",
      "epoch: 67 train_loss: tensor(0.2356, grad_fn=<NllLossBackward>) average train loss tensor(0.2477, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6274370999616553 val_avg_loss: tensor(0.8321)\n",
      "epoch: 68 train_loss: tensor(0.2613, grad_fn=<NllLossBackward>) average train loss tensor(0.2546, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5859363478158275 val_avg_loss: tensor(0.8808)\n",
      "epoch: 69 train_loss: tensor(0.2660, grad_fn=<NllLossBackward>) average train loss tensor(0.2505, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6233666637170752 val_avg_loss: tensor(0.8426)\n",
      "epoch: 70 train_loss: tensor(0.2487, grad_fn=<NllLossBackward>) average train loss tensor(0.2393, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6599415980886647 val_avg_loss: tensor(0.7804)\n",
      "epoch: 71 train_loss: tensor(0.2508, grad_fn=<NllLossBackward>) average train loss tensor(0.2535, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6256378491578917 val_avg_loss: tensor(0.8271)\n",
      "epoch: 72 train_loss: tensor(0.2517, grad_fn=<NllLossBackward>) average train loss tensor(0.2509, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6140164587204672 val_avg_loss: tensor(0.8621)\n",
      "epoch: 73 train_loss: tensor(0.2515, grad_fn=<NllLossBackward>) average train loss tensor(0.2434, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.641801610476949 val_avg_loss: tensor(0.8296)\n",
      "epoch: 74 train_loss: tensor(0.2483, grad_fn=<NllLossBackward>) average train loss tensor(0.2439, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.59637790166062 val_avg_loss: tensor(0.9086)\n",
      "epoch: 75 train_loss: tensor(0.2486, grad_fn=<NllLossBackward>) average train loss tensor(0.2490, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6164056278205469 val_avg_loss: tensor(0.8634)\n",
      "epoch: 76 train_loss: tensor(0.2499, grad_fn=<NllLossBackward>) average train loss tensor(0.2437, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6243990207356281 val_avg_loss: tensor(0.8598)\n",
      "epoch: 77 train_loss: tensor(0.2784, grad_fn=<NllLossBackward>) average train loss tensor(0.2504, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6177034480724419 val_avg_loss: tensor(0.8387)\n",
      "epoch: 78 train_loss: tensor(0.2371, grad_fn=<NllLossBackward>) average train loss tensor(0.2478, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6286759283839188 val_avg_loss: tensor(0.8442)\n",
      "epoch: 79 train_loss: tensor(0.2468, grad_fn=<NllLossBackward>) average train loss tensor(0.2428, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6231896882281804 val_avg_loss: tensor(0.8298)\n",
      "epoch: 80 train_loss: tensor(0.2390, grad_fn=<NllLossBackward>) average train loss tensor(0.2425, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.665899772881456 val_avg_loss: tensor(0.7424)\n",
      "epoch: 81 train_loss: tensor(0.2549, grad_fn=<NllLossBackward>) average train loss tensor(0.2434, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6145178892723357 val_avg_loss: tensor(0.8688)\n",
      "epoch: 82 train_loss: tensor(0.2910, grad_fn=<NllLossBackward>) average train loss tensor(0.2409, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6302687077839719 val_avg_loss: tensor(0.8312)\n",
      "epoch: 83 train_loss: tensor(0.2619, grad_fn=<NllLossBackward>) average train loss tensor(0.2530, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5980296728903047 val_avg_loss: tensor(0.8720)\n",
      "epoch: 84 train_loss: tensor(0.2706, grad_fn=<NllLossBackward>) average train loss tensor(0.2495, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6122761997463352 val_avg_loss: tensor(0.8500)\n",
      "epoch: 85 train_loss: tensor(0.2415, grad_fn=<NllLossBackward>) average train loss tensor(0.2518, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5887089638085126 val_avg_loss: tensor(0.8705)\n",
      "epoch: 86 train_loss: tensor(0.2383, grad_fn=<NllLossBackward>) average train loss tensor(0.2375, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6262277674542076 val_avg_loss: tensor(0.8422)\n",
      "epoch: 87 train_loss: tensor(0.2929, grad_fn=<NllLossBackward>) average train loss tensor(0.2378, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6533640090847418 val_avg_loss: tensor(0.7870)\n",
      "epoch: 88 train_loss: tensor(0.2449, grad_fn=<NllLossBackward>) average train loss tensor(0.2411, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6604725245553491 val_avg_loss: tensor(0.7578)\n",
      "epoch: 89 train_loss: tensor(0.2283, grad_fn=<NllLossBackward>) average train loss tensor(0.2372, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6529215703625048 val_avg_loss: tensor(0.7672)\n",
      "epoch: 90 train_loss: tensor(0.2347, grad_fn=<NllLossBackward>) average train loss tensor(0.2476, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6475533138660295 val_avg_loss: tensor(0.7674)\n",
      "epoch: 91 train_loss: tensor(0.2390, grad_fn=<NllLossBackward>) average train loss tensor(0.2356, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6799988201634074 val_avg_loss: tensor(0.7157)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 train_loss: tensor(0.2442, grad_fn=<NllLossBackward>) average train loss tensor(0.2337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6581423472849011 val_avg_loss: tensor(0.7843)\n",
      "epoch: 93 train_loss: tensor(0.2720, grad_fn=<NllLossBackward>) average train loss tensor(0.2377, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6683184378963514 val_avg_loss: tensor(0.7566)\n",
      "epoch: 94 train_loss: tensor(0.2413, grad_fn=<NllLossBackward>) average train loss tensor(0.2336, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6279385305135239 val_avg_loss: tensor(0.8652)\n",
      "epoch: 95 train_loss: tensor(0.2389, grad_fn=<NllLossBackward>) average train loss tensor(0.2382, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6516532460254255 val_avg_loss: tensor(0.7704)\n",
      "epoch: 96 train_loss: tensor(0.2243, grad_fn=<NllLossBackward>) average train loss tensor(0.2372, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6644249771406661 val_avg_loss: tensor(0.7441)\n",
      "epoch: 97 train_loss: tensor(0.2324, grad_fn=<NllLossBackward>) average train loss tensor(0.2341, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6833318585375926 val_avg_loss: tensor(0.7109)\n",
      "epoch: 98 train_loss: tensor(0.2191, grad_fn=<NllLossBackward>) average train loss tensor(0.2284, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6831843789635136 val_avg_loss: tensor(0.7210)\n",
      "epoch: 99 train_loss: tensor(0.2384, grad_fn=<NllLossBackward>) average train loss tensor(0.2250, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.684718166533935 val_avg_loss: tensor(0.7257)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.69      0.81     41251\n",
      "           1       0.05      0.57      0.09      1128\n",
      "\n",
      "    accuracy                           0.69     42379\n",
      "   macro avg       0.52      0.63      0.45     42379\n",
      "weighted avg       0.96      0.69      0.79     42379\n",
      "\n",
      "roc auc score: 0.6780813802751569\n"
     ]
    }
   ],
   "source": [
    "model = WtsNormModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/norm_bn_d128_drop05_wd0005_weight200')\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 200]).float()\n",
    ")\n",
    "\n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.6133, grad_fn=<NllLossBackward>) average train loss tensor(0.5558, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26499129870512933 val_avg_loss: tensor(1.0509)\n",
      "epoch: 1 train_loss: tensor(0.3602, grad_fn=<NllLossBackward>) average train loss tensor(0.3941, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21319647228858804 val_avg_loss: tensor(1.2913)\n",
      "epoch: 2 train_loss: tensor(0.4233, grad_fn=<NllLossBackward>) average train loss tensor(0.3425, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16570804943515324 val_avg_loss: tensor(1.5446)\n",
      "epoch: 3 train_loss: tensor(0.3665, grad_fn=<NllLossBackward>) average train loss tensor(0.3227, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15364422027549185 val_avg_loss: tensor(1.5891)\n",
      "epoch: 4 train_loss: tensor(0.3388, grad_fn=<NllLossBackward>) average train loss tensor(0.3140, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17390791375394507 val_avg_loss: tensor(1.6234)\n",
      "epoch: 5 train_loss: tensor(0.3409, grad_fn=<NllLossBackward>) average train loss tensor(0.3087, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18166533935049994 val_avg_loss: tensor(1.5904)\n",
      "epoch: 6 train_loss: tensor(0.3349, grad_fn=<NllLossBackward>) average train loss tensor(0.3044, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2106893195292452 val_avg_loss: tensor(1.5436)\n",
      "epoch: 7 train_loss: tensor(0.3287, grad_fn=<NllLossBackward>) average train loss tensor(0.3023, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20682535468837565 val_avg_loss: tensor(1.5751)\n",
      "epoch: 8 train_loss: tensor(0.3231, grad_fn=<NllLossBackward>) average train loss tensor(0.2930, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.237442114267174 val_avg_loss: tensor(1.5365)\n",
      "epoch: 9 train_loss: tensor(0.3082, grad_fn=<NllLossBackward>) average train loss tensor(0.2876, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2611568297790756 val_avg_loss: tensor(1.5038)\n",
      "epoch: 10 train_loss: tensor(0.3074, grad_fn=<NllLossBackward>) average train loss tensor(0.2856, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27493142199805326 val_avg_loss: tensor(1.4767)\n",
      "epoch: 11 train_loss: tensor(0.3178, grad_fn=<NllLossBackward>) average train loss tensor(0.2879, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26502079461994515 val_avg_loss: tensor(1.4862)\n",
      "epoch: 12 train_loss: tensor(0.3183, grad_fn=<NllLossBackward>) average train loss tensor(0.2849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2867297879243725 val_avg_loss: tensor(1.4707)\n",
      "epoch: 13 train_loss: tensor(0.3101, grad_fn=<NllLossBackward>) average train loss tensor(0.2770, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3010058106952187 val_avg_loss: tensor(1.4836)\n",
      "epoch: 14 train_loss: tensor(0.3038, grad_fn=<NllLossBackward>) average train loss tensor(0.2683, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3163731823142495 val_avg_loss: tensor(1.4655)\n",
      "epoch: 15 train_loss: tensor(0.2974, grad_fn=<NllLossBackward>) average train loss tensor(0.2771, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29133115063563697 val_avg_loss: tensor(1.5312)\n",
      "epoch: 16 train_loss: tensor(0.3119, grad_fn=<NllLossBackward>) average train loss tensor(0.2706, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32203639795888267 val_avg_loss: tensor(1.5074)\n",
      "epoch: 17 train_loss: tensor(0.3261, grad_fn=<NllLossBackward>) average train loss tensor(0.2659, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3470194378078636 val_avg_loss: tensor(1.4597)\n",
      "epoch: 18 train_loss: tensor(0.3023, grad_fn=<NllLossBackward>) average train loss tensor(0.2653, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32740465445535794 val_avg_loss: tensor(1.4871)\n",
      "epoch: 19 train_loss: tensor(0.3012, grad_fn=<NllLossBackward>) average train loss tensor(0.2663, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33755124915199247 val_avg_loss: tensor(1.4419)\n",
      "epoch: 20 train_loss: tensor(0.2964, grad_fn=<NllLossBackward>) average train loss tensor(0.2645, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3402353774002301 val_avg_loss: tensor(1.4733)\n",
      "epoch: 21 train_loss: tensor(0.2779, grad_fn=<NllLossBackward>) average train loss tensor(0.2620, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3452496829189157 val_avg_loss: tensor(1.4861)\n",
      "epoch: 22 train_loss: tensor(0.2757, grad_fn=<NllLossBackward>) average train loss tensor(0.2551, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3649529540158688 val_avg_loss: tensor(1.4540)\n",
      "epoch: 23 train_loss: tensor(0.2823, grad_fn=<NllLossBackward>) average train loss tensor(0.2592, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3511193699672595 val_avg_loss: tensor(1.4497)\n",
      "epoch: 24 train_loss: tensor(0.2792, grad_fn=<NllLossBackward>) average train loss tensor(0.2595, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36536589682328996 val_avg_loss: tensor(1.4400)\n",
      "epoch: 25 train_loss: tensor(0.2843, grad_fn=<NllLossBackward>) average train loss tensor(0.2519, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38368285992390055 val_avg_loss: tensor(1.4507)\n",
      "epoch: 26 train_loss: tensor(0.2776, grad_fn=<NllLossBackward>) average train loss tensor(0.2522, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40878388343214467 val_avg_loss: tensor(1.3572)\n",
      "epoch: 27 train_loss: tensor(0.2692, grad_fn=<NllLossBackward>) average train loss tensor(0.2475, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41727870689909446 val_avg_loss: tensor(1.3731)\n",
      "epoch: 28 train_loss: tensor(0.2738, grad_fn=<NllLossBackward>) average train loss tensor(0.2447, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41297230333598794 val_avg_loss: tensor(1.3654)\n",
      "epoch: 29 train_loss: tensor(0.2772, grad_fn=<NllLossBackward>) average train loss tensor(0.2489, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41043565466182935 val_avg_loss: tensor(1.3532)\n",
      "epoch: 30 train_loss: tensor(0.2690, grad_fn=<NllLossBackward>) average train loss tensor(0.2465, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4042415125505118 val_avg_loss: tensor(1.4009)\n",
      "epoch: 31 train_loss: tensor(0.2694, grad_fn=<NllLossBackward>) average train loss tensor(0.2446, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4087248916025131 val_avg_loss: tensor(1.4011)\n",
      "epoch: 32 train_loss: tensor(0.2666, grad_fn=<NllLossBackward>) average train loss tensor(0.2431, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4275432852549922 val_avg_loss: tensor(1.3759)\n",
      "epoch: 33 train_loss: tensor(0.2678, grad_fn=<NllLossBackward>) average train loss tensor(0.2466, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4264519364068077 val_avg_loss: tensor(1.3642)\n",
      "epoch: 34 train_loss: tensor(0.2720, grad_fn=<NllLossBackward>) average train loss tensor(0.2433, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.409314809898829 val_avg_loss: tensor(1.3940)\n",
      "epoch: 35 train_loss: tensor(0.2688, grad_fn=<NllLossBackward>) average train loss tensor(0.2456, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4167772763472259 val_avg_loss: tensor(1.3691)\n",
      "epoch: 36 train_loss: tensor(0.2889, grad_fn=<NllLossBackward>) average train loss tensor(0.2487, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39397693419461405 val_avg_loss: tensor(1.4004)\n",
      "epoch: 37 train_loss: tensor(0.2597, grad_fn=<NllLossBackward>) average train loss tensor(0.2393, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41618735805090995 val_avg_loss: tensor(1.3879)\n",
      "epoch: 38 train_loss: tensor(0.2561, grad_fn=<NllLossBackward>) average train loss tensor(0.2448, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.426835383299413 val_avg_loss: tensor(1.3607)\n",
      "epoch: 39 train_loss: tensor(0.2475, grad_fn=<NllLossBackward>) average train loss tensor(0.2387, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4552694451818423 val_avg_loss: tensor(1.3187)\n",
      "epoch: 40 train_loss: tensor(0.2620, grad_fn=<NllLossBackward>) average train loss tensor(0.2388, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44122938972952247 val_avg_loss: tensor(1.3501)\n",
      "epoch: 41 train_loss: tensor(0.2476, grad_fn=<NllLossBackward>) average train loss tensor(0.2331, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4675987375748459 val_avg_loss: tensor(1.2560)\n",
      "epoch: 42 train_loss: tensor(0.2532, grad_fn=<NllLossBackward>) average train loss tensor(0.2344, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4536471698669734 val_avg_loss: tensor(1.3253)\n",
      "epoch: 43 train_loss: tensor(0.2509, grad_fn=<NllLossBackward>) average train loss tensor(0.2395, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4555054125003687 val_avg_loss: tensor(1.2533)\n",
      "epoch: 44 train_loss: tensor(0.2621, grad_fn=<NllLossBackward>) average train loss tensor(0.2338, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4586024835560275 val_avg_loss: tensor(1.2951)\n",
      "epoch: 45 train_loss: tensor(0.2543, grad_fn=<NllLossBackward>) average train loss tensor(0.2358, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47500221219361116 val_avg_loss: tensor(1.2814)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.2790, grad_fn=<NllLossBackward>) average train loss tensor(0.2370, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43347196413296757 val_avg_loss: tensor(1.3404)\n",
      "epoch: 47 train_loss: tensor(0.2455, grad_fn=<NllLossBackward>) average train loss tensor(0.2344, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4466566380556293 val_avg_loss: tensor(1.3311)\n",
      "epoch: 48 train_loss: tensor(0.2569, grad_fn=<NllLossBackward>) average train loss tensor(0.2337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4570097041559744 val_avg_loss: tensor(1.3299)\n",
      "epoch: 49 train_loss: tensor(0.2440, grad_fn=<NllLossBackward>) average train loss tensor(0.2338, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48576822110137746 val_avg_loss: tensor(1.2244)\n",
      "epoch: 50 train_loss: tensor(0.2573, grad_fn=<NllLossBackward>) average train loss tensor(0.2296, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45591835530778985 val_avg_loss: tensor(1.2939)\n",
      "epoch: 51 train_loss: tensor(0.2469, grad_fn=<NllLossBackward>) average train loss tensor(0.2321, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4433530955962599 val_avg_loss: tensor(1.2983)\n",
      "epoch: 52 train_loss: tensor(0.2528, grad_fn=<NllLossBackward>) average train loss tensor(0.2327, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42482966109193876 val_avg_loss: tensor(1.3587)\n",
      "epoch: 53 train_loss: tensor(0.2873, grad_fn=<NllLossBackward>) average train loss tensor(0.2342, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44267468955549655 val_avg_loss: tensor(1.3229)\n",
      "epoch: 54 train_loss: tensor(0.2616, grad_fn=<NllLossBackward>) average train loss tensor(0.2337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43730643305902134 val_avg_loss: tensor(1.3361)\n",
      "epoch: 55 train_loss: tensor(0.2515, grad_fn=<NllLossBackward>) average train loss tensor(0.2355, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42028729021030586 val_avg_loss: tensor(1.3568)\n",
      "epoch: 56 train_loss: tensor(0.2551, grad_fn=<NllLossBackward>) average train loss tensor(0.2304, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47818777099371734 val_avg_loss: tensor(1.2600)\n",
      "epoch: 57 train_loss: tensor(0.2417, grad_fn=<NllLossBackward>) average train loss tensor(0.2266, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44978320502610386 val_avg_loss: tensor(1.3311)\n",
      "epoch: 58 train_loss: tensor(0.2313, grad_fn=<NllLossBackward>) average train loss tensor(0.2256, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48562074152729845 val_avg_loss: tensor(1.1991)\n",
      "epoch: 59 train_loss: tensor(0.2615, grad_fn=<NllLossBackward>) average train loss tensor(0.2309, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4426451936406808 val_avg_loss: tensor(1.3172)\n",
      "epoch: 60 train_loss: tensor(0.2441, grad_fn=<NllLossBackward>) average train loss tensor(0.2256, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4734389287083739 val_avg_loss: tensor(1.2596)\n",
      "epoch: 61 train_loss: tensor(0.2438, grad_fn=<NllLossBackward>) average train loss tensor(0.2233, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4959738076276436 val_avg_loss: tensor(1.2220)\n",
      "epoch: 62 train_loss: tensor(0.2417, grad_fn=<NllLossBackward>) average train loss tensor(0.2209, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4984514644721706 val_avg_loss: tensor(1.2131)\n",
      "epoch: 63 train_loss: tensor(0.2440, grad_fn=<NllLossBackward>) average train loss tensor(0.2243, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4732619532194791 val_avg_loss: tensor(1.2685)\n",
      "epoch: 64 train_loss: tensor(0.2478, grad_fn=<NllLossBackward>) average train loss tensor(0.2303, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4692800047193464 val_avg_loss: tensor(1.2600)\n",
      "epoch: 65 train_loss: tensor(0.2682, grad_fn=<NllLossBackward>) average train loss tensor(0.2271, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47809928324927 val_avg_loss: tensor(1.2614)\n",
      "epoch: 66 train_loss: tensor(0.2473, grad_fn=<NllLossBackward>) average train loss tensor(0.2223, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4971241483054597 val_avg_loss: tensor(1.2098)\n",
      "epoch: 67 train_loss: tensor(0.2359, grad_fn=<NllLossBackward>) average train loss tensor(0.2178, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46659587647110873 val_avg_loss: tensor(1.2856)\n",
      "epoch: 68 train_loss: tensor(0.2356, grad_fn=<NllLossBackward>) average train loss tensor(0.2279, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4618470341857653 val_avg_loss: tensor(1.2381)\n",
      "epoch: 69 train_loss: tensor(0.2336, grad_fn=<NllLossBackward>) average train loss tensor(0.2191, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5110167241837006 val_avg_loss: tensor(1.1891)\n",
      "epoch: 70 train_loss: tensor(0.2985, grad_fn=<NllLossBackward>) average train loss tensor(0.2224, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5079196531280418 val_avg_loss: tensor(1.1486)\n",
      "epoch: 71 train_loss: tensor(0.2298, grad_fn=<NllLossBackward>) average train loss tensor(0.2231, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5116656343096481 val_avg_loss: tensor(1.1783)\n",
      "epoch: 72 train_loss: tensor(0.2463, grad_fn=<NllLossBackward>) average train loss tensor(0.2236, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.494882458779459 val_avg_loss: tensor(1.2116)\n",
      "epoch: 73 train_loss: tensor(0.2336, grad_fn=<NllLossBackward>) average train loss tensor(0.2183, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5184201988024658 val_avg_loss: tensor(1.1703)\n",
      "epoch: 74 train_loss: tensor(0.2372, grad_fn=<NllLossBackward>) average train loss tensor(0.2211, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46105064448573874 val_avg_loss: tensor(1.2953)\n",
      "epoch: 75 train_loss: tensor(0.2859, grad_fn=<NllLossBackward>) average train loss tensor(0.2229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4676577294044775 val_avg_loss: tensor(1.2735)\n",
      "epoch: 76 train_loss: tensor(0.2249, grad_fn=<NllLossBackward>) average train loss tensor(0.2229, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5006341621685396 val_avg_loss: tensor(1.1692)\n",
      "epoch: 77 train_loss: tensor(0.2230, grad_fn=<NllLossBackward>) average train loss tensor(0.2254, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5088340264873316 val_avg_loss: tensor(1.1620)\n",
      "epoch: 78 train_loss: tensor(0.2256, grad_fn=<NllLossBackward>) average train loss tensor(0.2180, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5066218328761467 val_avg_loss: tensor(1.1824)\n",
      "epoch: 79 train_loss: tensor(0.2285, grad_fn=<NllLossBackward>) average train loss tensor(0.2159, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47854172197150696 val_avg_loss: tensor(1.2654)\n",
      "epoch: 80 train_loss: tensor(0.2295, grad_fn=<NllLossBackward>) average train loss tensor(0.2165, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5198949945432557 val_avg_loss: tensor(1.1564)\n",
      "epoch: 81 train_loss: tensor(0.2393, grad_fn=<NllLossBackward>) average train loss tensor(0.2196, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5203964250951243 val_avg_loss: tensor(1.1481)\n",
      "epoch: 82 train_loss: tensor(0.2678, grad_fn=<NllLossBackward>) average train loss tensor(0.2257, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4943810282275905 val_avg_loss: tensor(1.2190)\n",
      "epoch: 83 train_loss: tensor(0.2340, grad_fn=<NllLossBackward>) average train loss tensor(0.2142, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5330501725511017 val_avg_loss: tensor(1.1759)\n",
      "epoch: 84 train_loss: tensor(0.2351, grad_fn=<NllLossBackward>) average train loss tensor(0.2201, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5178302805061499 val_avg_loss: tensor(1.1581)\n",
      "epoch: 85 train_loss: tensor(0.2253, grad_fn=<NllLossBackward>) average train loss tensor(0.2162, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5299236055806271 val_avg_loss: tensor(1.1521)\n",
      "epoch: 86 train_loss: tensor(0.2290, grad_fn=<NllLossBackward>) average train loss tensor(0.2126, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5577087573371088 val_avg_loss: tensor(1.0928)\n",
      "epoch: 87 train_loss: tensor(0.2336, grad_fn=<NllLossBackward>) average train loss tensor(0.2177, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5215762616877563 val_avg_loss: tensor(1.1408)\n",
      "epoch: 88 train_loss: tensor(0.2174, grad_fn=<NllLossBackward>) average train loss tensor(0.2144, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5506297377813173 val_avg_loss: tensor(1.0816)\n",
      "epoch: 89 train_loss: tensor(0.2226, grad_fn=<NllLossBackward>) average train loss tensor(0.2144, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5062088900687255 val_avg_loss: tensor(1.1705)\n",
      "epoch: 90 train_loss: tensor(0.2251, grad_fn=<NllLossBackward>) average train loss tensor(0.2118, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5214287821136773 val_avg_loss: tensor(1.1756)\n",
      "epoch: 91 train_loss: tensor(0.2334, grad_fn=<NllLossBackward>) average train loss tensor(0.2103, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5207503760729139 val_avg_loss: tensor(1.1797)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 train_loss: tensor(0.2355, grad_fn=<NllLossBackward>) average train loss tensor(0.2166, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5245848449989676 val_avg_loss: tensor(1.1127)\n",
      "epoch: 93 train_loss: tensor(0.2121, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5469722443441584 val_avg_loss: tensor(1.0687)\n",
      "epoch: 94 train_loss: tensor(0.2275, grad_fn=<NllLossBackward>) average train loss tensor(0.2148, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5201309618617821 val_avg_loss: tensor(1.1133)\n",
      "epoch: 95 train_loss: tensor(0.2281, grad_fn=<NllLossBackward>) average train loss tensor(0.2117, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5027578680352771 val_avg_loss: tensor(1.2100)\n",
      "epoch: 96 train_loss: tensor(0.2341, grad_fn=<NllLossBackward>) average train loss tensor(0.2047, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5577087573371088 val_avg_loss: tensor(1.1023)\n",
      "epoch: 97 train_loss: tensor(0.2371, grad_fn=<NllLossBackward>) average train loss tensor(0.2135, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.506562841046515 val_avg_loss: tensor(1.2152)\n",
      "epoch: 98 train_loss: tensor(0.2362, grad_fn=<NllLossBackward>) average train loss tensor(0.2189, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5074182225761732 val_avg_loss: tensor(1.1814)\n",
      "epoch: 99 train_loss: tensor(0.2397, grad_fn=<NllLossBackward>) average train loss tensor(0.2117, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5044096392649619 val_avg_loss: tensor(1.1837)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.50      0.67     41251\n",
      "           1       0.04      0.74      0.07      1128\n",
      "\n",
      "    accuracy                           0.51     42379\n",
      "   macro avg       0.51      0.62      0.37     42379\n",
      "weighted avg       0.96      0.51      0.65     42379\n",
      "\n",
      "roc auc score: 0.6888603581671178\n"
     ]
    }
   ],
   "source": [
    "model = WtsNormModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/norm_bn_d128_drop05_wd0005_weight300')\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 300]).float()\n",
    ")\n",
    "\n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.4619, grad_fn=<NllLossBackward>) average train loss tensor(0.5538, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29622747249505943 val_avg_loss: tensor(1.0865)\n",
      "epoch: 1 train_loss: tensor(0.3213, grad_fn=<NllLossBackward>) average train loss tensor(0.3647, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18951125269150224 val_avg_loss: tensor(1.4988)\n",
      "epoch: 2 train_loss: tensor(0.3050, grad_fn=<NllLossBackward>) average train loss tensor(0.2968, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12379435448190426 val_avg_loss: tensor(1.5583)\n",
      "epoch: 3 train_loss: tensor(0.3108, grad_fn=<NllLossBackward>) average train loss tensor(0.2752, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12553461345603634 val_avg_loss: tensor(1.8261)\n",
      "epoch: 4 train_loss: tensor(0.3124, grad_fn=<NllLossBackward>) average train loss tensor(0.2623, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12818924578945817 val_avg_loss: tensor(1.8401)\n",
      "epoch: 5 train_loss: tensor(0.3076, grad_fn=<NllLossBackward>) average train loss tensor(0.2573, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15798011975341414 val_avg_loss: tensor(1.8285)\n",
      "epoch: 6 train_loss: tensor(0.2785, grad_fn=<NllLossBackward>) average train loss tensor(0.2535, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17113529776126007 val_avg_loss: tensor(1.8835)\n",
      "epoch: 7 train_loss: tensor(0.2882, grad_fn=<NllLossBackward>) average train loss tensor(0.2501, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18614871840250125 val_avg_loss: tensor(1.7898)\n",
      "epoch: 8 train_loss: tensor(0.2839, grad_fn=<NllLossBackward>) average train loss tensor(0.2488, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19190042179158187 val_avg_loss: tensor(1.7897)\n",
      "epoch: 9 train_loss: tensor(0.3021, grad_fn=<NllLossBackward>) average train loss tensor(0.2448, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21411084564787777 val_avg_loss: tensor(1.7550)\n",
      "epoch: 10 train_loss: tensor(0.2773, grad_fn=<NllLossBackward>) average train loss tensor(0.2421, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20095566764003187 val_avg_loss: tensor(1.7663)\n",
      "epoch: 11 train_loss: tensor(0.2652, grad_fn=<NllLossBackward>) average train loss tensor(0.2376, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21888918384803704 val_avg_loss: tensor(1.7493)\n",
      "epoch: 12 train_loss: tensor(0.2804, grad_fn=<NllLossBackward>) average train loss tensor(0.2390, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2298911600743297 val_avg_loss: tensor(1.6556)\n",
      "epoch: 13 train_loss: tensor(0.2702, grad_fn=<NllLossBackward>) average train loss tensor(0.2368, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24614340913783442 val_avg_loss: tensor(1.6946)\n",
      "epoch: 14 train_loss: tensor(0.2629, grad_fn=<NllLossBackward>) average train loss tensor(0.2340, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2516591452083886 val_avg_loss: tensor(1.6843)\n",
      "epoch: 15 train_loss: tensor(0.2677, grad_fn=<NllLossBackward>) average train loss tensor(0.2360, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25009586172315135 val_avg_loss: tensor(1.6756)\n",
      "epoch: 16 train_loss: tensor(0.2598, grad_fn=<NllLossBackward>) average train loss tensor(0.2289, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2636344866236026 val_avg_loss: tensor(1.6711)\n",
      "epoch: 17 train_loss: tensor(0.2580, grad_fn=<NllLossBackward>) average train loss tensor(0.2294, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28434061882429285 val_avg_loss: tensor(1.6198)\n",
      "epoch: 18 train_loss: tensor(0.2529, grad_fn=<NllLossBackward>) average train loss tensor(0.2265, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27628823407958 val_avg_loss: tensor(1.6363)\n",
      "epoch: 19 train_loss: tensor(0.2607, grad_fn=<NllLossBackward>) average train loss tensor(0.2243, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2950771318172433 val_avg_loss: tensor(1.6251)\n",
      "epoch: 20 train_loss: tensor(0.2480, grad_fn=<NllLossBackward>) average train loss tensor(0.2217, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3157832640179335 val_avg_loss: tensor(1.6017)\n",
      "epoch: 21 train_loss: tensor(0.2569, grad_fn=<NllLossBackward>) average train loss tensor(0.2247, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2921865321652951 val_avg_loss: tensor(1.6518)\n",
      "epoch: 22 train_loss: tensor(0.2641, grad_fn=<NllLossBackward>) average train loss tensor(0.2255, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2986461375099549 val_avg_loss: tensor(1.6396)\n",
      "epoch: 23 train_loss: tensor(0.2514, grad_fn=<NllLossBackward>) average train loss tensor(0.2189, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3174350352476182 val_avg_loss: tensor(1.6219)\n",
      "epoch: 24 train_loss: tensor(0.2491, grad_fn=<NllLossBackward>) average train loss tensor(0.2162, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3291154175146742 val_avg_loss: tensor(1.6042)\n",
      "epoch: 25 train_loss: tensor(0.2657, grad_fn=<NllLossBackward>) average train loss tensor(0.2194, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31634368639943367 val_avg_loss: tensor(1.6233)\n",
      "epoch: 26 train_loss: tensor(0.2590, grad_fn=<NllLossBackward>) average train loss tensor(0.2178, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29413326254313776 val_avg_loss: tensor(1.7053)\n",
      "epoch: 27 train_loss: tensor(0.2645, grad_fn=<NllLossBackward>) average train loss tensor(0.2232, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3114473645400112 val_avg_loss: tensor(1.6362)\n",
      "epoch: 28 train_loss: tensor(0.2457, grad_fn=<NllLossBackward>) average train loss tensor(0.2202, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2995900067840604 val_avg_loss: tensor(1.6667)\n",
      "epoch: 29 train_loss: tensor(0.2613, grad_fn=<NllLossBackward>) average train loss tensor(0.2177, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3096481137362475 val_avg_loss: tensor(1.6672)\n",
      "epoch: 30 train_loss: tensor(0.2450, grad_fn=<NllLossBackward>) average train loss tensor(0.2110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3486417131227325 val_avg_loss: tensor(1.6245)\n",
      "epoch: 31 train_loss: tensor(0.2370, grad_fn=<NllLossBackward>) average train loss tensor(0.2112, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36436303571955286 val_avg_loss: tensor(1.6092)\n",
      "epoch: 32 train_loss: tensor(0.2404, grad_fn=<NllLossBackward>) average train loss tensor(0.2151, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33292039052591216 val_avg_loss: tensor(1.6483)\n",
      "epoch: 33 train_loss: tensor(0.2443, grad_fn=<NllLossBackward>) average train loss tensor(0.2143, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.314839394743828 val_avg_loss: tensor(1.6239)\n",
      "epoch: 34 train_loss: tensor(0.2325, grad_fn=<NllLossBackward>) average train loss tensor(0.2109, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3548948470636817 val_avg_loss: tensor(1.5889)\n",
      "epoch: 35 train_loss: tensor(0.2443, grad_fn=<NllLossBackward>) average train loss tensor(0.2046, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3616199156416836 val_avg_loss: tensor(1.6200)\n",
      "epoch: 36 train_loss: tensor(0.2376, grad_fn=<NllLossBackward>) average train loss tensor(0.2070, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37427366309766097 val_avg_loss: tensor(1.5621)\n",
      "epoch: 37 train_loss: tensor(0.2328, grad_fn=<NllLossBackward>) average train loss tensor(0.2124, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3324484558888594 val_avg_loss: tensor(1.6005)\n",
      "epoch: 38 train_loss: tensor(0.2343, grad_fn=<NllLossBackward>) average train loss tensor(0.2132, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34988054154499604 val_avg_loss: tensor(1.5767)\n",
      "epoch: 39 train_loss: tensor(0.2418, grad_fn=<NllLossBackward>) average train loss tensor(0.2055, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36185588296021 val_avg_loss: tensor(1.6278)\n",
      "epoch: 40 train_loss: tensor(0.2926, grad_fn=<NllLossBackward>) average train loss tensor(0.2079, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3485827212931009 val_avg_loss: tensor(1.6524)\n",
      "epoch: 41 train_loss: tensor(0.2443, grad_fn=<NllLossBackward>) average train loss tensor(0.2167, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3454856502374421 val_avg_loss: tensor(1.6148)\n",
      "epoch: 42 train_loss: tensor(0.2461, grad_fn=<NllLossBackward>) average train loss tensor(0.2090, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3617378993009468 val_avg_loss: tensor(1.6119)\n",
      "epoch: 43 train_loss: tensor(0.2345, grad_fn=<NllLossBackward>) average train loss tensor(0.2094, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3720614694864761 val_avg_loss: tensor(1.5477)\n",
      "epoch: 44 train_loss: tensor(0.2381, grad_fn=<NllLossBackward>) average train loss tensor(0.2091, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3540689614488393 val_avg_loss: tensor(1.6857)\n",
      "epoch: 45 train_loss: tensor(0.2259, grad_fn=<NllLossBackward>) average train loss tensor(0.2009, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37636787304958264 val_avg_loss: tensor(1.5684)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.2219, grad_fn=<NllLossBackward>) average train loss tensor(0.2047, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3608530218564729 val_avg_loss: tensor(1.5872)\n",
      "epoch: 47 train_loss: tensor(0.2356, grad_fn=<NllLossBackward>) average train loss tensor(0.2050, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36858095153821196 val_avg_loss: tensor(1.5618)\n",
      "epoch: 48 train_loss: tensor(0.2168, grad_fn=<NllLossBackward>) average train loss tensor(0.2014, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4046249594431171 val_avg_loss: tensor(1.4709)\n",
      "epoch: 49 train_loss: tensor(0.2298, grad_fn=<NllLossBackward>) average train loss tensor(0.2060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3511783617968911 val_avg_loss: tensor(1.6068)\n",
      "epoch: 50 train_loss: tensor(0.2251, grad_fn=<NllLossBackward>) average train loss tensor(0.1997, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40294369229861665 val_avg_loss: tensor(1.5399)\n",
      "epoch: 51 train_loss: tensor(0.2307, grad_fn=<NllLossBackward>) average train loss tensor(0.2046, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3784030911718727 val_avg_loss: tensor(1.5454)\n",
      "epoch: 52 train_loss: tensor(0.2244, grad_fn=<NllLossBackward>) average train loss tensor(0.1990, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40851842019880247 val_avg_loss: tensor(1.5613)\n",
      "epoch: 53 train_loss: tensor(0.2521, grad_fn=<NllLossBackward>) average train loss tensor(0.2073, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35577972450815565 val_avg_loss: tensor(1.5926)\n",
      "epoch: 54 train_loss: tensor(0.2288, grad_fn=<NllLossBackward>) average train loss tensor(0.2056, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3975459398873256 val_avg_loss: tensor(1.4992)\n",
      "epoch: 55 train_loss: tensor(0.2242, grad_fn=<NllLossBackward>) average train loss tensor(0.2017, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4009674660059582 val_avg_loss: tensor(1.5064)\n",
      "epoch: 56 train_loss: tensor(0.2335, grad_fn=<NllLossBackward>) average train loss tensor(0.2008, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3812936908238209 val_avg_loss: tensor(1.6262)\n",
      "epoch: 57 train_loss: tensor(0.2262, grad_fn=<NllLossBackward>) average train loss tensor(0.2058, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3900834734389287 val_avg_loss: tensor(1.5358)\n",
      "epoch: 58 train_loss: tensor(0.2324, grad_fn=<NllLossBackward>) average train loss tensor(0.2049, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39775241129103617 val_avg_loss: tensor(1.5248)\n",
      "epoch: 59 train_loss: tensor(0.2272, grad_fn=<NllLossBackward>) average train loss tensor(0.2027, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3769577913458986 val_avg_loss: tensor(1.5976)\n",
      "epoch: 60 train_loss: tensor(0.2121, grad_fn=<NllLossBackward>) average train loss tensor(0.1950, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44479839542223404 val_avg_loss: tensor(1.3954)\n",
      "epoch: 61 train_loss: tensor(0.2212, grad_fn=<NllLossBackward>) average train loss tensor(0.1991, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3945963484057458 val_avg_loss: tensor(1.4816)\n",
      "epoch: 62 train_loss: tensor(0.2302, grad_fn=<NllLossBackward>) average train loss tensor(0.1976, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3974574521428782 val_avg_loss: tensor(1.5238)\n",
      "epoch: 63 train_loss: tensor(0.2229, grad_fn=<NllLossBackward>) average train loss tensor(0.2038, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3831224375424004 val_avg_loss: tensor(1.4957)\n",
      "epoch: 64 train_loss: tensor(0.2277, grad_fn=<NllLossBackward>) average train loss tensor(0.1985, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40530336548388046 val_avg_loss: tensor(1.5103)\n",
      "epoch: 65 train_loss: tensor(0.2343, grad_fn=<NllLossBackward>) average train loss tensor(0.1975, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39834232958735216 val_avg_loss: tensor(1.5388)\n",
      "epoch: 66 train_loss: tensor(0.2270, grad_fn=<NllLossBackward>) average train loss tensor(0.1981, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37604341798660884 val_avg_loss: tensor(1.5545)\n",
      "epoch: 67 train_loss: tensor(0.2196, grad_fn=<NllLossBackward>) average train loss tensor(0.1958, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3959826564020883 val_avg_loss: tensor(1.5245)\n",
      "epoch: 68 train_loss: tensor(0.2495, grad_fn=<NllLossBackward>) average train loss tensor(0.1991, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3828274783942424 val_avg_loss: tensor(1.5307)\n",
      "epoch: 69 train_loss: tensor(0.2148, grad_fn=<NllLossBackward>) average train loss tensor(0.1945, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4709317759490311 val_avg_loss: tensor(1.3618)\n",
      "epoch: 70 train_loss: tensor(0.2173, grad_fn=<NllLossBackward>) average train loss tensor(0.1951, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4231483939474383 val_avg_loss: tensor(1.4680)\n",
      "epoch: 71 train_loss: tensor(0.2208, grad_fn=<NllLossBackward>) average train loss tensor(0.1966, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3996991416688789 val_avg_loss: tensor(1.5508)\n",
      "epoch: 72 train_loss: tensor(0.2201, grad_fn=<NllLossBackward>) average train loss tensor(0.1935, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4110845647877769 val_avg_loss: tensor(1.5419)\n",
      "epoch: 73 train_loss: tensor(0.2301, grad_fn=<NllLossBackward>) average train loss tensor(0.1998, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38840220629442823 val_avg_loss: tensor(1.5342)\n",
      "epoch: 74 train_loss: tensor(0.2267, grad_fn=<NllLossBackward>) average train loss tensor(0.1962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39415390968350883 val_avg_loss: tensor(1.4722)\n",
      "epoch: 75 train_loss: tensor(0.2103, grad_fn=<NllLossBackward>) average train loss tensor(0.1957, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4021767985134059 val_avg_loss: tensor(1.4805)\n",
      "epoch: 76 train_loss: tensor(0.2154, grad_fn=<NllLossBackward>) average train loss tensor(0.1931, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4214671268029378 val_avg_loss: tensor(1.4818)\n",
      "epoch: 77 train_loss: tensor(0.2209, grad_fn=<NllLossBackward>) average train loss tensor(0.1925, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37905200129782024 val_avg_loss: tensor(1.6052)\n",
      "epoch: 78 train_loss: tensor(0.2746, grad_fn=<NllLossBackward>) average train loss tensor(0.1988, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3931510485797717 val_avg_loss: tensor(1.4690)\n",
      "epoch: 79 train_loss: tensor(0.2191, grad_fn=<NllLossBackward>) average train loss tensor(0.1949, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4007904905170634 val_avg_loss: tensor(1.4956)\n",
      "epoch: 80 train_loss: tensor(0.2135, grad_fn=<NllLossBackward>) average train loss tensor(0.1934, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4152139928619886 val_avg_loss: tensor(1.4470)\n",
      "epoch: 81 train_loss: tensor(0.2269, grad_fn=<NllLossBackward>) average train loss tensor(0.2001, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.377488717812583 val_avg_loss: tensor(1.4926)\n",
      "epoch: 82 train_loss: tensor(0.2273, grad_fn=<NllLossBackward>) average train loss tensor(0.1951, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4262159690882813 val_avg_loss: tensor(1.4695)\n",
      "epoch: 83 train_loss: tensor(0.2157, grad_fn=<NllLossBackward>) average train loss tensor(0.1940, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4081349733061971 val_avg_loss: tensor(1.4689)\n",
      "epoch: 84 train_loss: tensor(0.2188, grad_fn=<NllLossBackward>) average train loss tensor(0.1941, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4314367460106775 val_avg_loss: tensor(1.3916)\n",
      "epoch: 85 train_loss: tensor(0.2337, grad_fn=<NllLossBackward>) average train loss tensor(0.1972, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38356487626463737 val_avg_loss: tensor(1.5250)\n",
      "epoch: 86 train_loss: tensor(0.2139, grad_fn=<NllLossBackward>) average train loss tensor(0.1957, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41503701737309384 val_avg_loss: tensor(1.4759)\n",
      "epoch: 87 train_loss: tensor(0.2171, grad_fn=<NllLossBackward>) average train loss tensor(0.1903, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4084004365395393 val_avg_loss: tensor(1.4815)\n",
      "epoch: 88 train_loss: tensor(0.2148, grad_fn=<NllLossBackward>) average train loss tensor(0.1912, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47007639441937293 val_avg_loss: tensor(1.3858)\n",
      "epoch: 89 train_loss: tensor(0.2039, grad_fn=<NllLossBackward>) average train loss tensor(0.1882, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4748547326195322 val_avg_loss: tensor(1.3328)\n",
      "epoch: 90 train_loss: tensor(0.2536, grad_fn=<NllLossBackward>) average train loss tensor(0.1904, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4353597026811787 val_avg_loss: tensor(1.3988)\n",
      "epoch: 91 train_loss: tensor(0.2156, grad_fn=<NllLossBackward>) average train loss tensor(0.1964, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4141226440138041 val_avg_loss: tensor(1.4571)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 92 train_loss: tensor(0.2083, grad_fn=<NllLossBackward>) average train loss tensor(0.1885, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44627319116302394 val_avg_loss: tensor(1.4210)\n",
      "epoch: 93 train_loss: tensor(0.2093, grad_fn=<NllLossBackward>) average train loss tensor(0.1894, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4287231218476241 val_avg_loss: tensor(1.4428)\n",
      "epoch: 94 train_loss: tensor(0.2380, grad_fn=<NllLossBackward>) average train loss tensor(0.1920, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4514349762557886 val_avg_loss: tensor(1.3781)\n",
      "epoch: 95 train_loss: tensor(0.2208, grad_fn=<NllLossBackward>) average train loss tensor(0.1918, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40671916939503877 val_avg_loss: tensor(1.5034)\n",
      "epoch: 96 train_loss: tensor(0.2037, grad_fn=<NllLossBackward>) average train loss tensor(0.1912, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43400289059965197 val_avg_loss: tensor(1.4004)\n",
      "epoch: 97 train_loss: tensor(0.2119, grad_fn=<NllLossBackward>) average train loss tensor(0.1895, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4169247559213049 val_avg_loss: tensor(1.4509)\n",
      "epoch: 98 train_loss: tensor(0.2111, grad_fn=<NllLossBackward>) average train loss tensor(0.1922, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42111317582514823 val_avg_loss: tensor(1.4297)\n",
      "epoch: 99 train_loss: tensor(0.2100, grad_fn=<NllLossBackward>) average train loss tensor(0.1870, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4881278942866413 val_avg_loss: tensor(1.2847)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.48      0.65     41251\n",
      "           1       0.04      0.77      0.07      1128\n",
      "\n",
      "    accuracy                           0.49     42379\n",
      "   macro avg       0.51      0.63      0.36     42379\n",
      "weighted avg       0.96      0.49      0.63     42379\n",
      "\n",
      "roc auc score: 0.6855564677048018\n"
     ]
    }
   ],
   "source": [
    "model = WtsNormModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/norm_bn_d128_drop05_wd0005_weight400')\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 400]).float()\n",
    ")\n",
    "\n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(1.1756, grad_fn=<AddBackward0>) average train loss tensor(1.7794, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.9943)\n",
      "epoch: 1 train_loss: tensor(0.3762, grad_fn=<AddBackward0>) average train loss tensor(0.6927, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3632)\n",
      "epoch: 2 train_loss: tensor(0.3254, grad_fn=<AddBackward0>) average train loss tensor(0.4276, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3501)\n",
      "epoch: 3 train_loss: tensor(0.2988, grad_fn=<AddBackward0>) average train loss tensor(0.3949, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3456)\n",
      "epoch: 4 train_loss: tensor(0.2754, grad_fn=<AddBackward0>) average train loss tensor(0.3815, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3440)\n",
      "epoch: 5 train_loss: tensor(0.2770, grad_fn=<AddBackward0>) average train loss tensor(0.3707, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3415)\n",
      "epoch: 6 train_loss: tensor(0.2423, grad_fn=<AddBackward0>) average train loss tensor(0.3620, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3407)\n",
      "epoch: 7 train_loss: tensor(0.2472, grad_fn=<AddBackward0>) average train loss tensor(0.3575, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3404)\n",
      "epoch: 8 train_loss: tensor(0.2416, grad_fn=<AddBackward0>) average train loss tensor(0.3527, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3413)\n",
      "epoch: 9 train_loss: tensor(0.2329, grad_fn=<AddBackward0>) average train loss tensor(0.3482, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3414)\n",
      "epoch: 10 train_loss: tensor(0.2263, grad_fn=<AddBackward0>) average train loss tensor(0.3468, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3422)\n",
      "epoch: 11 train_loss: tensor(0.2340, grad_fn=<AddBackward0>) average train loss tensor(0.3429, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.3427)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-067de3918d61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwts_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwts_val_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-67-d54f4b3b8b10>\u001b[0m in \u001b[0;36mfit_wts_trident_model\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, writer, epochs, es_dif, es_tol)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx_img_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_txt_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_wts_cur\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mout_common\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_img_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_txt_cur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = WtsTridentModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/trident_bn_d128_drop05_wd0005')\n",
    "\n",
    "fit_wts_trident_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())[0]\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99     41251\n",
      "           1       0.00      0.00      0.00      1128\n",
      "\n",
      "    accuracy                           0.97     42379\n",
      "   macro avg       0.49      0.50      0.49     42379\n",
      "weighted avg       0.95      0.97      0.96     42379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(1.6483, grad_fn=<AddBackward0>) average train loss tensor(1.7840, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19316874612866117 val_avg_loss: tensor(3.3955)\n",
      "epoch: 1 train_loss: tensor(1.4027, grad_fn=<AddBackward0>) average train loss tensor(1.3853, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17346547503170812 val_avg_loss: tensor(4.0631)\n",
      "epoch: 2 train_loss: tensor(1.3353, grad_fn=<AddBackward0>) average train loss tensor(1.2676, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17086983452791787 val_avg_loss: tensor(4.4315)\n",
      "epoch: 3 train_loss: tensor(1.2844, grad_fn=<AddBackward0>) average train loss tensor(1.2223, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1766805297466301 val_avg_loss: tensor(4.5140)\n",
      "epoch: 4 train_loss: tensor(1.2315, grad_fn=<AddBackward0>) average train loss tensor(1.1869, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1853523287024747 val_avg_loss: tensor(4.5919)\n",
      "epoch: 5 train_loss: tensor(1.2346, grad_fn=<AddBackward0>) average train loss tensor(1.1656, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2296846886706191 val_avg_loss: tensor(4.5317)\n",
      "epoch: 6 train_loss: tensor(1.1777, grad_fn=<AddBackward0>) average train loss tensor(1.1515, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2549921835825738 val_avg_loss: tensor(4.4674)\n",
      "epoch: 7 train_loss: tensor(1.1615, grad_fn=<AddBackward0>) average train loss tensor(1.1200, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2908297200837684 val_avg_loss: tensor(4.4429)\n",
      "epoch: 8 train_loss: tensor(1.1733, grad_fn=<AddBackward0>) average train loss tensor(1.1114, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2882045836651624 val_avg_loss: tensor(4.4969)\n",
      "epoch: 9 train_loss: tensor(1.1411, grad_fn=<AddBackward0>) average train loss tensor(1.0892, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3223313571070407 val_avg_loss: tensor(4.4400)\n",
      "epoch: 10 train_loss: tensor(1.1149, grad_fn=<AddBackward0>) average train loss tensor(1.0750, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3431259770521783 val_avg_loss: tensor(4.3569)\n",
      "epoch: 11 train_loss: tensor(1.1085, grad_fn=<AddBackward0>) average train loss tensor(1.0689, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3520927351561809 val_avg_loss: tensor(4.3837)\n",
      "epoch: 12 train_loss: tensor(1.0992, grad_fn=<AddBackward0>) average train loss tensor(1.0484, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3753650119458455 val_avg_loss: tensor(4.2942)\n",
      "epoch: 13 train_loss: tensor(1.1166, grad_fn=<AddBackward0>) average train loss tensor(1.0306, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4057163082913017 val_avg_loss: tensor(4.2490)\n",
      "epoch: 14 train_loss: tensor(1.0460, grad_fn=<AddBackward0>) average train loss tensor(1.0147, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4147420582249359 val_avg_loss: tensor(4.2437)\n",
      "epoch: 15 train_loss: tensor(1.0426, grad_fn=<AddBackward0>) average train loss tensor(0.9993, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43252809485886207 val_avg_loss: tensor(4.2403)\n",
      "epoch: 16 train_loss: tensor(1.0054, grad_fn=<AddBackward0>) average train loss tensor(0.9876, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4582780284930537 val_avg_loss: tensor(4.1429)\n",
      "epoch: 17 train_loss: tensor(0.9921, grad_fn=<AddBackward0>) average train loss tensor(0.9818, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4347402884700469 val_avg_loss: tensor(4.1250)\n",
      "epoch: 18 train_loss: tensor(1.0135, grad_fn=<AddBackward0>) average train loss tensor(0.9756, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4452113382296552 val_avg_loss: tensor(4.0991)\n",
      "epoch: 19 train_loss: tensor(1.0037, grad_fn=<AddBackward0>) average train loss tensor(0.9555, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4789251688641123 val_avg_loss: tensor(3.9859)\n",
      "epoch: 20 train_loss: tensor(1.0033, grad_fn=<AddBackward0>) average train loss tensor(0.9615, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47358640828245285 val_avg_loss: tensor(3.9610)\n",
      "epoch: 21 train_loss: tensor(0.9794, grad_fn=<AddBackward0>) average train loss tensor(0.9516, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4621714892487391 val_avg_loss: tensor(4.1089)\n",
      "epoch: 22 train_loss: tensor(0.9675, grad_fn=<AddBackward0>) average train loss tensor(0.9194, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48390997846798217 val_avg_loss: tensor(4.0175)\n",
      "epoch: 23 train_loss: tensor(0.9583, grad_fn=<AddBackward0>) average train loss tensor(0.9306, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48609267616435126 val_avg_loss: tensor(3.9057)\n",
      "epoch: 24 train_loss: tensor(0.9278, grad_fn=<AddBackward0>) average train loss tensor(0.9231, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5139368197504646 val_avg_loss: tensor(3.7759)\n",
      "epoch: 25 train_loss: tensor(1.0313, grad_fn=<AddBackward0>) average train loss tensor(0.9080, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5024924048019349 val_avg_loss: tensor(3.8277)\n",
      "epoch: 26 train_loss: tensor(0.9071, grad_fn=<AddBackward0>) average train loss tensor(0.9052, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5029643394389877 val_avg_loss: tensor(3.8489)\n",
      "epoch: 27 train_loss: tensor(0.8844, grad_fn=<AddBackward0>) average train loss tensor(0.8919, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.545851399581158 val_avg_loss: tensor(3.6428)\n",
      "epoch: 28 train_loss: tensor(0.9326, grad_fn=<AddBackward0>) average train loss tensor(0.8745, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5312214258325222 val_avg_loss: tensor(3.7109)\n",
      "epoch: 29 train_loss: tensor(0.8637, grad_fn=<AddBackward0>) average train loss tensor(0.8744, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5345249682918916 val_avg_loss: tensor(3.7394)\n",
      "epoch: 30 train_loss: tensor(0.9197, grad_fn=<AddBackward0>) average train loss tensor(0.8771, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5619856649853995 val_avg_loss: tensor(3.5696)\n",
      "epoch: 31 train_loss: tensor(0.8879, grad_fn=<AddBackward0>) average train loss tensor(0.8618, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5419579388254727 val_avg_loss: tensor(3.6313)\n",
      "epoch: 32 train_loss: tensor(0.8711, grad_fn=<AddBackward0>) average train loss tensor(0.8684, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5517505825443176 val_avg_loss: tensor(3.5308)\n",
      "epoch: 33 train_loss: tensor(0.8614, grad_fn=<AddBackward0>) average train loss tensor(0.8505, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5910391410789606 val_avg_loss: tensor(3.4566)\n",
      "epoch: 34 train_loss: tensor(0.8101, grad_fn=<AddBackward0>) average train loss tensor(0.8362, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.581187505530484 val_avg_loss: tensor(3.4592)\n",
      "epoch: 35 train_loss: tensor(0.8705, grad_fn=<AddBackward0>) average train loss tensor(0.8515, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5786508568563254 val_avg_loss: tensor(3.4772)\n",
      "epoch: 36 train_loss: tensor(0.8761, grad_fn=<AddBackward0>) average train loss tensor(0.8582, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5570303512963455 val_avg_loss: tensor(3.5030)\n",
      "epoch: 37 train_loss: tensor(0.8328, grad_fn=<AddBackward0>) average train loss tensor(0.8353, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5895938412529864 val_avg_loss: tensor(3.4082)\n",
      "epoch: 38 train_loss: tensor(0.8058, grad_fn=<AddBackward0>) average train loss tensor(0.8240, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5870571925788278 val_avg_loss: tensor(3.4453)\n",
      "epoch: 39 train_loss: tensor(0.8307, grad_fn=<AddBackward0>) average train loss tensor(0.8249, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5898888004011444 val_avg_loss: tensor(3.4165)\n",
      "epoch: 40 train_loss: tensor(0.7969, grad_fn=<AddBackward0>) average train loss tensor(0.8174, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5990620299088576 val_avg_loss: tensor(3.3035)\n",
      "epoch: 41 train_loss: tensor(0.7958, grad_fn=<AddBackward0>) average train loss tensor(0.8178, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.595640503790225 val_avg_loss: tensor(3.3289)\n",
      "epoch: 42 train_loss: tensor(0.7885, grad_fn=<AddBackward0>) average train loss tensor(0.7993, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6219508598059169 val_avg_loss: tensor(3.2192)\n",
      "epoch: 43 train_loss: tensor(0.8353, grad_fn=<AddBackward0>) average train loss tensor(0.8005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5860248355602748 val_avg_loss: tensor(3.4052)\n",
      "epoch: 44 train_loss: tensor(0.8240, grad_fn=<AddBackward0>) average train loss tensor(0.7975, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6020116213904374 val_avg_loss: tensor(3.2621)\n",
      "epoch: 45 train_loss: tensor(0.8135, grad_fn=<AddBackward0>) average train loss tensor(0.8182, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6018051499867269 val_avg_loss: tensor(3.3249)\n",
      "epoch: 46 train_loss: tensor(0.7956, grad_fn=<AddBackward0>) average train loss tensor(0.7997, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.6000648910125947 val_avg_loss: tensor(3.3113)\n",
      "epoch: 47 train_loss: tensor(0.7556, grad_fn=<AddBackward0>) average train loss tensor(0.7751, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6251364186060231 val_avg_loss: tensor(3.2216)\n",
      "epoch: 48 train_loss: tensor(0.7554, grad_fn=<AddBackward0>) average train loss tensor(0.7740, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6158747013538625 val_avg_loss: tensor(3.2017)\n",
      "epoch: 49 train_loss: tensor(0.7420, grad_fn=<AddBackward0>) average train loss tensor(0.7759, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6265522225171813 val_avg_loss: tensor(3.2072)\n",
      "epoch: 50 train_loss: tensor(0.7664, grad_fn=<AddBackward0>) average train loss tensor(0.7724, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6014511990089373 val_avg_loss: tensor(3.2505)\n",
      "epoch: 51 train_loss: tensor(0.7814, grad_fn=<AddBackward0>) average train loss tensor(0.7786, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6099755183907029 val_avg_loss: tensor(3.1696)\n",
      "epoch: 52 train_loss: tensor(0.7624, grad_fn=<AddBackward0>) average train loss tensor(0.7614, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6002713624163053 val_avg_loss: tensor(3.2740)\n",
      "epoch: 53 train_loss: tensor(0.7702, grad_fn=<AddBackward0>) average train loss tensor(0.7703, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6362563784915789 val_avg_loss: tensor(3.1027)\n",
      "epoch: 54 train_loss: tensor(0.7421, grad_fn=<AddBackward0>) average train loss tensor(0.7494, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6623897590183759 val_avg_loss: tensor(3.0055)\n",
      "epoch: 55 train_loss: tensor(0.7173, grad_fn=<AddBackward0>) average train loss tensor(0.7603, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6467864200808188 val_avg_loss: tensor(3.0059)\n",
      "epoch: 56 train_loss: tensor(0.7652, grad_fn=<AddBackward0>) average train loss tensor(0.7596, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6492050850957143 val_avg_loss: tensor(3.1336)\n",
      "epoch: 57 train_loss: tensor(0.7108, grad_fn=<AddBackward0>) average train loss tensor(0.7479, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6264342388579182 val_avg_loss: tensor(3.1621)\n",
      "epoch: 58 train_loss: tensor(0.6909, grad_fn=<AddBackward0>) average train loss tensor(0.7301, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6571394861811639 val_avg_loss: tensor(3.0283)\n",
      "epoch: 59 train_loss: tensor(0.7361, grad_fn=<AddBackward0>) average train loss tensor(0.7349, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.676695277704038 val_avg_loss: tensor(2.9448)\n",
      "epoch: 60 train_loss: tensor(0.7340, grad_fn=<AddBackward0>) average train loss tensor(0.7393, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6346635990915258 val_avg_loss: tensor(3.0613)\n",
      "epoch: 61 train_loss: tensor(0.6898, grad_fn=<AddBackward0>) average train loss tensor(0.7342, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6653688464147716 val_avg_loss: tensor(2.9780)\n",
      "epoch: 62 train_loss: tensor(0.7272, grad_fn=<AddBackward0>) average train loss tensor(0.7355, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6428634634103176 val_avg_loss: tensor(3.1009)\n",
      "epoch: 63 train_loss: tensor(0.6933, grad_fn=<AddBackward0>) average train loss tensor(0.7526, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6686723888741409 val_avg_loss: tensor(3.0222)\n",
      "epoch: 64 train_loss: tensor(0.7611, grad_fn=<AddBackward0>) average train loss tensor(0.7323, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6510633277291095 val_avg_loss: tensor(3.0419)\n",
      "epoch: 65 train_loss: tensor(0.6946, grad_fn=<AddBackward0>) average train loss tensor(0.7182, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.673922661711353 val_avg_loss: tensor(2.9752)\n",
      "epoch: 66 train_loss: tensor(0.7044, grad_fn=<AddBackward0>) average train loss tensor(0.7212, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6857210276376722 val_avg_loss: tensor(2.8422)\n",
      "epoch: 67 train_loss: tensor(0.6977, grad_fn=<AddBackward0>) average train loss tensor(0.7255, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6707665988260626 val_avg_loss: tensor(2.8992)\n",
      "epoch: 68 train_loss: tensor(0.7295, grad_fn=<AddBackward0>) average train loss tensor(0.7055, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6680234787481933 val_avg_loss: tensor(3.0220)\n",
      "epoch: 69 train_loss: tensor(0.6629, grad_fn=<AddBackward0>) average train loss tensor(0.7174, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6877267498451465 val_avg_loss: tensor(2.8514)\n",
      "epoch: 70 train_loss: tensor(0.7631, grad_fn=<AddBackward0>) average train loss tensor(0.7082, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6679054950889302 val_avg_loss: tensor(2.9736)\n",
      "epoch: 71 train_loss: tensor(0.6779, grad_fn=<AddBackward0>) average train loss tensor(0.6961, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6794973896115388 val_avg_loss: tensor(2.9210)\n",
      "epoch: 72 train_loss: tensor(0.6740, grad_fn=<AddBackward0>) average train loss tensor(0.6874, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6498834911364776 val_avg_loss: tensor(3.0207)\n",
      "epoch: 73 train_loss: tensor(0.6821, grad_fn=<AddBackward0>) average train loss tensor(0.7106, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6523316520661888 val_avg_loss: tensor(2.9761)\n",
      "epoch: 74 train_loss: tensor(0.6682, grad_fn=<AddBackward0>) average train loss tensor(0.7241, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.664867415862903 val_avg_loss: tensor(2.9297)\n",
      "epoch: 75 train_loss: tensor(0.6604, grad_fn=<AddBackward0>) average train loss tensor(0.6987, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6749550187299059 val_avg_loss: tensor(2.9028)\n",
      "epoch: 76 train_loss: tensor(0.6543, grad_fn=<AddBackward0>) average train loss tensor(0.6995, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6777276347225909 val_avg_loss: tensor(2.8555)\n",
      "epoch: 77 train_loss: tensor(0.6896, grad_fn=<AddBackward0>) average train loss tensor(0.6860, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7050408518420199 val_avg_loss: tensor(2.7818)\n",
      "epoch: 78 train_loss: tensor(0.6620, grad_fn=<AddBackward0>) average train loss tensor(0.6881, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7056897619679674 val_avg_loss: tensor(2.8048)\n",
      "epoch: 79 train_loss: tensor(0.6440, grad_fn=<AddBackward0>) average train loss tensor(0.6847, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6737161903076424 val_avg_loss: tensor(2.9541)\n",
      "epoch: 80 train_loss: tensor(0.6489, grad_fn=<AddBackward0>) average train loss tensor(0.6816, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6955726631861487 val_avg_loss: tensor(2.7989)\n",
      "epoch: 81 train_loss: tensor(0.6874, grad_fn=<AddBackward0>) average train loss tensor(0.6990, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7083148983865735 val_avg_loss: tensor(2.7636)\n",
      "epoch: 82 train_loss: tensor(0.6738, grad_fn=<AddBackward0>) average train loss tensor(0.6762, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7006754564492818 val_avg_loss: tensor(2.8377)\n",
      "epoch: 83 train_loss: tensor(0.6745, grad_fn=<AddBackward0>) average train loss tensor(0.6891, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.664130017992508 val_avg_loss: tensor(2.9347)\n",
      "epoch: 84 train_loss: tensor(0.6531, grad_fn=<AddBackward0>) average train loss tensor(0.6902, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.685013125682093 val_avg_loss: tensor(2.8709)\n",
      "epoch: 85 train_loss: tensor(0.6973, grad_fn=<AddBackward0>) average train loss tensor(0.6938, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6774916674040645 val_avg_loss: tensor(2.8749)\n",
      "epoch: 86 train_loss: tensor(0.6100, grad_fn=<AddBackward0>) average train loss tensor(0.6618, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.720791670353656 val_avg_loss: tensor(2.6533)\n",
      "epoch: 87 train_loss: tensor(0.6858, grad_fn=<AddBackward0>) average train loss tensor(0.6854, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6943338347638852 val_avg_loss: tensor(2.8122)\n",
      "epoch: 88 train_loss: tensor(0.6596, grad_fn=<AddBackward0>) average train loss tensor(0.6647, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7406129251098723 val_avg_loss: tensor(2.6685)\n",
      "epoch: 89 train_loss: tensor(0.6647, grad_fn=<AddBackward0>) average train loss tensor(0.6771, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7138011385423119 val_avg_loss: tensor(2.7022)\n",
      "epoch: 90 train_loss: tensor(0.7093, grad_fn=<AddBackward0>) average train loss tensor(0.6628, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7140960976904699 val_avg_loss: tensor(2.6521)\n",
      "epoch: 91 train_loss: tensor(0.6714, grad_fn=<AddBackward0>) average train loss tensor(0.6684, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6771967082559066 val_avg_loss: tensor(2.8760)\n",
      "epoch: 92 train_loss: tensor(0.6279, grad_fn=<AddBackward0>) average train loss tensor(0.6691, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7151579506238386 val_avg_loss: tensor(2.7661)\n",
      "epoch: 93 train_loss: tensor(0.6193, grad_fn=<AddBackward0>) average train loss tensor(0.6647, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.7022682358493348 val_avg_loss: tensor(2.7702)\n",
      "epoch: 94 train_loss: tensor(0.6671, grad_fn=<AddBackward0>) average train loss tensor(0.6745, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.716986697342418 val_avg_loss: tensor(2.7569)\n",
      "epoch: 95 train_loss: tensor(0.6101, grad_fn=<AddBackward0>) average train loss tensor(0.6611, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7428546146358729 val_avg_loss: tensor(2.6305)\n",
      "epoch: 96 train_loss: tensor(0.6405, grad_fn=<AddBackward0>) average train loss tensor(0.6584, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6811786567560393 val_avg_loss: tensor(2.8386)\n",
      "epoch: 97 train_loss: tensor(0.6300, grad_fn=<AddBackward0>) average train loss tensor(0.6482, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7322950771318173 val_avg_loss: tensor(2.5888)\n",
      "epoch: 98 train_loss: tensor(0.6182, grad_fn=<AddBackward0>) average train loss tensor(0.6508, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.7079609474087839 val_avg_loss: tensor(2.6607)\n",
      "epoch: 99 train_loss: tensor(0.6632, grad_fn=<AddBackward0>) average train loss tensor(0.6656, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6922101288971477 val_avg_loss: tensor(2.7877)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.69      0.81     41251\n",
      "           1       0.04      0.52      0.08      1128\n",
      "\n",
      "    accuracy                           0.69     42379\n",
      "   macro avg       0.51      0.61      0.45     42379\n",
      "weighted avg       0.96      0.69      0.79     42379\n",
      "\n",
      "roc auc score: 0.6548965619316169\n"
     ]
    }
   ],
   "source": [
    "model = WtsTridentModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/trident_bn_d128_drop05_wd0005_weight200')\n",
    "\n",
    "fit_wts_trident_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 200]).float()\n",
    ")\n",
    "\n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())[0]\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(1.6898, grad_fn=<AddBackward0>) average train loss tensor(1.7415, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23531840840043655 val_avg_loss: tensor(3.4458)\n",
      "epoch: 1 train_loss: tensor(1.0159, grad_fn=<AddBackward0>) average train loss tensor(1.2337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15500103235701856 val_avg_loss: tensor(4.3456)\n",
      "epoch: 2 train_loss: tensor(1.1144, grad_fn=<AddBackward0>) average train loss tensor(1.0647, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13759844261569773 val_avg_loss: tensor(4.9705)\n",
      "epoch: 3 train_loss: tensor(1.0784, grad_fn=<AddBackward0>) average train loss tensor(0.9980, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.14588679467893698 val_avg_loss: tensor(5.3533)\n",
      "epoch: 4 train_loss: tensor(1.0693, grad_fn=<AddBackward0>) average train loss tensor(0.9671, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.15155001032357018 val_avg_loss: tensor(5.3229)\n",
      "epoch: 5 train_loss: tensor(0.9872, grad_fn=<AddBackward0>) average train loss tensor(0.9408, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1822847535616317 val_avg_loss: tensor(5.3960)\n",
      "epoch: 6 train_loss: tensor(1.0256, grad_fn=<AddBackward0>) average train loss tensor(0.9305, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19313925021384537 val_avg_loss: tensor(5.2529)\n",
      "epoch: 7 train_loss: tensor(0.9667, grad_fn=<AddBackward0>) average train loss tensor(0.9175, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20664837919948087 val_avg_loss: tensor(5.2419)\n",
      "epoch: 8 train_loss: tensor(0.9914, grad_fn=<AddBackward0>) average train loss tensor(0.9099, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.219626581718432 val_avg_loss: tensor(5.1799)\n",
      "epoch: 9 train_loss: tensor(0.9828, grad_fn=<AddBackward0>) average train loss tensor(0.8873, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2537828510751261 val_avg_loss: tensor(5.2330)\n",
      "epoch: 10 train_loss: tensor(0.9647, grad_fn=<AddBackward0>) average train loss tensor(0.8746, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.26820635342005134 val_avg_loss: tensor(5.1334)\n",
      "epoch: 11 train_loss: tensor(0.9335, grad_fn=<AddBackward0>) average train loss tensor(0.8693, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27965076836858094 val_avg_loss: tensor(5.1618)\n",
      "epoch: 12 train_loss: tensor(0.9110, grad_fn=<AddBackward0>) average train loss tensor(0.8496, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3157537681031177 val_avg_loss: tensor(5.0216)\n",
      "epoch: 13 train_loss: tensor(0.8916, grad_fn=<AddBackward0>) average train loss tensor(0.8445, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3180249535439342 val_avg_loss: tensor(5.0628)\n",
      "epoch: 14 train_loss: tensor(0.9154, grad_fn=<AddBackward0>) average train loss tensor(0.8368, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3324189599740436 val_avg_loss: tensor(5.0161)\n",
      "epoch: 15 train_loss: tensor(0.9585, grad_fn=<AddBackward0>) average train loss tensor(0.8253, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.350470459841312 val_avg_loss: tensor(4.9626)\n",
      "epoch: 16 train_loss: tensor(0.9027, grad_fn=<AddBackward0>) average train loss tensor(0.8200, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3544524083414447 val_avg_loss: tensor(4.9488)\n",
      "epoch: 17 train_loss: tensor(0.8461, grad_fn=<AddBackward0>) average train loss tensor(0.8022, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3741556794383978 val_avg_loss: tensor(4.9412)\n",
      "epoch: 18 train_loss: tensor(0.8745, grad_fn=<AddBackward0>) average train loss tensor(0.7974, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39256113028345574 val_avg_loss: tensor(4.8574)\n",
      "epoch: 19 train_loss: tensor(0.8426, grad_fn=<AddBackward0>) average train loss tensor(0.7992, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.379818895083031 val_avg_loss: tensor(4.9120)\n",
      "epoch: 20 train_loss: tensor(0.8455, grad_fn=<AddBackward0>) average train loss tensor(0.7923, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39096835088340265 val_avg_loss: tensor(4.8041)\n",
      "epoch: 21 train_loss: tensor(0.8151, grad_fn=<AddBackward0>) average train loss tensor(0.7718, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40907884258030264 val_avg_loss: tensor(4.8396)\n",
      "epoch: 22 train_loss: tensor(0.9246, grad_fn=<AddBackward0>) average train loss tensor(0.7849, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3981063622688258 val_avg_loss: tensor(4.8283)\n",
      "epoch: 23 train_loss: tensor(0.8132, grad_fn=<AddBackward0>) average train loss tensor(0.7673, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43105329911807216 val_avg_loss: tensor(4.6643)\n",
      "epoch: 24 train_loss: tensor(0.8160, grad_fn=<AddBackward0>) average train loss tensor(0.7618, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43615609238120523 val_avg_loss: tensor(4.7138)\n",
      "epoch: 25 train_loss: tensor(0.8423, grad_fn=<AddBackward0>) average train loss tensor(0.7613, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4394301389257588 val_avg_loss: tensor(4.6377)\n",
      "epoch: 26 train_loss: tensor(0.7967, grad_fn=<AddBackward0>) average train loss tensor(0.7394, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4657404949414506 val_avg_loss: tensor(4.5715)\n",
      "epoch: 27 train_loss: tensor(0.8252, grad_fn=<AddBackward0>) average train loss tensor(0.7403, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4519953986372887 val_avg_loss: tensor(4.6388)\n",
      "epoch: 28 train_loss: tensor(0.8073, grad_fn=<AddBackward0>) average train loss tensor(0.7561, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45606583488186886 val_avg_loss: tensor(4.5232)\n",
      "epoch: 29 train_loss: tensor(0.7786, grad_fn=<AddBackward0>) average train loss tensor(0.7396, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44037400819986433 val_avg_loss: tensor(4.5610)\n",
      "epoch: 30 train_loss: tensor(0.7815, grad_fn=<AddBackward0>) average train loss tensor(0.7382, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46984042710084656 val_avg_loss: tensor(4.5128)\n",
      "epoch: 31 train_loss: tensor(0.7763, grad_fn=<AddBackward0>) average train loss tensor(0.7203, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.49901188685367076 val_avg_loss: tensor(4.3947)\n",
      "epoch: 32 train_loss: tensor(0.7651, grad_fn=<AddBackward0>) average train loss tensor(0.7186, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48703654543845676 val_avg_loss: tensor(4.3957)\n",
      "epoch: 33 train_loss: tensor(0.8109, grad_fn=<AddBackward0>) average train loss tensor(0.7293, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45627230628557947 val_avg_loss: tensor(4.5646)\n",
      "epoch: 34 train_loss: tensor(0.7831, grad_fn=<AddBackward0>) average train loss tensor(0.7238, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4852077987198773 val_avg_loss: tensor(4.3579)\n",
      "epoch: 35 train_loss: tensor(0.7725, grad_fn=<AddBackward0>) average train loss tensor(0.7175, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4877149514792201 val_avg_loss: tensor(4.3565)\n",
      "epoch: 36 train_loss: tensor(0.7338, grad_fn=<AddBackward0>) average train loss tensor(0.7025, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5013420641241189 val_avg_loss: tensor(4.2571)\n",
      "epoch: 37 train_loss: tensor(0.7458, grad_fn=<AddBackward0>) average train loss tensor(0.7060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5356163171400761 val_avg_loss: tensor(4.1529)\n",
      "epoch: 38 train_loss: tensor(0.7362, grad_fn=<AddBackward0>) average train loss tensor(0.7110, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5045571188390408 val_avg_loss: tensor(4.2903)\n",
      "epoch: 39 train_loss: tensor(0.7467, grad_fn=<AddBackward0>) average train loss tensor(0.6962, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5221661799840722 val_avg_loss: tensor(4.2200)\n",
      "epoch: 40 train_loss: tensor(0.7473, grad_fn=<AddBackward0>) average train loss tensor(0.6928, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.521753237176651 val_avg_loss: tensor(4.1890)\n",
      "epoch: 41 train_loss: tensor(0.7528, grad_fn=<AddBackward0>) average train loss tensor(0.7003, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5278293956287055 val_avg_loss: tensor(4.1760)\n",
      "epoch: 42 train_loss: tensor(0.7109, grad_fn=<AddBackward0>) average train loss tensor(0.6830, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5408075981476566 val_avg_loss: tensor(4.1202)\n",
      "epoch: 43 train_loss: tensor(0.7018, grad_fn=<AddBackward0>) average train loss tensor(0.6837, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5243488776804412 val_avg_loss: tensor(4.1034)\n",
      "epoch: 44 train_loss: tensor(0.7414, grad_fn=<AddBackward0>) average train loss tensor(0.6891, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.538123469899419 val_avg_loss: tensor(4.1175)\n",
      "epoch: 45 train_loss: tensor(0.6982, grad_fn=<AddBackward0>) average train loss tensor(0.6746, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5445240834144471 val_avg_loss: tensor(4.0595)\n",
      "epoch: 46 train_loss: tensor(0.7205, grad_fn=<AddBackward0>) average train loss tensor(0.6663, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.5571483349556087 val_avg_loss: tensor(3.9324)\n",
      "epoch: 47 train_loss: tensor(0.6967, grad_fn=<AddBackward0>) average train loss tensor(0.6806, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5306610034510221 val_avg_loss: tensor(4.0804)\n",
      "epoch: 48 train_loss: tensor(0.6772, grad_fn=<AddBackward0>) average train loss tensor(0.6774, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5728401616376132 val_avg_loss: tensor(3.9678)\n",
      "epoch: 49 train_loss: tensor(0.7497, grad_fn=<AddBackward0>) average train loss tensor(0.6727, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5106332772910952 val_avg_loss: tensor(4.2124)\n",
      "epoch: 50 train_loss: tensor(0.6878, grad_fn=<AddBackward0>) average train loss tensor(0.6665, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5612482671150045 val_avg_loss: tensor(3.8997)\n",
      "epoch: 51 train_loss: tensor(0.6772, grad_fn=<AddBackward0>) average train loss tensor(0.6673, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5364422027549185 val_avg_loss: tensor(3.9579)\n",
      "epoch: 52 train_loss: tensor(0.7034, grad_fn=<AddBackward0>) average train loss tensor(0.6793, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.536648674158629 val_avg_loss: tensor(3.9378)\n",
      "epoch: 53 train_loss: tensor(0.7062, grad_fn=<AddBackward0>) average train loss tensor(0.6589, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5536678170073445 val_avg_loss: tensor(3.9258)\n",
      "epoch: 54 train_loss: tensor(0.6641, grad_fn=<AddBackward0>) average train loss tensor(0.6352, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6003598501607528 val_avg_loss: tensor(3.7495)\n",
      "epoch: 55 train_loss: tensor(0.6805, grad_fn=<AddBackward0>) average train loss tensor(0.6449, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5605698610742412 val_avg_loss: tensor(3.9612)\n",
      "epoch: 56 train_loss: tensor(0.6971, grad_fn=<AddBackward0>) average train loss tensor(0.6591, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5724567147450078 val_avg_loss: tensor(3.9374)\n",
      "epoch: 57 train_loss: tensor(0.6839, grad_fn=<AddBackward0>) average train loss tensor(0.6478, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5809515382119577 val_avg_loss: tensor(3.8055)\n",
      "epoch: 58 train_loss: tensor(0.6731, grad_fn=<AddBackward0>) average train loss tensor(0.6421, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6037518803645695 val_avg_loss: tensor(3.7203)\n",
      "epoch: 59 train_loss: tensor(0.6690, grad_fn=<AddBackward0>) average train loss tensor(0.6427, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5668229950151904 val_avg_loss: tensor(3.8127)\n",
      "epoch: 60 train_loss: tensor(0.6585, grad_fn=<AddBackward0>) average train loss tensor(0.6415, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5502757868035277 val_avg_loss: tensor(4.0819)\n",
      "epoch: 61 train_loss: tensor(0.6939, grad_fn=<AddBackward0>) average train loss tensor(0.6604, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5549951331740554 val_avg_loss: tensor(3.9378)\n",
      "epoch: 62 train_loss: tensor(0.6835, grad_fn=<AddBackward0>) average train loss tensor(0.6282, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6288823997876294 val_avg_loss: tensor(3.6237)\n",
      "epoch: 63 train_loss: tensor(0.6714, grad_fn=<AddBackward0>) average train loss tensor(0.6365, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5794472465563519 val_avg_loss: tensor(3.8072)\n",
      "epoch: 64 train_loss: tensor(0.6561, grad_fn=<AddBackward0>) average train loss tensor(0.6293, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5939887325605404 val_avg_loss: tensor(3.7422)\n",
      "epoch: 65 train_loss: tensor(0.6397, grad_fn=<AddBackward0>) average train loss tensor(0.6356, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.585139958115801 val_avg_loss: tensor(3.7366)\n",
      "epoch: 66 train_loss: tensor(0.6513, grad_fn=<AddBackward0>) average train loss tensor(0.6247, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6025425478571218 val_avg_loss: tensor(3.6672)\n",
      "epoch: 67 train_loss: tensor(0.7157, grad_fn=<AddBackward0>) average train loss tensor(0.6382, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5829277645046161 val_avg_loss: tensor(3.7651)\n",
      "epoch: 68 train_loss: tensor(0.6289, grad_fn=<AddBackward0>) average train loss tensor(0.6335, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5585641388667669 val_avg_loss: tensor(3.8049)\n",
      "epoch: 69 train_loss: tensor(0.6068, grad_fn=<AddBackward0>) average train loss tensor(0.6149, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6245170043948913 val_avg_loss: tensor(3.6333)\n",
      "epoch: 70 train_loss: tensor(0.6998, grad_fn=<AddBackward0>) average train loss tensor(0.6309, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5369141373919712 val_avg_loss: tensor(3.9733)\n",
      "epoch: 71 train_loss: tensor(0.6625, grad_fn=<AddBackward0>) average train loss tensor(0.6376, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5643748340854792 val_avg_loss: tensor(3.7675)\n",
      "epoch: 72 train_loss: tensor(0.6327, grad_fn=<AddBackward0>) average train loss tensor(0.6216, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.605138188360912 val_avg_loss: tensor(3.7026)\n",
      "epoch: 73 train_loss: tensor(0.6718, grad_fn=<AddBackward0>) average train loss tensor(0.6271, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5978526974014099 val_avg_loss: tensor(3.6554)\n",
      "epoch: 74 train_loss: tensor(0.6126, grad_fn=<AddBackward0>) average train loss tensor(0.6095, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6423030410288175 val_avg_loss: tensor(3.4977)\n",
      "epoch: 75 train_loss: tensor(0.6352, grad_fn=<AddBackward0>) average train loss tensor(0.6051, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6084417308202814 val_avg_loss: tensor(3.6029)\n",
      "epoch: 76 train_loss: tensor(0.6229, grad_fn=<AddBackward0>) average train loss tensor(0.6105, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6151373034834675 val_avg_loss: tensor(3.5619)\n",
      "epoch: 77 train_loss: tensor(0.6181, grad_fn=<AddBackward0>) average train loss tensor(0.6027, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6303571955284193 val_avg_loss: tensor(3.4686)\n",
      "epoch: 78 train_loss: tensor(0.6111, grad_fn=<AddBackward0>) average train loss tensor(0.6084, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.596820340382857 val_avg_loss: tensor(3.6403)\n",
      "epoch: 79 train_loss: tensor(0.6287, grad_fn=<AddBackward0>) average train loss tensor(0.6032, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6139574668908356 val_avg_loss: tensor(3.6166)\n",
      "epoch: 80 train_loss: tensor(0.6125, grad_fn=<AddBackward0>) average train loss tensor(0.6168, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5816889360823526 val_avg_loss: tensor(3.6975)\n",
      "epoch: 81 train_loss: tensor(0.6355, grad_fn=<AddBackward0>) average train loss tensor(0.6027, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6122467038315194 val_avg_loss: tensor(3.5756)\n",
      "epoch: 82 train_loss: tensor(0.6022, grad_fn=<AddBackward0>) average train loss tensor(0.5984, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6363743621508421 val_avg_loss: tensor(3.4813)\n",
      "epoch: 83 train_loss: tensor(0.6319, grad_fn=<AddBackward0>) average train loss tensor(0.5986, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6196501784502846 val_avg_loss: tensor(3.5261)\n",
      "epoch: 84 train_loss: tensor(0.5984, grad_fn=<AddBackward0>) average train loss tensor(0.5906, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6116272896203876 val_avg_loss: tensor(3.6012)\n",
      "epoch: 85 train_loss: tensor(0.6051, grad_fn=<AddBackward0>) average train loss tensor(0.5888, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6126006548093089 val_avg_loss: tensor(3.5765)\n",
      "epoch: 86 train_loss: tensor(0.6397, grad_fn=<AddBackward0>) average train loss tensor(0.6056, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6088841695425183 val_avg_loss: tensor(3.5732)\n",
      "epoch: 87 train_loss: tensor(0.6123, grad_fn=<AddBackward0>) average train loss tensor(0.6081, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6243105329911807 val_avg_loss: tensor(3.5582)\n",
      "epoch: 88 train_loss: tensor(0.5959, grad_fn=<AddBackward0>) average train loss tensor(0.5895, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6581128513700852 val_avg_loss: tensor(3.3642)\n",
      "epoch: 89 train_loss: tensor(0.6332, grad_fn=<AddBackward0>) average train loss tensor(0.5904, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6284399610653925 val_avg_loss: tensor(3.5199)\n",
      "epoch: 90 train_loss: tensor(0.5904, grad_fn=<AddBackward0>) average train loss tensor(0.5829, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6570804943515323 val_avg_loss: tensor(3.4018)\n",
      "epoch: 91 train_loss: tensor(0.6227, grad_fn=<AddBackward0>) average train loss tensor(0.5990, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.605138188360912 val_avg_loss: tensor(3.6315)\n",
      "epoch: 92 train_loss: tensor(0.5935, grad_fn=<AddBackward0>) average train loss tensor(0.6042, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6373182314249476 val_avg_loss: tensor(3.4863)\n",
      "epoch: 93 train_loss: tensor(0.6249, grad_fn=<AddBackward0>) average train loss tensor(0.5973, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.5851989499454325 val_avg_loss: tensor(3.6917)\n",
      "epoch: 94 train_loss: tensor(0.5775, grad_fn=<AddBackward0>) average train loss tensor(0.5896, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6729198006076158 val_avg_loss: tensor(3.2720)\n",
      "epoch: 95 train_loss: tensor(0.6035, grad_fn=<AddBackward0>) average train loss tensor(0.5711, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.649971978880925 val_avg_loss: tensor(3.4860)\n",
      "epoch: 96 train_loss: tensor(0.6269, grad_fn=<AddBackward0>) average train loss tensor(0.5742, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6455770875733711 val_avg_loss: tensor(3.4098)\n",
      "epoch: 97 train_loss: tensor(0.5823, grad_fn=<AddBackward0>) average train loss tensor(0.5892, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6436598531103442 val_avg_loss: tensor(3.4043)\n",
      "epoch: 98 train_loss: tensor(0.5887, grad_fn=<AddBackward0>) average train loss tensor(0.5942, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6188537887502581 val_avg_loss: tensor(3.5111)\n",
      "epoch: 99 train_loss: tensor(0.6054, grad_fn=<AddBackward0>) average train loss tensor(0.5779, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6361678907471315 val_avg_loss: tensor(3.4611)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-e1165a068f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_img_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_txt_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwts_test_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_img_cur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_txt_cur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "model = WtsTridentModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/trident_bn_d128_drop05_wd0005_weight300')\n",
    "\n",
    "fit_wts_trident_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 300]).float()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.64      0.78     41251\n",
      "           1       0.04      0.60      0.08      1128\n",
      "\n",
      "    accuracy                           0.64     42379\n",
      "   macro avg       0.51      0.62      0.43     42379\n",
      "weighted avg       0.96      0.64      0.76     42379\n",
      "\n",
      "roc auc score: 0.6685758348690795\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())[0]\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(1.2018, grad_fn=<AddBackward0>) average train loss tensor(1.6803, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20968645842550807 val_avg_loss: tensor(3.7209)\n",
      "epoch: 1 train_loss: tensor(1.0303, grad_fn=<AddBackward0>) average train loss tensor(1.1339, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1824912249653423 val_avg_loss: tensor(4.7416)\n",
      "epoch: 2 train_loss: tensor(0.9356, grad_fn=<AddBackward0>) average train loss tensor(0.9283, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12290947703743031 val_avg_loss: tensor(5.6319)\n",
      "epoch: 3 train_loss: tensor(0.9425, grad_fn=<AddBackward0>) average train loss tensor(0.8664, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.11686281450019172 val_avg_loss: tensor(6.0184)\n",
      "epoch: 4 train_loss: tensor(0.9064, grad_fn=<AddBackward0>) average train loss tensor(0.8272, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.13535675308969708 val_avg_loss: tensor(6.0717)\n",
      "epoch: 5 train_loss: tensor(0.9601, grad_fn=<AddBackward0>) average train loss tensor(0.8115, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.151432026664307 val_avg_loss: tensor(5.9805)\n",
      "epoch: 6 train_loss: tensor(0.9292, grad_fn=<AddBackward0>) average train loss tensor(0.7946, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.17196118337610242 val_avg_loss: tensor(5.9582)\n",
      "epoch: 7 train_loss: tensor(0.8193, grad_fn=<AddBackward0>) average train loss tensor(0.7758, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1917234463026871 val_avg_loss: tensor(5.9366)\n",
      "epoch: 8 train_loss: tensor(0.8474, grad_fn=<AddBackward0>) average train loss tensor(0.7663, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.20723829749579684 val_avg_loss: tensor(5.9771)\n",
      "epoch: 9 train_loss: tensor(0.8155, grad_fn=<AddBackward0>) average train loss tensor(0.7572, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.21706043712945758 val_avg_loss: tensor(5.7898)\n",
      "epoch: 10 train_loss: tensor(0.8628, grad_fn=<AddBackward0>) average train loss tensor(0.7454, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24295785033772824 val_avg_loss: tensor(5.8077)\n",
      "epoch: 11 train_loss: tensor(0.8118, grad_fn=<AddBackward0>) average train loss tensor(0.7405, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25283898180102055 val_avg_loss: tensor(5.8388)\n",
      "epoch: 12 train_loss: tensor(0.7886, grad_fn=<AddBackward0>) average train loss tensor(0.7338, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24711677432675574 val_avg_loss: tensor(5.7669)\n",
      "epoch: 13 train_loss: tensor(0.8006, grad_fn=<AddBackward0>) average train loss tensor(0.7337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2628086010087603 val_avg_loss: tensor(5.7944)\n",
      "epoch: 14 train_loss: tensor(0.7795, grad_fn=<AddBackward0>) average train loss tensor(0.7163, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3057251570657464 val_avg_loss: tensor(5.6255)\n",
      "epoch: 15 train_loss: tensor(0.7576, grad_fn=<AddBackward0>) average train loss tensor(0.7071, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3085272689732472 val_avg_loss: tensor(5.5785)\n",
      "epoch: 16 train_loss: tensor(0.7825, grad_fn=<AddBackward0>) average train loss tensor(0.6984, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3100905524584845 val_avg_loss: tensor(5.7196)\n",
      "epoch: 17 train_loss: tensor(0.7761, grad_fn=<AddBackward0>) average train loss tensor(0.6950, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.31628469456980207 val_avg_loss: tensor(5.6416)\n",
      "epoch: 18 train_loss: tensor(0.7539, grad_fn=<AddBackward0>) average train loss tensor(0.6874, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3200306757514084 val_avg_loss: tensor(5.6024)\n",
      "epoch: 19 train_loss: tensor(0.7423, grad_fn=<AddBackward0>) average train loss tensor(0.6831, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34934961507831164 val_avg_loss: tensor(5.5262)\n",
      "epoch: 20 train_loss: tensor(0.7644, grad_fn=<AddBackward0>) average train loss tensor(0.6820, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33005928678877977 val_avg_loss: tensor(5.6242)\n",
      "epoch: 21 train_loss: tensor(0.7407, grad_fn=<AddBackward0>) average train loss tensor(0.6729, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34209362003362537 val_avg_loss: tensor(5.5546)\n",
      "epoch: 22 train_loss: tensor(0.7315, grad_fn=<AddBackward0>) average train loss tensor(0.6630, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35999764032681475 val_avg_loss: tensor(5.4890)\n",
      "epoch: 23 train_loss: tensor(0.7401, grad_fn=<AddBackward0>) average train loss tensor(0.6641, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35837536501194583 val_avg_loss: tensor(5.4890)\n",
      "epoch: 24 train_loss: tensor(0.7597, grad_fn=<AddBackward0>) average train loss tensor(0.6607, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38220806418311065 val_avg_loss: tensor(5.4238)\n",
      "epoch: 25 train_loss: tensor(0.7640, grad_fn=<AddBackward0>) average train loss tensor(0.6592, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3879302716573755 val_avg_loss: tensor(5.3695)\n",
      "epoch: 26 train_loss: tensor(0.6844, grad_fn=<AddBackward0>) average train loss tensor(0.6534, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3771642627496092 val_avg_loss: tensor(5.3670)\n",
      "epoch: 27 train_loss: tensor(0.7111, grad_fn=<AddBackward0>) average train loss tensor(0.6563, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3774297259829514 val_avg_loss: tensor(5.3727)\n",
      "epoch: 28 train_loss: tensor(0.6991, grad_fn=<AddBackward0>) average train loss tensor(0.6467, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3691708698345279 val_avg_loss: tensor(5.3785)\n",
      "epoch: 29 train_loss: tensor(0.6655, grad_fn=<AddBackward0>) average train loss tensor(0.6320, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.41332625431377756 val_avg_loss: tensor(5.2346)\n",
      "epoch: 30 train_loss: tensor(0.7002, grad_fn=<AddBackward0>) average train loss tensor(0.6440, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3879302716573755 val_avg_loss: tensor(5.3231)\n",
      "epoch: 31 train_loss: tensor(0.6925, grad_fn=<AddBackward0>) average train loss tensor(0.6250, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42400377547709645 val_avg_loss: tensor(5.1731)\n",
      "epoch: 32 train_loss: tensor(0.6806, grad_fn=<AddBackward0>) average train loss tensor(0.6364, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4140636521841725 val_avg_loss: tensor(5.1513)\n",
      "epoch: 33 train_loss: tensor(0.6888, grad_fn=<AddBackward0>) average train loss tensor(0.6279, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.40736807952098636 val_avg_loss: tensor(5.2691)\n",
      "epoch: 34 train_loss: tensor(0.7187, grad_fn=<AddBackward0>) average train loss tensor(0.6347, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3959236645724567 val_avg_loss: tensor(5.3291)\n",
      "epoch: 35 train_loss: tensor(0.6774, grad_fn=<AddBackward0>) average train loss tensor(0.6322, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42497714066601777 val_avg_loss: tensor(5.0313)\n",
      "epoch: 36 train_loss: tensor(0.6979, grad_fn=<AddBackward0>) average train loss tensor(0.6217, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.42598000176975487 val_avg_loss: tensor(5.1365)\n",
      "epoch: 37 train_loss: tensor(0.6643, grad_fn=<AddBackward0>) average train loss tensor(0.6177, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44939975813349853 val_avg_loss: tensor(5.0546)\n",
      "epoch: 38 train_loss: tensor(0.6902, grad_fn=<AddBackward0>) average train loss tensor(0.6090, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.44922278264460375 val_avg_loss: tensor(5.0934)\n",
      "epoch: 39 train_loss: tensor(0.6512, grad_fn=<AddBackward0>) average train loss tensor(0.6134, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45125800076689376 val_avg_loss: tensor(5.0605)\n",
      "epoch: 40 train_loss: tensor(0.6958, grad_fn=<AddBackward0>) average train loss tensor(0.6069, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4360971005515736 val_avg_loss: tensor(5.1103)\n",
      "epoch: 41 train_loss: tensor(0.6648, grad_fn=<AddBackward0>) average train loss tensor(0.6152, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4360971005515736 val_avg_loss: tensor(5.0445)\n",
      "epoch: 42 train_loss: tensor(0.6662, grad_fn=<AddBackward0>) average train loss tensor(0.6117, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45311624340028905 val_avg_loss: tensor(5.0017)\n",
      "epoch: 43 train_loss: tensor(0.6435, grad_fn=<AddBackward0>) average train loss tensor(0.6029, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45338170663363125 val_avg_loss: tensor(4.9440)\n",
      "epoch: 44 train_loss: tensor(0.6766, grad_fn=<AddBackward0>) average train loss tensor(0.6133, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.412234905465593 val_avg_loss: tensor(5.2123)\n",
      "epoch: 45 train_loss: tensor(0.6423, grad_fn=<AddBackward0>) average train loss tensor(0.6060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4484263929445772 val_avg_loss: tensor(4.9778)\n",
      "epoch: 46 train_loss: tensor(0.6485, grad_fn=<AddBackward0>) average train loss tensor(0.5979, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.4629383830339498 val_avg_loss: tensor(4.9602)\n",
      "epoch: 47 train_loss: tensor(0.6525, grad_fn=<AddBackward0>) average train loss tensor(0.5966, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.43827979824794266 val_avg_loss: tensor(5.0094)\n",
      "epoch: 48 train_loss: tensor(0.6674, grad_fn=<AddBackward0>) average train loss tensor(0.5943, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4433530955962599 val_avg_loss: tensor(4.9712)\n",
      "epoch: 49 train_loss: tensor(0.6685, grad_fn=<AddBackward0>) average train loss tensor(0.5995, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4448278913370498 val_avg_loss: tensor(4.9863)\n",
      "epoch: 50 train_loss: tensor(0.6253, grad_fn=<AddBackward0>) average train loss tensor(0.6000, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4640887237117659 val_avg_loss: tensor(4.8778)\n",
      "epoch: 51 train_loss: tensor(0.6273, grad_fn=<AddBackward0>) average train loss tensor(0.5844, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4732914491342949 val_avg_loss: tensor(4.8611)\n",
      "epoch: 52 train_loss: tensor(0.6782, grad_fn=<AddBackward0>) average train loss tensor(0.5990, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47078429637495206 val_avg_loss: tensor(4.8206)\n",
      "epoch: 53 train_loss: tensor(0.6309, grad_fn=<AddBackward0>) average train loss tensor(0.5828, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48326106834203464 val_avg_loss: tensor(4.8105)\n",
      "epoch: 54 train_loss: tensor(0.6153, grad_fn=<AddBackward0>) average train loss tensor(0.5861, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4557118839040793 val_avg_loss: tensor(4.9236)\n",
      "epoch: 55 train_loss: tensor(0.6406, grad_fn=<AddBackward0>) average train loss tensor(0.5769, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.486122172079167 val_avg_loss: tensor(4.8217)\n",
      "epoch: 56 train_loss: tensor(0.6525, grad_fn=<AddBackward0>) average train loss tensor(0.5892, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.45535793292628973 val_avg_loss: tensor(4.9009)\n",
      "epoch: 57 train_loss: tensor(0.6192, grad_fn=<AddBackward0>) average train loss tensor(0.5822, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4853552782939563 val_avg_loss: tensor(4.7541)\n",
      "epoch: 58 train_loss: tensor(0.6179, grad_fn=<AddBackward0>) average train loss tensor(0.5740, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.48615166799398285 val_avg_loss: tensor(4.7852)\n",
      "epoch: 59 train_loss: tensor(0.6127, grad_fn=<AddBackward0>) average train loss tensor(0.5794, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4734389287083739 val_avg_loss: tensor(4.8778)\n",
      "epoch: 60 train_loss: tensor(0.6049, grad_fn=<AddBackward0>) average train loss tensor(0.5738, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5184496947172816 val_avg_loss: tensor(4.5823)\n",
      "epoch: 61 train_loss: tensor(0.6221, grad_fn=<AddBackward0>) average train loss tensor(0.5660, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5182137273987553 val_avg_loss: tensor(4.6080)\n",
      "epoch: 62 train_loss: tensor(0.6013, grad_fn=<AddBackward0>) average train loss tensor(0.5690, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.47809928324927 val_avg_loss: tensor(4.7672)\n",
      "epoch: 63 train_loss: tensor(0.6186, grad_fn=<AddBackward0>) average train loss tensor(0.5754, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4985989440462496 val_avg_loss: tensor(4.6699)\n",
      "epoch: 64 train_loss: tensor(0.6266, grad_fn=<AddBackward0>) average train loss tensor(0.5675, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5014305518685662 val_avg_loss: tensor(4.6730)\n",
      "epoch: 65 train_loss: tensor(0.6189, grad_fn=<AddBackward0>) average train loss tensor(0.5734, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4847653599976403 val_avg_loss: tensor(4.6727)\n",
      "epoch: 66 train_loss: tensor(0.5814, grad_fn=<AddBackward0>) average train loss tensor(0.5738, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5062383859835413 val_avg_loss: tensor(4.5966)\n",
      "epoch: 67 train_loss: tensor(0.6026, grad_fn=<AddBackward0>) average train loss tensor(0.5514, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5191870925876766 val_avg_loss: tensor(4.5831)\n",
      "epoch: 68 train_loss: tensor(0.6968, grad_fn=<AddBackward0>) average train loss tensor(0.5603, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.49907087868330235 val_avg_loss: tensor(4.6748)\n",
      "epoch: 69 train_loss: tensor(0.6119, grad_fn=<AddBackward0>) average train loss tensor(0.5594, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5022859333982244 val_avg_loss: tensor(4.6591)\n",
      "epoch: 70 train_loss: tensor(0.6117, grad_fn=<AddBackward0>) average train loss tensor(0.5612, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.46650738872666137 val_avg_loss: tensor(4.8995)\n",
      "epoch: 71 train_loss: tensor(0.6080, grad_fn=<AddBackward0>) average train loss tensor(0.5658, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5241129103619149 val_avg_loss: tensor(4.4575)\n",
      "epoch: 72 train_loss: tensor(0.6216, grad_fn=<AddBackward0>) average train loss tensor(0.5655, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5080671327021208 val_avg_loss: tensor(4.6724)\n",
      "epoch: 73 train_loss: tensor(0.6257, grad_fn=<AddBackward0>) average train loss tensor(0.5629, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.49184437955343185 val_avg_loss: tensor(4.7338)\n",
      "epoch: 74 train_loss: tensor(0.6147, grad_fn=<AddBackward0>) average train loss tensor(0.5608, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5123440403504115 val_avg_loss: tensor(4.5415)\n",
      "epoch: 75 train_loss: tensor(0.5831, grad_fn=<AddBackward0>) average train loss tensor(0.5495, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5420464265699201 val_avg_loss: tensor(4.3570)\n",
      "epoch: 76 train_loss: tensor(0.6121, grad_fn=<AddBackward0>) average train loss tensor(0.5735, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4899566410052208 val_avg_loss: tensor(4.6093)\n",
      "epoch: 77 train_loss: tensor(0.5843, grad_fn=<AddBackward0>) average train loss tensor(0.5475, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5461758546441318 val_avg_loss: tensor(4.3358)\n",
      "epoch: 78 train_loss: tensor(0.5907, grad_fn=<AddBackward0>) average train loss tensor(0.5412, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5508362091850278 val_avg_loss: tensor(4.3895)\n",
      "epoch: 79 train_loss: tensor(0.6292, grad_fn=<AddBackward0>) average train loss tensor(0.5446, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4899566410052208 val_avg_loss: tensor(4.7492)\n",
      "epoch: 80 train_loss: tensor(0.5765, grad_fn=<AddBackward0>) average train loss tensor(0.5578, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4903990797274577 val_avg_loss: tensor(4.6774)\n",
      "epoch: 81 train_loss: tensor(0.6117, grad_fn=<AddBackward0>) average train loss tensor(0.5550, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5315458808954959 val_avg_loss: tensor(4.4002)\n",
      "epoch: 82 train_loss: tensor(0.5803, grad_fn=<AddBackward0>) average train loss tensor(0.5348, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5548476535999765 val_avg_loss: tensor(4.3278)\n",
      "epoch: 83 train_loss: tensor(0.5919, grad_fn=<AddBackward0>) average train loss tensor(0.5362, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5029643394389877 val_avg_loss: tensor(4.6587)\n",
      "epoch: 84 train_loss: tensor(0.5920, grad_fn=<AddBackward0>) average train loss tensor(0.5646, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5036722413945669 val_avg_loss: tensor(4.6559)\n",
      "epoch: 85 train_loss: tensor(0.5847, grad_fn=<AddBackward0>) average train loss tensor(0.5501, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5022859333982244 val_avg_loss: tensor(4.5336)\n",
      "epoch: 86 train_loss: tensor(0.6040, grad_fn=<AddBackward0>) average train loss tensor(0.5478, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.525853169336047 val_avg_loss: tensor(4.5124)\n",
      "epoch: 87 train_loss: tensor(0.6220, grad_fn=<AddBackward0>) average train loss tensor(0.5511, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.523670471639678 val_avg_loss: tensor(4.4772)\n",
      "epoch: 88 train_loss: tensor(0.6392, grad_fn=<AddBackward0>) average train loss tensor(0.5553, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.4964162463498805 val_avg_loss: tensor(4.5841)\n",
      "epoch: 89 train_loss: tensor(0.5701, grad_fn=<AddBackward0>) average train loss tensor(0.5379, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5171518744653866 val_avg_loss: tensor(4.5095)\n",
      "epoch: 90 train_loss: tensor(0.5782, grad_fn=<AddBackward0>) average train loss tensor(0.5429, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5023449252278559 val_avg_loss: tensor(4.6379)\n",
      "epoch: 91 train_loss: tensor(0.5809, grad_fn=<AddBackward0>) average train loss tensor(0.5456, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.498982390938855 val_avg_loss: tensor(4.6153)\n",
      "epoch: 92 train_loss: tensor(0.5572, grad_fn=<AddBackward0>) average train loss tensor(0.5431, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5406601185735775 val_avg_loss: tensor(4.3526)\n",
      "epoch: 93 train_loss: tensor(0.5727, grad_fn=<AddBackward0>) average train loss tensor(0.5337, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.5574727900185824 val_avg_loss: tensor(4.2845)\n",
      "epoch: 94 train_loss: tensor(0.5774, grad_fn=<AddBackward0>) average train loss tensor(0.5413, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5497153644220275 val_avg_loss: tensor(4.3484)\n",
      "epoch: 95 train_loss: tensor(0.5768, grad_fn=<AddBackward0>) average train loss tensor(0.5388, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5319588237029171 val_avg_loss: tensor(4.4421)\n",
      "epoch: 96 train_loss: tensor(0.5621, grad_fn=<AddBackward0>) average train loss tensor(0.5326, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5320473114473645 val_avg_loss: tensor(4.3767)\n",
      "epoch: 97 train_loss: tensor(0.5594, grad_fn=<AddBackward0>) average train loss tensor(0.5393, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5351738784178391 val_avg_loss: tensor(4.4216)\n",
      "epoch: 98 train_loss: tensor(0.5520, grad_fn=<AddBackward0>) average train loss tensor(0.5338, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5740199982302451 val_avg_loss: tensor(4.2358)\n",
      "epoch: 99 train_loss: tensor(0.5811, grad_fn=<AddBackward0>) average train loss tensor(0.5183, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.542636344866236 val_avg_loss: tensor(4.5145)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.54      0.70     41251\n",
      "           1       0.04      0.67      0.07      1128\n",
      "\n",
      "    accuracy                           0.54     42379\n",
      "   macro avg       0.51      0.60      0.38     42379\n",
      "weighted avg       0.96      0.54      0.68     42379\n",
      "\n",
      "roc auc score: 0.6619142297603445\n"
     ]
    }
   ],
   "source": [
    "model = WtsTridentModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/trident_bn_d128_drop05_wd0005_weight400')\n",
    "\n",
    "fit_wts_trident_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=100,\n",
    "    writer=writer,\n",
    "    train_loader=wts_train_loader,\n",
    "    val_loader=wts_val_loader,\n",
    "    weight=torch.tensor([1, 400]).float()\n",
    ")\n",
    "\n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())[0]\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))\n",
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_upsample_worthiness_loader(upsample_rate=1):\n",
    "    indices_0 = (y_wts_train[:, 1] == 0).nonzero()[0]\n",
    "    indices_1 = (y_wts_train[:, 1] == 1).nonzero()[0]\n",
    "    \n",
    "    n0 = (y_wts_train[:, 1] == 0).nonzero()[0].shape[0]\n",
    "    n1 = (y_wts_train[:, 1] == 1).nonzero()[0].shape[0]\n",
    "    \n",
    "    upsample_indices = indices_1[np.random.choice(n1, n0 * upsample_rate)]\n",
    "    concatenated_indices = np.concatenate((indices_0, upsample_indices))\n",
    "    shuffled_indices = np.random.permutation(concatenated_indices)\n",
    "\n",
    "    x_img_train_upsampled = x_img_train[shuffled_indices]\n",
    "    x_txt_train_upsampled = x_txt_train[shuffled_indices]\n",
    "    y_wts_train_upsampled = y_wts_train[shuffled_indices]\n",
    "    \n",
    "    x_img_train_upsampled_t = torch.tensor(x_img_train_upsampled).float()\n",
    "    x_txt_train_upsampled_t = torch.tensor(x_txt_train_upsampled).float()\n",
    "    y_wts_train_upsampled_t = torch.tensor(y_wts_train_upsampled).float()\n",
    " \n",
    "    train_ds_upsampled = TensorDataset(\n",
    "        x_img_train_upsampled_t, \n",
    "        x_txt_train_upsampled_t,\n",
    "        y_wts_train_upsampled_t\n",
    "    )\n",
    "    \n",
    "    return DataLoader(train_ds_upsampled, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with upsample_rate 0\n",
      "epoch: 0 train_loss: tensor(0.2462, grad_fn=<NllLossBackward>) average train loss tensor(0.5391, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.2293)\n",
      "epoch: 1 train_loss: tensor(0.0280, grad_fn=<NllLossBackward>) average train loss tensor(0.0895, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1176)\n",
      "epoch: 2 train_loss: tensor(0.0104, grad_fn=<NllLossBackward>) average train loss tensor(0.0167, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1292)\n",
      "epoch: 3 train_loss: tensor(0.0057, grad_fn=<NllLossBackward>) average train loss tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1405)\n",
      "epoch: 4 train_loss: tensor(0.0037, grad_fn=<NllLossBackward>) average train loss tensor(0.0045, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1497)\n",
      "epoch: 5 train_loss: tensor(0.0026, grad_fn=<NllLossBackward>) average train loss tensor(0.0031, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1576)\n",
      "epoch: 6 train_loss: tensor(0.0020, grad_fn=<NllLossBackward>) average train loss tensor(0.0023, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1638)\n",
      "epoch: 7 train_loss: tensor(0.0016, grad_fn=<NllLossBackward>) average train loss tensor(0.0018, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1684)\n",
      "epoch: 8 train_loss: tensor(0.0013, grad_fn=<NllLossBackward>) average train loss tensor(0.0014, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1729)\n",
      "epoch: 9 train_loss: tensor(0.0011, grad_fn=<NllLossBackward>) average train loss tensor(0.0012, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1770)\n",
      "epoch: 10 train_loss: tensor(0.0009, grad_fn=<NllLossBackward>) average train loss tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1812)\n",
      "epoch: 11 train_loss: tensor(0.0008, grad_fn=<NllLossBackward>) average train loss tensor(0.0009, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1839)\n",
      "epoch: 12 train_loss: tensor(0.0007, grad_fn=<NllLossBackward>) average train loss tensor(0.0008, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1852)\n",
      "epoch: 13 train_loss: tensor(0.0007, grad_fn=<NllLossBackward>) average train loss tensor(0.0007, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1881)\n",
      "epoch: 14 train_loss: tensor(0.0006, grad_fn=<NllLossBackward>) average train loss tensor(0.0007, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1887)\n",
      "epoch: 15 train_loss: tensor(0.0006, grad_fn=<NllLossBackward>) average train loss tensor(0.0006, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1895)\n",
      "epoch: 16 train_loss: tensor(0.0005, grad_fn=<NllLossBackward>) average train loss tensor(0.0006, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1919)\n",
      "epoch: 17 train_loss: tensor(0.0005, grad_fn=<NllLossBackward>) average train loss tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1921)\n",
      "epoch: 18 train_loss: tensor(0.0005, grad_fn=<NllLossBackward>) average train loss tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1934)\n",
      "epoch: 19 train_loss: tensor(0.0005, grad_fn=<NllLossBackward>) average train loss tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1943)\n",
      "epoch: 20 train_loss: tensor(0.0005, grad_fn=<NllLossBackward>) average train loss tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1941)\n",
      "epoch: 21 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1946)\n",
      "epoch: 22 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1952)\n",
      "epoch: 23 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1957)\n",
      "epoch: 24 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1962)\n",
      "epoch: 25 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1966)\n",
      "epoch: 26 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1969)\n",
      "epoch: 27 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1972)\n",
      "epoch: 28 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1974)\n",
      "epoch: 29 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1976)\n",
      "epoch: 30 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1978)\n",
      "epoch: 31 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1979)\n",
      "epoch: 32 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1981)\n",
      "epoch: 33 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1981)\n",
      "epoch: 34 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1982)\n",
      "epoch: 35 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1983)\n",
      "epoch: 36 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1984)\n",
      "epoch: 37 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1984)\n",
      "epoch: 38 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1985)\n",
      "epoch: 39 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1985)\n",
      "epoch: 40 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1986)\n",
      "epoch: 41 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1986)\n",
      "epoch: 42 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1987)\n",
      "epoch: 43 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1987)\n",
      "epoch: 44 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1988)\n",
      "epoch: 45 train_loss: tensor(0.0004, grad_fn=<NllLossBackward>) average train loss tensor(0.0004, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc: 0.9747220010028611 val_avg_loss: tensor(0.1988)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-df21f5131a70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_upsample_worthiness_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsample_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwts_val_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-55a655007f08>\u001b[0m in \u001b[0;36mfit_wts_model\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, writer, epochs, es_dif, es_tol, weight)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_loss_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx_img_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_txt_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_wts_cur\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_img_cur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_txt_cur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for upsample_rate in range(3):\n",
    "    print('train with upsample_rate', upsample_rate)\n",
    "    \n",
    "    model = WtsNormModelBN(d=128, drop=0.5)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "    writer = SummaryWriter('runs/norm_d128_drop05_wd0005_upsample' + str(upsample_rate))\n",
    "\n",
    "    fit_wts_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epochs=100,\n",
    "        writer=writer,\n",
    "        train_loader=get_upsample_worthiness_loader(upsample_rate=upsample_rate),\n",
    "        val_loader=wts_val_loader\n",
    "    )\n",
    "    \n",
    "    predictions = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "            outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "            predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "    predictions = predictions.numpy()\n",
    "\n",
    "    print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report for norm model fitted on upsampled ds with upsample_rate=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99     41251\n",
      "           1       0.00      0.00      0.00      1128\n",
      "\n",
      "    accuracy                           0.97     42379\n",
      "   macro avg       0.49      0.50      0.49     42379\n",
      "weighted avg       0.95      0.97      0.96     42379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(0.3790, grad_fn=<NllLossBackward>) average train loss tensor(0.5243, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6273191163023921 val_avg_loss: tensor(0.7520)\n",
      "epoch: 1 train_loss: tensor(0.2378, grad_fn=<NllLossBackward>) average train loss tensor(0.3125, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8132908592159985 val_avg_loss: tensor(0.4452)\n",
      "epoch: 2 train_loss: tensor(0.1782, grad_fn=<NllLossBackward>) average train loss tensor(0.2118, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8644662714214081 val_avg_loss: tensor(0.3616)\n",
      "epoch: 3 train_loss: tensor(0.1123, grad_fn=<NllLossBackward>) average train loss tensor(0.1618, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.8943456331298115 val_avg_loss: tensor(0.3110)\n",
      "epoch: 4 train_loss: tensor(0.1100, grad_fn=<NllLossBackward>) average train loss tensor(0.1328, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9101849393858951 val_avg_loss: tensor(0.2810)\n",
      "epoch: 5 train_loss: tensor(0.0857, grad_fn=<NllLossBackward>) average train loss tensor(0.1151, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9201840545084505 val_avg_loss: tensor(0.2526)\n",
      "epoch: 6 train_loss: tensor(0.0815, grad_fn=<NllLossBackward>) average train loss tensor(0.1048, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9304191369495325 val_avg_loss: tensor(0.2399)\n",
      "epoch: 7 train_loss: tensor(0.0675, grad_fn=<NllLossBackward>) average train loss tensor(0.0947, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9358758811904552 val_avg_loss: tensor(0.2328)\n",
      "epoch: 8 train_loss: tensor(0.0464, grad_fn=<NllLossBackward>) average train loss tensor(0.0892, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9299766982272956 val_avg_loss: tensor(0.2464)\n",
      "epoch: 9 train_loss: tensor(0.0431, grad_fn=<NllLossBackward>) average train loss tensor(0.0839, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9359938648497184 val_avg_loss: tensor(0.2282)\n",
      "epoch: 10 train_loss: tensor(0.0651, grad_fn=<NllLossBackward>) average train loss tensor(0.0808, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9424534701943781 val_avg_loss: tensor(0.2204)\n",
      "epoch: 11 train_loss: tensor(0.0507, grad_fn=<NllLossBackward>) average train loss tensor(0.0772, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9349910037459812 val_avg_loss: tensor(0.2324)\n",
      "epoch: 12 train_loss: tensor(0.0661, grad_fn=<NllLossBackward>) average train loss tensor(0.0757, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9347845323422705 val_avg_loss: tensor(0.2338)\n",
      "epoch: 13 train_loss: tensor(0.0725, grad_fn=<NllLossBackward>) average train loss tensor(0.0737, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.944813143379642 val_avg_loss: tensor(0.2149)\n",
      "epoch: 14 train_loss: tensor(0.0465, grad_fn=<NllLossBackward>) average train loss tensor(0.0720, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9431613721499572 val_avg_loss: tensor(0.2188)\n",
      "epoch: 15 train_loss: tensor(0.0626, grad_fn=<NllLossBackward>) average train loss tensor(0.0705, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.949031059198301 val_avg_loss: tensor(0.2031)\n",
      "epoch: 16 train_loss: tensor(0.0608, grad_fn=<NllLossBackward>) average train loss tensor(0.0673, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9481461817538271 val_avg_loss: tensor(0.2100)\n",
      "epoch: 17 train_loss: tensor(0.0630, grad_fn=<NllLossBackward>) average train loss tensor(0.0669, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9418340559832463 val_avg_loss: tensor(0.2171)\n",
      "epoch: 18 train_loss: tensor(0.0414, grad_fn=<NllLossBackward>) average train loss tensor(0.0660, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9482346694982745 val_avg_loss: tensor(0.2033)\n",
      "epoch: 19 train_loss: tensor(0.0235, grad_fn=<NllLossBackward>) average train loss tensor(0.0652, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9451965902722473 val_avg_loss: tensor(0.2147)\n",
      "epoch: 20 train_loss: tensor(0.0505, grad_fn=<NllLossBackward>) average train loss tensor(0.0632, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.942895908916615 val_avg_loss: tensor(0.2232)\n",
      "epoch: 21 train_loss: tensor(0.0421, grad_fn=<NllLossBackward>) average train loss tensor(0.0639, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9478512226056691 val_avg_loss: tensor(0.2110)\n",
      "epoch: 22 train_loss: tensor(0.0519, grad_fn=<NllLossBackward>) average train loss tensor(0.0630, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9484116449871692 val_avg_loss: tensor(0.2103)\n",
      "epoch: 23 train_loss: tensor(0.0386, grad_fn=<NllLossBackward>) average train loss tensor(0.0623, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9478807185204849 val_avg_loss: tensor(0.2108)\n",
      "epoch: 24 train_loss: tensor(0.0536, grad_fn=<NllLossBackward>) average train loss tensor(0.0616, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9460519718019055 val_avg_loss: tensor(0.2150)\n",
      "epoch: 25 train_loss: tensor(0.0361, grad_fn=<NllLossBackward>) average train loss tensor(0.0604, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9366132790608501 val_avg_loss: tensor(0.2360)\n",
      "epoch: 26 train_loss: tensor(0.0516, grad_fn=<NllLossBackward>) average train loss tensor(0.0608, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9425419579388254 val_avg_loss: tensor(0.2213)\n",
      "epoch: 27 train_loss: tensor(0.0446, grad_fn=<NllLossBackward>) average train loss tensor(0.0602, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9461994513759844 val_avg_loss: tensor(0.2107)\n",
      "epoch: 28 train_loss: tensor(0.0358, grad_fn=<NllLossBackward>) average train loss tensor(0.0590, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.953278470931776 val_avg_loss: tensor(0.1984)\n",
      "epoch: 29 train_loss: tensor(0.0380, grad_fn=<NllLossBackward>) average train loss tensor(0.0601, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9487066041353273 val_avg_loss: tensor(0.2202)\n",
      "epoch: 30 train_loss: tensor(0.0484, grad_fn=<NllLossBackward>) average train loss tensor(0.0597, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9487655959649589 val_avg_loss: tensor(0.2077)\n",
      "epoch: 31 train_loss: tensor(0.0328, grad_fn=<NllLossBackward>) average train loss tensor(0.0585, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9553726808836976 val_avg_loss: tensor(0.1927)\n",
      "epoch: 32 train_loss: tensor(0.0325, grad_fn=<NllLossBackward>) average train loss tensor(0.0575, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9459339881426423 val_avg_loss: tensor(0.2174)\n",
      "epoch: 33 train_loss: tensor(0.0389, grad_fn=<NllLossBackward>) average train loss tensor(0.0586, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9404182520720881 val_avg_loss: tensor(0.2318)\n",
      "epoch: 34 train_loss: tensor(0.0551, grad_fn=<NllLossBackward>) average train loss tensor(0.0577, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9413326254313777 val_avg_loss: tensor(0.2202)\n",
      "epoch: 35 train_loss: tensor(0.0487, grad_fn=<NllLossBackward>) average train loss tensor(0.0581, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9474972716278796 val_avg_loss: tensor(0.2148)\n",
      "epoch: 36 train_loss: tensor(0.0381, grad_fn=<NllLossBackward>) average train loss tensor(0.0572, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9446066719759313 val_avg_loss: tensor(0.2156)\n",
      "epoch: 37 train_loss: tensor(0.0355, grad_fn=<NllLossBackward>) average train loss tensor(0.0569, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9472318083945374 val_avg_loss: tensor(0.2140)\n",
      "epoch: 38 train_loss: tensor(0.0469, grad_fn=<NllLossBackward>) average train loss tensor(0.0571, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9535144382503024 val_avg_loss: tensor(0.2048)\n",
      "epoch: 39 train_loss: tensor(0.0446, grad_fn=<NllLossBackward>) average train loss tensor(0.0568, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.9532489750169602 val_avg_loss: tensor(0.2119)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98     41251\n",
      "           1       0.07      0.07      0.07      1128\n",
      "\n",
      "    accuracy                           0.95     42379\n",
      "   macro avg       0.52      0.52      0.52     42379\n",
      "weighted avg       0.95      0.95      0.95     42379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = WtsNormModelBN(d=128, drop=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "writer = SummaryWriter('runs/norm_d128_drop05_wd0005_upsample2')\n",
    "\n",
    "fit_wts_model(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    epochs=40,\n",
    "    writer=writer,\n",
    "    train_loader=get_upsample_worthiness_loader(upsample_rate=2),\n",
    "    val_loader=wts_val_loader\n",
    ")\n",
    "    \n",
    "predictions = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_img_cur, x_txt_cur, _ in wts_test_loader:\n",
    "        outputs = model(x_img_cur.float(), x_txt_cur.float())\n",
    "        predictions = torch.cat((predictions, outputs), 0)\n",
    "\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "print(classification_report(np.argmax(y_wts_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score: 0.6079165327777999\n"
     ]
    }
   ],
   "source": [
    "print('roc auc score:', roc_auc_score(y_wts_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
