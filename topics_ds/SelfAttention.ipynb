{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch import torch_models\n",
    "from data import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_LEN = 1024\n",
    "TXT_LEN = 300\n",
    "N_CLASSES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img, x_txt, y = data.get_unpacked_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_train, x_img_test, x_txt_train, x_txt_test, y_train, y_test = train_test_split(\n",
    "    x_img, \n",
    "    x_txt, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "x_img_train, x_img_val, x_txt_train, x_txt_val, y_train, y_val = train_test_split(\n",
    "    x_img_train,\n",
    "    x_txt_train,\n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "img_sscaler = StandardScaler()\n",
    "img_sscaler.fit(x_img_train)\n",
    "\n",
    "x_img_train = img_sscaler.transform(x_img_train)\n",
    "x_img_val = img_sscaler.transform(x_img_val)\n",
    "x_img_test = img_sscaler.transform(x_img_test)\n",
    "\n",
    "txt_sscaler = StandardScaler()\n",
    "txt_sscaler.fit(x_txt_train)\n",
    "\n",
    "x_txt_train = txt_sscaler.transform(x_txt_train)\n",
    "x_txt_val = txt_sscaler.transform(x_txt_val)\n",
    "x_txt_test = txt_sscaler.transform(x_txt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "x_img_train_t = torch.tensor(x_img_train)\n",
    "x_img_val_t = torch.tensor(x_img_val)\n",
    "x_img_test_t = torch.tensor(x_img_test)\n",
    "\n",
    "x_txt_train_t = torch.tensor(x_txt_train)\n",
    "x_txt_val_t = torch.tensor(x_txt_val)\n",
    "x_txt_test_t = torch.tensor(x_txt_test)\n",
    "\n",
    "y_train_t = torch.tensor(y_train)\n",
    "y_val_t = torch.tensor(y_val)\n",
    "y_test_t = torch.tensor(y_test)\n",
    "    \n",
    "train_ds = TensorDataset(x_img_train_t, x_txt_train_t, y_train_t)\n",
    "val_ds = TensorDataset(x_img_val_t, x_txt_val_t, y_val_t)\n",
    "test_ds = TensorDataset(x_img_test_t, x_txt_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attention_v1 = torch_models.SelfAttentionModel1()\n",
    "optimizer = optim.Adam(model_attention_v1.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter('runs/self_attention_v1_bs512_rs42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(3.3161, grad_fn=<NllLossBackward>) average train loss tensor(3.5828, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10063281824871229 val_avg_loss: tensor(3.4450)\n",
      "epoch: 1 train_loss: tensor(3.1592, grad_fn=<NllLossBackward>) average train loss tensor(3.3537, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10990434142752023 val_avg_loss: tensor(3.3107)\n",
      "epoch: 2 train_loss: tensor(3.3190, grad_fn=<NllLossBackward>) average train loss tensor(3.2236, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1325091979396615 val_avg_loss: tensor(3.1952)\n",
      "epoch: 3 train_loss: tensor(3.1255, grad_fn=<NllLossBackward>) average train loss tensor(3.1339, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.16176600441501104 val_avg_loss: tensor(3.0839)\n",
      "epoch: 4 train_loss: tensor(3.0006, grad_fn=<NllLossBackward>) average train loss tensor(3.0035, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18881530537159677 val_avg_loss: tensor(2.9689)\n",
      "epoch: 5 train_loss: tensor(2.9385, grad_fn=<NllLossBackward>) average train loss tensor(2.8537, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19423105224429726 val_avg_loss: tensor(2.9186)\n",
      "epoch: 6 train_loss: tensor(2.7400, grad_fn=<NllLossBackward>) average train loss tensor(2.7934, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.24085356880058867 val_avg_loss: tensor(2.7806)\n",
      "epoch: 7 train_loss: tensor(2.7252, grad_fn=<NllLossBackward>) average train loss tensor(2.7748, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2488300220750552 val_avg_loss: tensor(2.7339)\n",
      "epoch: 8 train_loss: tensor(2.7389, grad_fn=<NllLossBackward>) average train loss tensor(2.6718, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2288447387785136 val_avg_loss: tensor(2.7992)\n",
      "epoch: 9 train_loss: tensor(3.4418, grad_fn=<NllLossBackward>) average train loss tensor(3.3790, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.12488594554819721 val_avg_loss: tensor(3.2726)\n",
      "epoch: 10 train_loss: tensor(3.5442, grad_fn=<NllLossBackward>) average train loss tensor(3.1899, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1489036055923473 val_avg_loss: tensor(3.1400)\n",
      "epoch: 11 train_loss: tensor(3.0811, grad_fn=<NllLossBackward>) average train loss tensor(3.0602, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.18054451802796173 val_avg_loss: tensor(3.0658)\n",
      "epoch: 12 train_loss: tensor(2.9786, grad_fn=<NllLossBackward>) average train loss tensor(2.8305, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.22263428991905812 val_avg_loss: tensor(2.8318)\n",
      "epoch: 13 train_loss: tensor(3.0034, grad_fn=<NllLossBackward>) average train loss tensor(2.7906, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2242531272994849 val_avg_loss: tensor(2.8251)\n",
      "epoch: 14 train_loss: tensor(2.6872, grad_fn=<NllLossBackward>) average train loss tensor(2.6799, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2491243561442237 val_avg_loss: tensor(2.7205)\n",
      "epoch: 15 train_loss: tensor(2.4920, grad_fn=<NllLossBackward>) average train loss tensor(2.5981, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2479175864606328 val_avg_loss: tensor(2.7383)\n",
      "epoch: 16 train_loss: tensor(2.7466, grad_fn=<NllLossBackward>) average train loss tensor(2.6326, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2512729948491538 val_avg_loss: tensor(2.7225)\n",
      "epoch: 17 train_loss: tensor(2.7080, grad_fn=<NllLossBackward>) average train loss tensor(2.5860, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2763502575423105 val_avg_loss: tensor(2.6375)\n",
      "epoch: 18 train_loss: tensor(2.8364, grad_fn=<NllLossBackward>) average train loss tensor(2.5783, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2836497424576895 val_avg_loss: tensor(2.6061)\n",
      "epoch: 19 train_loss: tensor(2.4389, grad_fn=<NllLossBackward>) average train loss tensor(2.5067, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.28974245768947754 val_avg_loss: tensor(2.5935)\n",
      "epoch: 20 train_loss: tensor(3.1036, grad_fn=<NllLossBackward>) average train loss tensor(2.6870, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2503016924208977 val_avg_loss: tensor(2.7149)\n",
      "epoch: 21 train_loss: tensor(2.8339, grad_fn=<NllLossBackward>) average train loss tensor(2.7221, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.23396615158204562 val_avg_loss: tensor(2.7606)\n",
      "epoch: 22 train_loss: tensor(2.6301, grad_fn=<NllLossBackward>) average train loss tensor(2.6060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2791758646063282 val_avg_loss: tensor(2.6124)\n",
      "epoch: 23 train_loss: tensor(3.1793, grad_fn=<NllLossBackward>) average train loss tensor(3.0167, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1973215599705666 val_avg_loss: tensor(2.9149)\n",
      "epoch: 24 train_loss: tensor(2.7473, grad_fn=<NllLossBackward>) average train loss tensor(2.6831, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2529506990434143 val_avg_loss: tensor(2.6938)\n",
      "epoch: 25 train_loss: tensor(3.0308, grad_fn=<NllLossBackward>) average train loss tensor(2.6600, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.25954378219278884 val_avg_loss: tensor(2.6763)\n",
      "epoch: 26 train_loss: tensor(2.7500, grad_fn=<NllLossBackward>) average train loss tensor(2.5520, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.27484915378955116 val_avg_loss: tensor(2.6137)\n",
      "epoch: 27 train_loss: tensor(3.2000, grad_fn=<NllLossBackward>) average train loss tensor(2.5724, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2769683590875644 val_avg_loss: tensor(2.6237)\n",
      "epoch: 28 train_loss: tensor(3.2452, grad_fn=<NllLossBackward>) average train loss tensor(2.5115, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2527740986019132 val_avg_loss: tensor(2.7129)\n",
      "epoch: 29 train_loss: tensor(3.4006, grad_fn=<NllLossBackward>) average train loss tensor(3.1661, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.19214128035320088 val_avg_loss: tensor(2.9757)\n",
      "epoch: 30 train_loss: tensor(2.7748, grad_fn=<NllLossBackward>) average train loss tensor(2.6348, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2748197203826343 val_avg_loss: tensor(2.6255)\n",
      "epoch: 31 train_loss: tensor(3.0149, grad_fn=<NllLossBackward>) average train loss tensor(2.5178, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2824135393671818 val_avg_loss: tensor(2.6242)\n",
      "epoch: 32 train_loss: tensor(2.8665, grad_fn=<NllLossBackward>) average train loss tensor(2.5855, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.29212656364974243 val_avg_loss: tensor(2.5624)\n",
      "epoch: 33 train_loss: tensor(2.8060, grad_fn=<NllLossBackward>) average train loss tensor(2.5248, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2891832229580574 val_avg_loss: tensor(2.5796)\n",
      "epoch: 34 train_loss: tensor(2.6860, grad_fn=<NllLossBackward>) average train loss tensor(2.4230, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30793230316409126 val_avg_loss: tensor(2.5108)\n",
      "epoch: 35 train_loss: tensor(2.7112, grad_fn=<NllLossBackward>) average train loss tensor(2.4118, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.32479764532744665 val_avg_loss: tensor(2.4377)\n",
      "epoch: 36 train_loss: tensor(2.5610, grad_fn=<NllLossBackward>) average train loss tensor(2.3651, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.33336276674025017 val_avg_loss: tensor(2.4221)\n",
      "epoch: 37 train_loss: tensor(2.4641, grad_fn=<NllLossBackward>) average train loss tensor(2.3046, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3335393671817513 val_avg_loss: tensor(2.4412)\n",
      "epoch: 38 train_loss: tensor(2.2689, grad_fn=<NllLossBackward>) average train loss tensor(2.2759, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.34799116997792495 val_avg_loss: tensor(2.3661)\n",
      "epoch: 39 train_loss: tensor(2.3692, grad_fn=<NllLossBackward>) average train loss tensor(2.2552, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35161147902869755 val_avg_loss: tensor(2.3555)\n",
      "epoch: 40 train_loss: tensor(2.4893, grad_fn=<NllLossBackward>) average train loss tensor(2.2516, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35573215599705665 val_avg_loss: tensor(2.3420)\n",
      "epoch: 41 train_loss: tensor(2.4620, grad_fn=<NllLossBackward>) average train loss tensor(2.2292, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3565562913907285 val_avg_loss: tensor(2.3371)\n",
      "epoch: 42 train_loss: tensor(2.4623, grad_fn=<NllLossBackward>) average train loss tensor(2.2213, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3606475349521707 val_avg_loss: tensor(2.3352)\n",
      "epoch: 43 train_loss: tensor(2.7668, grad_fn=<NllLossBackward>) average train loss tensor(2.1882, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3353053715967623 val_avg_loss: tensor(2.4386)\n",
      "epoch: 44 train_loss: tensor(2.1113, grad_fn=<NllLossBackward>) average train loss tensor(2.2320, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.373009565857248 val_avg_loss: tensor(2.2986)\n",
      "epoch: 45 train_loss: tensor(2.1112, grad_fn=<NllLossBackward>) average train loss tensor(2.1602, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3750699043414275 val_avg_loss: tensor(2.2854)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(3.2198, grad_fn=<NllLossBackward>) average train loss tensor(2.1446, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3640618101545254 val_avg_loss: tensor(2.3183)\n",
      "epoch: 47 train_loss: tensor(2.3505, grad_fn=<NllLossBackward>) average train loss tensor(2.2173, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.37654157468727006 val_avg_loss: tensor(2.2927)\n",
      "epoch: 48 train_loss: tensor(2.4119, grad_fn=<NllLossBackward>) average train loss tensor(2.1355, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3831346578366446 val_avg_loss: tensor(2.2590)\n",
      "epoch: 49 train_loss: tensor(2.3217, grad_fn=<NllLossBackward>) average train loss tensor(2.1063, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.39311258278145694 val_avg_loss: tensor(2.2494)\n",
      "epoch: 50 train_loss: tensor(2.2029, grad_fn=<NllLossBackward>) average train loss tensor(2.0799, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3958204562178072 val_avg_loss: tensor(2.2438)\n",
      "epoch: 51 train_loss: tensor(4.0609, grad_fn=<NllLossBackward>) average train loss tensor(3.1421, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.10166298749080206 val_avg_loss: tensor(3.4100)\n",
      "epoch: 52 train_loss: tensor(3.8668, grad_fn=<NllLossBackward>) average train loss tensor(3.1682, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.1756291390728477 val_avg_loss: tensor(3.0659)\n",
      "epoch: 53 train_loss: tensor(3.3080, grad_fn=<NllLossBackward>) average train loss tensor(2.8726, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.2561000735835173 val_avg_loss: tensor(2.7723)\n",
      "epoch: 54 train_loss: tensor(2.9498, grad_fn=<NllLossBackward>) average train loss tensor(2.5681, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.30083885209713024 val_avg_loss: tensor(2.6236)\n",
      "epoch: 55 train_loss: tensor(2.4690, grad_fn=<NllLossBackward>) average train loss tensor(2.4457, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3304194260485651 val_avg_loss: tensor(2.4427)\n",
      "epoch: 56 train_loss: tensor(3.3512, grad_fn=<NllLossBackward>) average train loss tensor(2.3261, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.3521707137601177 val_avg_loss: tensor(2.3791)\n",
      "epoch: 57 train_loss: tensor(4.1883, grad_fn=<NllLossBackward>) average train loss tensor(2.3255, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.36161883738042677 val_avg_loss: tensor(2.3740)\n",
      "epoch: 58 train_loss: tensor(2.6721, grad_fn=<NllLossBackward>) average train loss tensor(2.3060, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.35514348785871963 val_avg_loss: tensor(2.3407)\n",
      "epoch: 59 train_loss: tensor(2.2943, grad_fn=<NllLossBackward>) average train loss tensor(2.1770, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.38328182487122886 val_avg_loss: tensor(2.2787)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 60\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "        \n",
    "    for x_img_cur, x_txt_cur, y_cur in train_loader:\n",
    "        model_attention_v1.zero_grad()\n",
    "        output = model_attention_v1(x_img_cur.view(-1, IMG_LEN).float(), x_txt_cur.view(-1, TXT_LEN).float())\n",
    "        loss = F.nll_loss(output, torch.argmax(y_cur, dim=1))\n",
    "        loss.backward()\n",
    "            \n",
    "        loss_sum += loss\n",
    "        loss_count += 1\n",
    "            \n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch:', epoch, 'train_loss:', loss, 'average train loss', loss_sum/loss_count)\n",
    "    writer.add_scalar('train_loss', loss, epoch)\n",
    "    writer.add_scalar('avg_train_loss', loss_sum/loss_count, epoch)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for x_img_cur, x_txt_cur, y_cur in val_loader:\n",
    "            output = model_attention_v1(x_img_cur.view(-1, IMG_LEN).float(), x_txt_cur.view(-1, TXT_LEN).float())\n",
    "            loss = F.nll_loss(output, torch.argmax(y_cur, dim=1))\n",
    "            loss_sum += loss\n",
    "            loss_count += 1\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == torch.argmax(y_cur, dim=1)[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    print('val_acc:', correct/total, 'val_avg_loss:', loss_sum/loss_count)\n",
    "    writer.add_scalar('val_acc', correct/total, epoch)\n",
    "    writer.add_scalar('val_avg_loss', loss_sum/loss_count, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ua_v1 = torch_models.UAModel1()\n",
    "optimizer = optim.Adam(model_ua_v1.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter('runs/ua_v1_bs512_rs42')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: tensor(1.2151, grad_fn=<NllLossBackward>) average train loss tensor(1.6174, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5988226637233259 val_avg_loss: tensor(1.4589)\n",
      "epoch: 1 train_loss: tensor(1.0648, grad_fn=<NllLossBackward>) average train loss tensor(1.3605, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6077115526122149 val_avg_loss: tensor(1.4206)\n",
      "epoch: 2 train_loss: tensor(0.9884, grad_fn=<NllLossBackward>) average train loss tensor(1.2905, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6070640176600441 val_avg_loss: tensor(1.4223)\n",
      "epoch: 3 train_loss: tensor(0.8413, grad_fn=<NllLossBackward>) average train loss tensor(1.2334, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6102722590139809 val_avg_loss: tensor(1.4242)\n",
      "epoch: 4 train_loss: tensor(0.7145, grad_fn=<NllLossBackward>) average train loss tensor(1.1803, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.6082119205298013 val_avg_loss: tensor(1.4456)\n",
      "epoch: 5 train_loss: tensor(0.6366, grad_fn=<NllLossBackward>) average train loss tensor(1.1276, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.602766740250184 val_avg_loss: tensor(1.4848)\n",
      "epoch: 6 train_loss: tensor(0.5736, grad_fn=<NllLossBackward>) average train loss tensor(1.0668, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5896100073583518 val_avg_loss: tensor(1.5768)\n",
      "epoch: 7 train_loss: tensor(0.5148, grad_fn=<NllLossBackward>) average train loss tensor(1.1281, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5935540838852097 val_avg_loss: tensor(1.5549)\n",
      "epoch: 8 train_loss: tensor(0.4656, grad_fn=<NllLossBackward>) average train loss tensor(1.0013, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5917586460632819 val_avg_loss: tensor(1.5973)\n",
      "epoch: 9 train_loss: tensor(0.3751, grad_fn=<NllLossBackward>) average train loss tensor(0.9290, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5795437821927888 val_avg_loss: tensor(1.6937)\n",
      "epoch: 10 train_loss: tensor(0.2978, grad_fn=<NllLossBackward>) average train loss tensor(0.8675, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5722737306843267 val_avg_loss: tensor(1.7680)\n",
      "epoch: 11 train_loss: tensor(0.2980, grad_fn=<NllLossBackward>) average train loss tensor(0.8190, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5686828550404709 val_avg_loss: tensor(1.8667)\n",
      "epoch: 12 train_loss: tensor(0.2642, grad_fn=<NllLossBackward>) average train loss tensor(0.7540, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5636497424576895 val_avg_loss: tensor(1.9763)\n",
      "epoch: 13 train_loss: tensor(0.2394, grad_fn=<NllLossBackward>) average train loss tensor(0.7091, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5572332597498161 val_avg_loss: tensor(2.0834)\n",
      "epoch: 14 train_loss: tensor(0.2319, grad_fn=<NllLossBackward>) average train loss tensor(0.6668, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5542016188373804 val_avg_loss: tensor(2.2110)\n",
      "epoch: 15 train_loss: tensor(0.2313, grad_fn=<NllLossBackward>) average train loss tensor(0.6328, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.546195732155997 val_avg_loss: tensor(2.3214)\n",
      "epoch: 16 train_loss: tensor(0.2070, grad_fn=<NllLossBackward>) average train loss tensor(0.6235, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5502869757174393 val_avg_loss: tensor(2.3789)\n",
      "epoch: 17 train_loss: tensor(0.1918, grad_fn=<NllLossBackward>) average train loss tensor(0.5919, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5449300956585725 val_avg_loss: tensor(2.4667)\n",
      "epoch: 18 train_loss: tensor(0.1765, grad_fn=<NllLossBackward>) average train loss tensor(0.5609, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5448712288447388 val_avg_loss: tensor(2.5637)\n",
      "epoch: 19 train_loss: tensor(0.2434, grad_fn=<NllLossBackward>) average train loss tensor(0.5706, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5393377483443709 val_avg_loss: tensor(2.5851)\n",
      "epoch: 20 train_loss: tensor(0.2193, grad_fn=<NllLossBackward>) average train loss tensor(0.5369, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5436644591611479 val_avg_loss: tensor(2.7121)\n",
      "epoch: 21 train_loss: tensor(0.1637, grad_fn=<NllLossBackward>) average train loss tensor(0.4802, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5432229580573952 val_avg_loss: tensor(2.8142)\n",
      "epoch: 22 train_loss: tensor(0.1321, grad_fn=<NllLossBackward>) average train loss tensor(0.4598, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5413097866077998 val_avg_loss: tensor(2.9096)\n",
      "epoch: 23 train_loss: tensor(0.1905, grad_fn=<NllLossBackward>) average train loss tensor(0.4532, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5426048565121413 val_avg_loss: tensor(3.0001)\n",
      "epoch: 24 train_loss: tensor(0.2067, grad_fn=<NllLossBackward>) average train loss tensor(0.4452, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5409860191317145 val_avg_loss: tensor(3.0456)\n",
      "epoch: 25 train_loss: tensor(0.1446, grad_fn=<NllLossBackward>) average train loss tensor(0.4337, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5416041206769684 val_avg_loss: tensor(3.1063)\n",
      "epoch: 26 train_loss: tensor(0.1722, grad_fn=<NllLossBackward>) average train loss tensor(0.4046, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5431346578366446 val_avg_loss: tensor(3.2398)\n",
      "epoch: 27 train_loss: tensor(0.1201, grad_fn=<NllLossBackward>) average train loss tensor(0.4113, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5425165562913907 val_avg_loss: tensor(3.2946)\n",
      "epoch: 28 train_loss: tensor(0.1163, grad_fn=<NllLossBackward>) average train loss tensor(0.3918, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.539896983075791 val_avg_loss: tensor(3.3215)\n",
      "epoch: 29 train_loss: tensor(0.0942, grad_fn=<NllLossBackward>) average train loss tensor(0.3687, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5455187637969094 val_avg_loss: tensor(3.4346)\n",
      "epoch: 30 train_loss: tensor(0.2103, grad_fn=<NllLossBackward>) average train loss tensor(0.3762, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5445474613686534 val_avg_loss: tensor(3.4808)\n",
      "epoch: 31 train_loss: tensor(0.1008, grad_fn=<NllLossBackward>) average train loss tensor(0.3859, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5413392200147167 val_avg_loss: tensor(3.5098)\n",
      "epoch: 32 train_loss: tensor(0.1167, grad_fn=<NllLossBackward>) average train loss tensor(0.3578, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5469315673289183 val_avg_loss: tensor(3.5517)\n",
      "epoch: 33 train_loss: tensor(0.1122, grad_fn=<NllLossBackward>) average train loss tensor(0.3718, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5415452538631347 val_avg_loss: tensor(3.5703)\n",
      "epoch: 34 train_loss: tensor(0.1038, grad_fn=<NllLossBackward>) average train loss tensor(0.3334, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5440176600441501 val_avg_loss: tensor(3.6542)\n",
      "epoch: 35 train_loss: tensor(0.0913, grad_fn=<NllLossBackward>) average train loss tensor(0.3213, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5397792494481236 val_avg_loss: tensor(3.8310)\n",
      "epoch: 36 train_loss: tensor(0.1373, grad_fn=<NllLossBackward>) average train loss tensor(0.3598, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5425754231052244 val_avg_loss: tensor(3.7711)\n",
      "epoch: 37 train_loss: tensor(0.0822, grad_fn=<NllLossBackward>) average train loss tensor(0.3099, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5447534952170714 val_avg_loss: tensor(3.8160)\n",
      "epoch: 38 train_loss: tensor(0.0918, grad_fn=<NllLossBackward>) average train loss tensor(0.2967, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5436644591611479 val_avg_loss: tensor(3.8999)\n",
      "epoch: 39 train_loss: tensor(0.1316, grad_fn=<NllLossBackward>) average train loss tensor(0.2946, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5413980868285504 val_avg_loss: tensor(3.9950)\n",
      "epoch: 40 train_loss: tensor(0.0883, grad_fn=<NllLossBackward>) average train loss tensor(0.3253, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5433995584988962 val_avg_loss: tensor(3.9660)\n",
      "epoch: 41 train_loss: tensor(0.1051, grad_fn=<NllLossBackward>) average train loss tensor(0.3364, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5434584253127299 val_avg_loss: tensor(3.9673)\n",
      "epoch: 42 train_loss: tensor(0.0967, grad_fn=<NllLossBackward>) average train loss tensor(0.2914, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5448712288447388 val_avg_loss: tensor(4.0056)\n",
      "epoch: 43 train_loss: tensor(0.1029, grad_fn=<NllLossBackward>) average train loss tensor(0.2708, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.54186902133922 val_avg_loss: tensor(4.1477)\n",
      "epoch: 44 train_loss: tensor(0.0773, grad_fn=<NllLossBackward>) average train loss tensor(0.2713, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5423693892568064 val_avg_loss: tensor(4.1474)\n",
      "epoch: 45 train_loss: tensor(0.1023, grad_fn=<NllLossBackward>) average train loss tensor(0.2719, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5407211184694628 val_avg_loss: tensor(4.2145)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46 train_loss: tensor(0.0725, grad_fn=<NllLossBackward>) average train loss tensor(0.2710, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5374834437086092 val_avg_loss: tensor(4.3104)\n",
      "epoch: 47 train_loss: tensor(0.1542, grad_fn=<NllLossBackward>) average train loss tensor(0.2884, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5388962472406181 val_avg_loss: tensor(4.3034)\n",
      "epoch: 48 train_loss: tensor(0.1144, grad_fn=<NllLossBackward>) average train loss tensor(0.3179, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5448123620309051 val_avg_loss: tensor(4.2255)\n",
      "epoch: 49 train_loss: tensor(0.1527, grad_fn=<NllLossBackward>) average train loss tensor(0.2672, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5420456217807211 val_avg_loss: tensor(4.2607)\n",
      "epoch: 50 train_loss: tensor(0.1054, grad_fn=<NllLossBackward>) average train loss tensor(0.2438, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5445768947755703 val_avg_loss: tensor(4.3671)\n",
      "epoch: 51 train_loss: tensor(0.0852, grad_fn=<NllLossBackward>) average train loss tensor(0.2502, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5393671817512877 val_avg_loss: tensor(4.4347)\n",
      "epoch: 52 train_loss: tensor(0.0970, grad_fn=<NllLossBackward>) average train loss tensor(0.2486, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5428991905813098 val_avg_loss: tensor(4.4576)\n",
      "epoch: 53 train_loss: tensor(0.1082, grad_fn=<NllLossBackward>) average train loss tensor(0.2567, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5396615158204562 val_avg_loss: tensor(4.4623)\n",
      "epoch: 54 train_loss: tensor(0.1056, grad_fn=<NllLossBackward>) average train loss tensor(0.2499, grad_fn=<DivBackward0>)\n",
      "val_acc: 0.5418101545253863 val_avg_loss: tensor(4.5574)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 60\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "        \n",
    "    for x_img_cur, x_txt_cur, y_cur in train_loader:\n",
    "        model_ua_v1.zero_grad()\n",
    "        output = model_ua_v1(x_img_cur.view(-1, IMG_LEN).float(), x_txt_cur.view(-1, TXT_LEN).float())\n",
    "        loss = F.nll_loss(output, torch.argmax(y_cur, dim=1))\n",
    "        loss.backward()\n",
    "            \n",
    "        loss_sum += loss\n",
    "        loss_count += 1\n",
    "            \n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch:', epoch, 'train_loss:', loss, 'average train loss', loss_sum/loss_count)\n",
    "    writer.add_scalar('train_loss', loss, epoch)\n",
    "    writer.add_scalar('avg_train_loss', loss_sum/loss_count, epoch)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    loss_count = 0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for x_img_cur, x_txt_cur, y_cur in val_loader:\n",
    "            output = model_ua_v1(x_img_cur.view(-1, IMG_LEN).float(), x_txt_cur.view(-1, TXT_LEN).float())\n",
    "            loss = F.nll_loss(output, torch.argmax(y_cur, dim=1))\n",
    "            loss_sum += loss\n",
    "            loss_count += 1\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == torch.argmax(y_cur, dim=1)[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "    print('val_acc:', correct/total, 'val_avg_loss:', loss_sum/loss_count)\n",
    "    writer.add_scalar('val_acc', correct/total, epoch)\n",
    "    writer.add_scalar('val_avg_loss', loss_sum/loss_count, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
